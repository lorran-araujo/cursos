{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1: Classificação Binária Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18f0eabc810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2: Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('./Bases/Bases/entradas_breast.csv')\n",
    "classe = pd.read_csv('./Bases/Bases/saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>186.0000</td>\n",
       "      <td>275.0000</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>243.0000</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>173.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>198.0000</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>205.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    radius_mean   texture_mean   perimeter_mean   area_mean   smoothness_mean  \\\n",
       "0         17.99          10.38           122.80      1001.0           0.11840   \n",
       "1         20.57          17.77           132.90      1326.0           0.08474   \n",
       "2         19.69          21.25           130.00      1203.0           0.10960   \n",
       "3         11.42          20.38            77.58       386.1           0.14250   \n",
       "4         20.29          14.34           135.10      1297.0           0.10030   \n",
       "\n",
       "    compactness_mean   concavity_mean  concave_points_mean   symmetry_mean  \\\n",
       "0            0.27760           0.3001              0.14710          0.2419   \n",
       "1            0.07864           0.0869              0.07017          0.1812   \n",
       "2            0.15990           0.1974              0.12790          0.2069   \n",
       "3            0.28390           0.2414              0.10520          0.2597   \n",
       "4            0.13280         198.0000              0.10430          0.1809   \n",
       "\n",
       "    fractal_dimension_mean  ...   radius_worst   texture_worst  \\\n",
       "0                  0.07871  ...          25.38           17.33   \n",
       "1                  0.05667  ...          24.99           23.41   \n",
       "2                  0.05999  ...          23.57           25.53   \n",
       "3                  0.09744  ...          14.91           26.50   \n",
       "4                  0.05883  ...          22.54           16.67   \n",
       "\n",
       "    perimeter_worst   area_worst   smoothness_worst   compactness_worst  \\\n",
       "0            184.60       2019.0             0.1622              0.6656   \n",
       "1            158.80       1956.0             0.1238              0.1866   \n",
       "2            152.50       1709.0             0.1444              0.4245   \n",
       "3             98.87        567.7             0.2098              0.8663   \n",
       "4            152.20       1575.0             0.1374            205.0000   \n",
       "\n",
       "    concavity_worst   concave_points_worst   symmetry_worst  \\\n",
       "0            0.7119                 0.2654           0.4601   \n",
       "1            0.2416               186.0000         275.0000   \n",
       "2            0.4504               243.0000           0.3613   \n",
       "3            0.6869                 0.2575           0.6638   \n",
       "4            0.4000                 0.1625           0.2364   \n",
       "\n",
       "    fractal_dimension_worst  \n",
       "0                   0.11890  \n",
       "1                   0.08902  \n",
       "2                   0.08758  \n",
       "3                 173.00000  \n",
       "4                   0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='0', data=classe, palette='hls', hue='0', legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), (143, 30), (426, 1), (143, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores_treinamento.shape, previsores_teste.shape, classe_treinamento.shape, classe_teste.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3: Transformação dos dados para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_treinamento = torch.tensor(np.array(previsores_treinamento), dtype=torch.float)\n",
    "classe_treinamento = torch.tensor(np.array(classe_treinamento), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(previsores_treinamento), type(classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4: Construção do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = nn.Sequential(\n",
    "    nn.Linear(30, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classificador.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5: Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs} loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 loss: 10.768446304077326\n",
      "Epoch 2/100 loss: 2.7603256135832432\n",
      "Epoch 3/100 loss: 1.4159593069276144\n",
      "Epoch 4/100 loss: 0.7044955443504245\n",
      "Epoch 5/100 loss: 0.6835518099887427\n",
      "Epoch 6/100 loss: 0.383892047630493\n",
      "Epoch 7/100 loss: 0.6037732985650384\n",
      "Epoch 8/100 loss: 0.2824384972106579\n",
      "Epoch 9/100 loss: 0.2595168122545231\n",
      "Epoch 10/100 loss: 0.2475065805537756\n",
      "Epoch 11/100 loss: 0.310854519609102\n",
      "Epoch 12/100 loss: 0.2356721483127669\n",
      "Epoch 13/100 loss: 0.25209619858583737\n",
      "Epoch 14/100 loss: 0.23270163944987363\n",
      "Epoch 15/100 loss: 0.23848347108118062\n",
      "Epoch 16/100 loss: 0.2063277908275987\n",
      "Epoch 17/100 loss: 0.2438664756888567\n",
      "Epoch 18/100 loss: 0.28400414688296094\n",
      "Epoch 19/100 loss: 0.2219947864149892\n",
      "Epoch 20/100 loss: 0.22884747333997904\n",
      "Epoch 21/100 loss: 0.1983977863781674\n",
      "Epoch 22/100 loss: 0.2613160200850215\n",
      "Epoch 23/100 loss: 0.27163379145569577\n",
      "Epoch 24/100 loss: 0.22425989174219066\n",
      "Epoch 25/100 loss: 0.26578477820987967\n",
      "Epoch 26/100 loss: 0.2259029916942466\n",
      "Epoch 27/100 loss: 0.20857289953287259\n",
      "Epoch 28/100 loss: 0.2045137111184209\n",
      "Epoch 29/100 loss: 0.20278227632475454\n",
      "Epoch 30/100 loss: 0.20289249250362085\n",
      "Epoch 31/100 loss: 0.2388489660482074\n",
      "Epoch 32/100 loss: 0.19189588249076245\n",
      "Epoch 33/100 loss: 0.24388594740222014\n",
      "Epoch 34/100 loss: 0.38608070426122393\n",
      "Epoch 35/100 loss: 0.2392370104573147\n",
      "Epoch 36/100 loss: 0.2686873806510554\n",
      "Epoch 37/100 loss: 0.22982984550600483\n",
      "Epoch 38/100 loss: 0.35326690097924235\n",
      "Epoch 39/100 loss: 0.41073521430235965\n",
      "Epoch 40/100 loss: 0.23598100701996752\n",
      "Epoch 41/100 loss: 0.24683137902946667\n",
      "Epoch 42/100 loss: 0.1795020456274235\n",
      "Epoch 43/100 loss: 0.1740408208611053\n",
      "Epoch 44/100 loss: 0.20017181520978378\n",
      "Epoch 45/100 loss: 0.19187140605564035\n",
      "Epoch 46/100 loss: 0.16739080789997135\n",
      "Epoch 47/100 loss: 0.2203508538172342\n",
      "Epoch 48/100 loss: 0.20408502410079332\n",
      "Epoch 49/100 loss: 0.1718868647463793\n",
      "Epoch 50/100 loss: 0.18399150572110748\n",
      "Epoch 51/100 loss: 0.18034590434196385\n",
      "Epoch 52/100 loss: 0.1843583179572814\n",
      "Epoch 53/100 loss: 0.17364782880082033\n",
      "Epoch 54/100 loss: 0.1957476897230155\n",
      "Epoch 55/100 loss: 0.18712427437868576\n",
      "Epoch 56/100 loss: 0.18083897696504758\n",
      "Epoch 57/100 loss: 0.19333971335097802\n",
      "Epoch 58/100 loss: 0.190994520747471\n",
      "Epoch 59/100 loss: 0.16327973806061025\n",
      "Epoch 60/100 loss: 0.1940933725536736\n",
      "Epoch 61/100 loss: 0.15603080554323834\n",
      "Epoch 62/100 loss: 0.21006038954730558\n",
      "Epoch 63/100 loss: 0.14879655970130548\n",
      "Epoch 64/100 loss: 0.17918689076810382\n",
      "Epoch 65/100 loss: 0.23829650664485472\n",
      "Epoch 66/100 loss: 0.24273295408158108\n",
      "Epoch 67/100 loss: 0.15339786548514006\n",
      "Epoch 68/100 loss: 0.16052241770680561\n",
      "Epoch 69/100 loss: 0.16395934421073666\n",
      "Epoch 70/100 loss: 0.17296975231621153\n",
      "Epoch 71/100 loss: 0.15892597753554583\n",
      "Epoch 72/100 loss: 0.1554348022178855\n",
      "Epoch 73/100 loss: 0.19977604282131894\n",
      "Epoch 74/100 loss: 0.18473049491470636\n",
      "Epoch 75/100 loss: 0.1694030391897053\n",
      "Epoch 76/100 loss: 0.1382003915225437\n",
      "Epoch 77/100 loss: 0.15341744486372486\n",
      "Epoch 78/100 loss: 0.1715977456485636\n",
      "Epoch 79/100 loss: 0.17440481943099997\n",
      "Epoch 80/100 loss: 0.19348150923804835\n",
      "Epoch 81/100 loss: 0.16987438831305088\n",
      "Epoch 82/100 loss: 0.15533627956282609\n",
      "Epoch 83/100 loss: 0.16130495082240465\n",
      "Epoch 84/100 loss: 0.15592765405254308\n",
      "Epoch 85/100 loss: 0.15051184386708016\n",
      "Epoch 86/100 loss: 0.15631995965228523\n",
      "Epoch 87/100 loss: 0.1883719834758965\n",
      "Epoch 88/100 loss: 0.16733699000094\n",
      "Epoch 89/100 loss: 0.18310373757294443\n",
      "Epoch 90/100 loss: 0.1759909560557368\n",
      "Epoch 91/100 loss: 0.15742648159002148\n",
      "Epoch 92/100 loss: 0.1686086965248335\n",
      "Epoch 93/100 loss: 0.1706886213532714\n",
      "Epoch 94/100 loss: 0.18725604280222988\n",
      "Epoch 95/100 loss: 0.1815171796492799\n",
      "Epoch 96/100 loss: 0.166911656231901\n",
      "Epoch 97/100 loss: 0.15680443678527725\n",
      "Epoch 98/100 loss: 0.14504611476995918\n",
      "Epoch 99/100 loss: 0.1535668951394253\n",
      "Epoch 100/100 loss: 0.15444134124879574\n"
     ]
    }
   ],
   "source": [
    "train(classificador, train_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6: Visualização dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(classificador.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-6.7047e-02,  1.2647e-01, -2.6380e-01,  8.0228e-02, -2.1039e-02,\n",
       "           7.3646e-02, -3.8681e-02,  6.4473e-03,  4.6321e-02, -5.0237e-04,\n",
       "           1.5157e-01,  3.8141e-02,  9.7813e-02,  9.5623e-02,  8.0932e-03,\n",
       "          -1.7582e-01,  1.4276e-01, -1.4056e-01,  6.8775e-02,  2.6168e-01,\n",
       "          -3.2357e-01,  4.1729e-02, -2.4658e-01,  6.9130e-02, -2.9534e-01,\n",
       "          -1.2235e-01, -2.5239e-01, -1.6792e-01, -1.5795e-01,  5.0240e-02],\n",
       "         [ 1.1477e-01, -1.5002e-01, -3.9405e-02,  6.5307e-05,  9.1533e-02,\n",
       "           7.2518e-02,  8.4781e-02,  1.1925e-01, -2.8373e-02, -6.8659e-02,\n",
       "           9.2077e-02, -2.1360e-02,  6.9109e-02,  1.2491e-01,  2.4290e-02,\n",
       "          -2.1839e-01,  1.1823e-01, -1.9843e-01,  1.1583e-01,  1.8124e-01,\n",
       "          -3.7764e-01,  2.6968e-02, -2.6733e-01,  1.6276e-01,  4.6862e-02,\n",
       "           7.9380e-02,  2.8672e-02,  4.0244e-02,  1.2518e-01,  1.0407e-01],\n",
       "         [ 4.3195e-02, -2.5811e-01,  1.4246e-01, -7.2937e-02, -1.6527e-01,\n",
       "           1.2222e-01,  3.9138e-02, -2.3809e-01, -1.7673e-01,  1.7223e-01,\n",
       "           9.4839e-02, -7.9147e-03,  1.1745e-01, -2.2494e-01, -6.1517e-03,\n",
       "          -7.3340e-02, -9.9158e-02,  9.2183e-02,  2.6290e-01, -5.6939e-02,\n",
       "          -3.1801e-03, -3.3379e-01,  1.3782e-01, -1.3056e-01, -3.4711e-03,\n",
       "          -9.1616e-02, -3.8217e-02, -2.9595e-01, -2.0149e-01, -1.0746e-01],\n",
       "         [ 8.8821e-02, -6.0318e-02,  3.1275e-01,  1.7184e-01, -2.6762e-01,\n",
       "          -3.3584e-02,  4.5876e-02, -2.8641e-02,  1.1290e-02, -2.3993e-01,\n",
       "          -5.5896e-02,  3.4274e-02,  1.5611e-03, -2.2053e-01, -4.2143e-02,\n",
       "           1.5312e-01, -4.5466e-01,  2.6459e-01, -2.5416e-01, -3.9402e-01,\n",
       "           4.0701e-01, -8.6132e-02,  2.0564e-01, -1.5236e-01, -8.4136e-02,\n",
       "          -2.2127e-02,  4.7320e-02, -1.0612e-01, -4.1908e-02, -4.1474e-01],\n",
       "         [ 1.3501e-02, -4.0778e-02, -3.3750e-04, -1.1867e-01,  9.1017e-03,\n",
       "          -7.5296e-02, -3.2823e-02, -1.5092e-04,  1.4979e-02,  5.9529e-03,\n",
       "           1.1411e-01,  6.1579e-02, -1.2573e-01,  1.8238e-01,  6.6660e-05,\n",
       "           9.7706e-04,  1.6840e-03,  3.8222e-05,  7.2907e-04,  6.0636e-05,\n",
       "           7.3148e-02, -1.2025e-01,  1.3876e-01, -1.3619e-01,  1.4475e-02,\n",
       "           9.2103e-02,  1.2034e-01,  6.9397e-02,  5.1547e-02,  9.3453e-03],\n",
       "         [ 6.0550e-02, -7.5059e-02,  8.6719e-02, -9.9590e-02,  3.3844e-01,\n",
       "          -3.3562e-01,  2.6207e-01,  3.5905e-01,  1.3568e-01,  2.5742e-01,\n",
       "           2.0200e-01,  2.7413e-02,  2.2685e-02, -1.8240e-01, -1.4534e-03,\n",
       "           4.6672e-02, -6.5529e-02, -1.6770e-02, -1.7452e-01, -1.7808e-01,\n",
       "           1.0832e-02, -1.4051e-01,  1.4784e-01, -6.0195e-02,  4.8304e-02,\n",
       "          -2.0371e-01, -1.1830e-01,  1.6421e-01,  2.9488e-01, -3.5432e-01],\n",
       "         [ 1.0992e-01,  5.0214e-02,  1.7151e-01, -5.8091e-02, -2.2453e-02,\n",
       "           8.6194e-02,  1.3873e-02,  2.8137e-02, -6.4671e-03,  4.0278e-01,\n",
       "          -5.2416e-02, -8.8593e-02,  7.6948e-02,  2.0087e-01, -1.5658e-02,\n",
       "          -4.3934e-02, -4.2916e-02,  2.0814e-01,  1.5568e-01, -2.0714e-01,\n",
       "          -1.2041e-02, -2.0320e-01,  2.4245e-01,  3.6520e-02,  1.8116e-02,\n",
       "          -1.1502e-01, -4.2715e-02, -1.6214e-01, -3.7540e-03, -1.5701e-01],\n",
       "         [ 7.7793e-02, -4.3989e-02, -2.6754e-01, -1.4515e-01,  7.8209e-03,\n",
       "           8.8361e-02,  2.9317e-01, -4.9156e-01, -1.9329e-01,  2.9879e-02,\n",
       "          -1.2303e-01,  1.0965e-01, -5.5234e-02, -3.2385e-02,  4.3117e-03,\n",
       "          -2.2615e-01,  1.1543e-01, -1.9467e-02,  2.4877e-01,  1.2911e-01,\n",
       "          -2.6156e-01,  6.5394e-02,  1.4073e-02,  1.4570e-01,  2.7911e-01,\n",
       "          -9.1466e-02, -2.4093e-01, -1.2874e-01, -2.4784e-01, -9.9299e-02],\n",
       "         [ 2.9534e-02, -6.7833e-02, -2.9292e-02, -9.6403e-02, -2.5075e-18,\n",
       "          -5.2963e-19, -9.2906e-29, -5.8607e-39, -3.9808e-02,  6.0646e-25,\n",
       "          -2.7435e-02, -1.2573e-01, -2.7512e-02, -6.9255e-02, -2.3891e-39,\n",
       "           1.6744e-39,  3.4950e-34, -7.5344e-40,  1.4192e-39, -1.1676e-40,\n",
       "           5.9436e-02, -1.1471e-01,  8.1167e-02, -7.8333e-02,  1.2290e-13,\n",
       "           1.1026e-01, -4.4946e-10, -1.9526e-26, -4.9734e-08, -1.4129e-17],\n",
       "         [ 1.2716e-01, -9.6606e-02,  2.8694e-01,  1.6601e-01, -3.0490e-01,\n",
       "          -3.3055e-01, -6.4633e-02, -1.0475e-01,  4.9066e-02,  2.5819e-01,\n",
       "           7.0249e-02, -1.3344e-01,  1.2577e-02, -1.7376e-01, -5.7485e-03,\n",
       "          -2.4566e-01, -1.4370e-01,  3.8815e-01,  5.1748e-01, -2.0229e-02,\n",
       "           2.0967e-01, -9.0606e-02,  1.4338e-02, -1.5380e-01,  2.3413e-01,\n",
       "          -3.5902e-02, -2.1231e-02,  1.0400e-01,  6.2560e-02, -2.0216e-02],\n",
       "         [ 2.7853e-02,  1.6159e-01, -9.6521e-03, -1.1881e-01,  1.4116e-02,\n",
       "           1.5208e-01, -5.7755e-02,  1.6146e-01,  1.1628e-02, -1.8266e-01,\n",
       "           1.5855e-01,  1.2438e-02, -1.4102e-01,  1.0609e-01,  1.3672e-03,\n",
       "           1.7644e-01,  9.5820e-02,  2.0391e-01, -1.8304e-01,  3.7876e-05,\n",
       "          -4.8692e-02,  1.6483e-01,  1.7487e-01,  7.0017e-02, -6.9379e-02,\n",
       "          -1.1804e-01, -8.6535e-02, -2.0486e-01, -8.6340e-02, -4.6421e-02],\n",
       "         [ 1.1218e-01,  2.2193e-03, -1.0309e-01, -1.2595e-01, -7.0416e-39,\n",
       "          -9.3553e-40, -1.5592e-34,  1.6269e-39, -6.1585e-03,  4.7078e-40,\n",
       "          -2.1236e-03, -8.6196e-03, -4.4310e-02, -1.0601e-01, -2.5136e-39,\n",
       "          -5.9850e-39,  4.1170e-02, -2.7112e-39, -1.2288e-39, -6.4282e-39,\n",
       "           8.3844e-03,  4.2835e-02,  9.3997e-02,  6.4559e-02,  1.5098e-36,\n",
       "           1.6260e-03,  3.9202e-10, -4.2908e-03, -1.3149e-19, -4.8595e-39],\n",
       "         [ 7.4144e-02, -1.4809e-02, -6.0388e-02,  1.6538e-02,  2.1062e-02,\n",
       "           3.8204e-01, -3.3811e-01,  2.5637e-02,  1.2803e-02,  4.9826e-02,\n",
       "           2.4000e-01,  7.2982e-02, -8.6779e-02, -8.0673e-02, -3.4704e-04,\n",
       "           6.0820e-03,  8.0768e-03,  1.2242e-01, -8.3847e-02, -3.5987e-01,\n",
       "           2.2570e-01, -1.4122e-01,  1.0777e-01,  1.2284e-01,  9.5424e-03,\n",
       "           1.7132e-01, -1.9748e-01, -4.6376e-02,  4.5086e-02, -1.7439e-01],\n",
       "         [-1.5007e-01, -9.1985e-02,  3.1264e-02,  8.3296e-02,  2.4996e-01,\n",
       "          -2.3224e-01, -9.7256e-02, -4.7672e-02,  4.4018e-02, -3.8151e-01,\n",
       "          -3.6912e-02,  5.6293e-02, -5.9913e-02, -3.3119e-02,  5.1589e-03,\n",
       "           1.1874e-01, -1.5311e-01, -1.4603e-01, -3.1566e-01,  3.7451e-02,\n",
       "          -1.4446e-01, -3.0895e-02,  1.1052e-01,  8.5961e-02,  2.5860e-01,\n",
       "          -1.2972e-01,  8.7094e-02,  1.9532e-01, -1.0291e-01, -2.9762e-02],\n",
       "         [-1.7822e-01, -3.7844e-01, -2.7535e-01,  9.2501e-02,  2.3872e-01,\n",
       "           1.8500e-02, -7.3023e-03, -2.8289e-01, -2.6067e-01,  3.9017e-01,\n",
       "           1.1881e-01,  1.3754e-02, -1.2240e-01,  1.5031e-01,  2.1110e-03,\n",
       "          -2.3711e-01, -5.9796e-02, -1.9282e-01,  3.2608e-01,  4.6188e-03,\n",
       "          -2.2751e-01, -2.1180e-01, -1.3079e-01,  1.1792e-01, -2.1731e-02,\n",
       "          -2.6842e-01, -3.3961e-01,  1.2572e-01,  1.5307e-01, -9.0433e-02],\n",
       "         [ 3.0581e-02, -2.1170e-01,  1.2721e-02,  6.3441e-02, -9.9352e-03,\n",
       "          -1.2652e-02, -1.2286e-01,  2.4372e-01,  1.0769e-01, -1.1544e-01,\n",
       "          -2.8653e-02,  1.1493e-05,  3.2059e-02, -2.5998e-01, -3.1251e-02,\n",
       "           3.6119e-02, -1.9893e-01,  1.2675e-01,  5.1788e-02, -1.6271e-01,\n",
       "           3.4288e-01, -1.5072e-02,  2.4994e-01, -3.4623e-02,  3.6937e-03,\n",
       "           1.0811e-01,  5.1946e-02, -8.9355e-02,  6.9783e-02, -2.0119e-01]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.8851e-01, -4.4215e-01,  1.1075e-01,  4.4364e-01,  1.2411e-02,\n",
       "          9.2622e-02,  3.8603e-01, -2.0032e-01,  4.1769e-03,  5.0906e-01,\n",
       "          1.7345e-01,  1.8578e-07,  2.3364e-01, -1.5114e-01, -2.6121e-01,\n",
       "          3.4378e-01], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 9.0053e-02,  2.1054e-01, -4.5575e-02, -1.5081e-01, -1.1605e-01,\n",
       "          -1.4659e-01,  4.7113e-02,  2.0343e-01,  1.4870e-01, -1.9120e-02,\n",
       "          -1.6337e-01, -1.2577e-01, -7.7386e-02, -3.2097e-02,  6.5872e-02,\n",
       "          -2.0700e-01],\n",
       "         [ 4.7573e-02, -1.9387e-01, -1.4569e-01, -1.5025e-01,  2.7567e-01,\n",
       "          -2.4806e-01,  2.0552e-01, -1.9425e-02,  3.4365e-39,  8.1790e-02,\n",
       "          -3.0753e-02,  7.8878e-03,  1.2137e-01, -2.4114e-01,  9.6395e-02,\n",
       "           1.4073e-02],\n",
       "         [ 4.7877e-02,  4.6476e-02,  3.4307e-02, -8.0660e-02,  2.7790e-02,\n",
       "           2.5554e-01,  1.4765e-01, -1.8934e-02, -1.8650e-01,  1.1208e-01,\n",
       "          -1.0312e-01,  1.9479e-01, -4.4671e-03, -2.5917e-02, -7.6423e-02,\n",
       "           3.7138e-03],\n",
       "         [ 2.2333e-01,  3.2526e-02,  1.0980e-01,  2.3122e-01,  2.5425e-01,\n",
       "          -7.1837e-02, -7.3761e-02,  1.0064e-01, -2.3472e-02,  2.0652e-01,\n",
       "          -8.1729e-02, -1.2428e-01,  1.4642e-01,  2.7107e-02,  8.7929e-02,\n",
       "           2.1478e-01],\n",
       "         [ 1.7566e-01,  2.2281e-01,  3.8303e-02, -2.4343e-01, -2.2805e-01,\n",
       "          -1.4010e-01, -2.1046e-01, -1.9559e-02, -7.8371e-03, -3.6770e-01,\n",
       "          -9.6737e-02, -9.1212e-02, -1.6629e-01,  1.1189e-01,  2.5042e-01,\n",
       "          -4.9377e-02],\n",
       "         [-8.9457e-02,  1.3507e-01, -1.4863e-01, -2.0226e-01,  9.2352e-02,\n",
       "          -2.6069e-01, -1.1561e-02,  2.1167e-01, -5.9378e-04, -2.7679e-02,\n",
       "          -1.4410e-01, -1.9088e-02,  1.0696e-01,  8.5203e-02,  1.0268e-01,\n",
       "           2.2267e-01],\n",
       "         [-1.2049e-02, -1.3234e-02, -3.0445e-04, -1.5551e-02,  3.2579e-39,\n",
       "           3.2716e-02, -1.8339e-02,  1.7345e-02,  6.2195e-39, -2.5983e-02,\n",
       "          -5.3474e-03,  4.3572e-39, -1.6551e-02, -1.6511e-02, -2.1821e-02,\n",
       "          -1.5352e-02],\n",
       "         [ 1.7206e-01,  2.0464e-01,  1.6810e-01, -1.9009e-01, -2.1676e-01,\n",
       "           1.8553e-01,  8.0508e-02,  1.7886e-02, -8.6048e-03, -3.0522e-01,\n",
       "          -7.8265e-02,  1.9449e-01, -1.3913e-01, -1.4298e-02, -1.7963e-01,\n",
       "          -1.2617e-01],\n",
       "         [-2.4776e-02, -2.1986e-02, -2.1740e-02, -1.8166e-02, -4.0730e-40,\n",
       "           4.4174e-06, -1.9426e-02,  6.8559e-39, -6.9030e-39,  7.1663e-08,\n",
       "          -2.0777e-39,  4.3574e-39, -9.3889e-04, -1.8907e-02, -5.4142e-39,\n",
       "          -1.8981e-02],\n",
       "         [ 2.9479e-02, -2.6623e-01,  1.9974e-01,  6.3920e-02, -6.4290e-02,\n",
       "          -9.5489e-02,  7.8283e-02, -2.3343e-03,  6.3336e-39, -2.6168e-01,\n",
       "           2.3557e-01,  1.7701e-39,  9.2351e-02, -2.2418e-02,  1.1910e-01,\n",
       "           1.6901e-01],\n",
       "         [-6.5511e-02, -1.4794e-01, -2.9743e-03,  1.7489e-01,  5.2550e-02,\n",
       "           6.7962e-03,  3.1872e-02,  1.0739e-01,  1.8176e-39,  2.3533e-02,\n",
       "           4.5111e-02,  8.9760e-39,  1.1175e-01, -2.1872e-01, -7.5851e-02,\n",
       "           9.6636e-02],\n",
       "         [ 1.8848e-01,  5.7645e-02, -2.1190e-01,  2.6615e-02, -2.9334e-02,\n",
       "          -3.3817e-02, -2.6880e-01, -6.3718e-02,  1.2307e-01, -2.3841e-02,\n",
       "          -1.2188e-01,  3.2155e-02, -9.3068e-02,  1.8730e-01, -2.2214e-01,\n",
       "          -1.4003e-01],\n",
       "         [-2.1974e-01, -4.9250e-02,  7.4883e-02,  2.6275e-01,  2.1270e-02,\n",
       "           2.4820e-01,  6.1721e-02,  1.1758e-01,  1.7203e-01,  1.9158e-01,\n",
       "          -1.2055e-01,  2.1813e-01,  6.5272e-02,  2.0364e-02, -4.4985e-02,\n",
       "          -5.5374e-02],\n",
       "         [ 4.3372e-02, -1.6594e-01,  5.3485e-02, -3.6117e-02, -1.2146e-01,\n",
       "           9.4079e-02, -1.5753e-02,  7.5033e-02,  7.9484e-02, -1.7332e-01,\n",
       "           2.8514e-01,  1.4926e-01,  6.1813e-02, -7.9571e-02,  4.1635e-02,\n",
       "          -1.6560e-01],\n",
       "         [-7.9491e-02,  1.4291e-01,  5.0270e-02,  2.3348e-01, -4.1763e-02,\n",
       "           8.5774e-02,  6.3540e-02, -1.9568e-01, -1.9062e-01,  1.2294e-02,\n",
       "           1.6548e-01, -1.8815e-01, -1.4119e-01,  1.3705e-01,  1.4150e-01,\n",
       "           1.6720e-01],\n",
       "         [-2.2225e-01, -2.0902e-01,  1.1910e-01,  2.1236e-01,  7.1882e-02,\n",
       "           8.9716e-02,  6.9654e-02,  2.1259e-01,  2.5527e-39,  2.7578e-01,\n",
       "           1.3768e-01, -1.4271e-40, -1.5693e-01,  1.5650e-01,  4.0648e-02,\n",
       "           1.2886e-01]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.8343e-01,  4.4484e-02,  5.6076e-01,  2.5006e-01, -3.9766e-01,\n",
       "         -2.2151e-01, -1.3293e-04, -1.3577e-01,  6.2919e-30, -3.7099e-01,\n",
       "          4.4133e-02, -1.8495e-01,  1.8065e-01, -1.7572e-01,  4.1992e-01,\n",
       "          4.1599e-02], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1122,  0.2684,  0.0739,  0.0909, -0.1418, -0.1336, -0.0055, -0.0427,\n",
       "          -0.0243, -0.0252,  0.0501,  0.0043,  0.0350, -0.1384,  0.0421,  0.1067]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2867], requires_grad=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos0 = params[0]\n",
    "pesos0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 7: Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colocando em modo de avaliação\n",
    "classificador.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_teste = torch.tensor(np.array(previsores_teste), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia obtida no conjunto de treinamento: 88.81%\n"
     ]
    }
   ],
   "source": [
    "previsoes = classificador(previsores_teste)\n",
    "previsoes = np.array(previsoes > 0.5)\n",
    "accuracy = accuracy_score(classe_teste, previsoes)\n",
    "print(f\"Acurácia obtida no conjunto de treinamento: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz = confusion_matrix(classe_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  5],\n",
       "       [11, 78]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(matriz, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2: Classificação binária Brest Cancer com validação cruzada e dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2: Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('./Bases/Bases/entradas_breast.csv')\n",
    "classe = pd.read_csv('./Bases/Bases/saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='0', data=classe, palette='hls', hue='0', legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores = np.array(previsores, dtype=np.float32)\n",
    "classe = np.array(classe, dtype=np.float32).squeeze(1)\n",
    "classe.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3: Classe para estrutura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module): # necessario herdar nn.Module para que haja integração com o skorch\n",
    "    def __init__(self):\n",
    "        super().__init__() # herdando da classe nn.Module\n",
    "\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight)\n",
    "        self.activation0 = nn.ReLU()\n",
    "\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X): # interessante pois posso determinar meu próprio forward\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4: Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
    "                                                   criterion=nn.BCELoss, \n",
    "                                                   optimizer=torch.optim.Adam, \n",
    "                                                   optimizer__weight_decay=0.0001,\n",
    "                                                   lr=0.001, \n",
    "                                                   max_epochs=100, \n",
    "                                                   batch_size=10, \n",
    "                                                   train_split=False, # iremos configurar manualmente a validação cruzada\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5: Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2032\u001b[0m  0.1021\n",
      "      2       37.2032  0.0663\n",
      "      3       37.2032  0.0615\n",
      "      4       37.2032  0.0551\n",
      "      5       37.2032  0.0501\n",
      "      6       37.2032  0.0498\n",
      "      7       37.2032  0.0336\n",
      "      8       37.2032  0.0558\n",
      "      9       37.2032  0.0443\n",
      "     10       37.2032  0.0332\n",
      "     11       37.2032  0.0457\n",
      "     12       37.2032  0.0393\n",
      "     13       37.2032  0.0296\n",
      "     14       37.2032  0.0490\n",
      "     15       37.2032  0.0334\n",
      "     16       37.2032  0.0331\n",
      "     17       37.2032  0.0479\n",
      "     18       37.2032  0.0338\n",
      "     19       37.2032  0.0337\n",
      "     20       37.2032  0.0331\n",
      "     21       37.2032  0.0419\n",
      "     22       37.2032  0.0259\n",
      "     23       37.2032  0.0487\n",
      "     24       37.2032  0.0334\n",
      "     25       37.2032  0.0335\n",
      "     26       37.2032  0.0330\n",
      "     27       37.2032  0.0460\n",
      "     28       37.2032  0.0351\n",
      "     29       37.2032  0.0338\n",
      "     30       37.2032  0.0332\n",
      "     31       37.2032  0.0372\n",
      "     32       37.2032  0.0463\n",
      "     33       37.2032  0.0450\n",
      "     34       37.2032  0.0383\n",
      "     35       37.2032  0.0334\n",
      "     36       37.2032  0.0332\n",
      "     37       37.2032  0.0467\n",
      "     38       37.2032  0.0363\n",
      "     39       37.2032  0.0338\n",
      "     40       37.2032  0.0333\n",
      "     41       37.2032  0.0411\n",
      "     42       37.2032  0.0373\n",
      "     43       37.2032  0.0374\n",
      "     44       37.2032  0.0352\n",
      "     45       37.2032  0.0301\n",
      "     46       37.2032  0.0333\n",
      "     47       37.2032  0.0431\n",
      "     48       37.2032  0.0371\n",
      "     49       37.2032  0.0365\n",
      "     50       37.2032  0.0392\n",
      "     51       37.2032  0.0272\n",
      "     52       37.2032  0.0417\n",
      "     53       37.2032  0.0358\n",
      "     54       37.2032  0.0360\n",
      "     55       37.2032  0.0337\n",
      "     56       37.2032  0.0334\n",
      "     57       37.2032  0.0334\n",
      "     58       37.2032  0.0333\n",
      "     59       37.2032  0.0333\n",
      "     60       37.2032  0.0332\n",
      "     61       37.2032  0.0453\n",
      "     62       37.2032  0.0716\n",
      "     63       37.2032  0.0500\n",
      "     64       37.2032  0.0332\n",
      "     65       37.2032  0.0337\n",
      "     66       37.2032  0.0481\n",
      "     67       37.2032  0.0333\n",
      "     68       37.2032  0.0498\n",
      "     69       37.2032  0.0334\n",
      "     70       37.2032  0.0501\n",
      "     71       37.2032  0.0457\n",
      "     72       37.2032  0.0401\n",
      "     73       37.2032  0.0505\n",
      "     74       37.2032  0.0499\n",
      "     75       37.2032  0.0410\n",
      "     76       37.2032  0.0373\n",
      "     77       37.2032  0.0441\n",
      "     78       37.2032  0.0392\n",
      "     79       37.2032  0.0334\n",
      "     80       37.2032  0.0332\n",
      "     81       37.2032  0.0501\n",
      "     82       37.2032  0.0334\n",
      "     83       37.2032  0.0334\n",
      "     84       37.2032  0.0333\n",
      "     85       37.2032  0.0784\n",
      "     86       37.2032  0.0386\n",
      "     87       37.2032  0.0333\n",
      "     88       37.2032  0.0499\n",
      "     89       37.2032  0.0669\n",
      "     90       37.2032  0.0429\n",
      "     91       37.2032  0.0333\n",
      "     92       37.2032  0.0333\n",
      "     93       37.2032  0.0480\n",
      "     94       37.2032  0.0403\n",
      "     95       37.2032  0.0423\n",
      "     96       37.2032  0.0332\n",
      "     97       37.2032  0.0667\n",
      "     98       37.2032  0.0501\n",
      "     99       37.2032  0.0499\n",
      "    100       37.2032  0.0580\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2032\u001b[0m  0.0466\n",
      "      2       37.2032  0.0482\n",
      "      3       37.2032  0.0502\n",
      "      4       37.2032  0.0469\n",
      "      5       37.2032  0.0429\n",
      "      6       37.2032  0.0467\n",
      "      7       37.2032  0.0488\n",
      "      8       37.2032  0.0336\n",
      "      9       37.2032  0.0336\n",
      "     10       37.2032  0.0498\n",
      "     11       37.2032  0.0499\n",
      "     12       37.2032  0.0332\n",
      "     13       37.2032  0.0343\n",
      "     14       37.2032  0.0463\n",
      "     15       37.2032  0.0342\n",
      "     16       37.2032  0.0501\n",
      "     17       37.2032  0.0482\n",
      "     18       37.2032  0.0335\n",
      "     19       37.2032  0.0498\n",
      "     20       37.2032  0.0334\n",
      "     21       37.2032  0.0597\n",
      "     22       37.2032  0.0383\n",
      "     23       37.2032  0.0672\n",
      "     24       37.2032  0.0479\n",
      "     25       37.2032  0.0498\n",
      "     26       37.2032  0.0574\n",
      "     27       37.2032  0.0597\n",
      "     28       37.2032  0.0613\n",
      "     29       37.2032  0.0527\n",
      "     30       37.2032  0.0483\n",
      "     31       37.2032  0.0668\n",
      "     32       37.2032  0.0512\n",
      "     33       37.2032  0.0504\n",
      "     34       37.2032  0.0502\n",
      "     35       \u001b[36m19.8849\u001b[0m  0.0659\n",
      "     36        \u001b[36m0.6339\u001b[0m  0.1060\n",
      "     37        \u001b[36m0.5861\u001b[0m  0.0950\n",
      "     38        \u001b[36m0.5631\u001b[0m  0.0980\n",
      "     39        \u001b[36m0.5451\u001b[0m  0.1693\n",
      "     40        \u001b[36m0.5229\u001b[0m  0.0928\n",
      "     41        \u001b[36m0.5053\u001b[0m  0.0785\n",
      "     42        \u001b[36m0.4949\u001b[0m  0.0834\n",
      "     43        \u001b[36m0.4865\u001b[0m  0.0787\n",
      "     44        \u001b[36m0.4780\u001b[0m  0.0733\n",
      "     45        \u001b[36m0.4701\u001b[0m  0.0650\n",
      "     46        \u001b[36m0.4605\u001b[0m  0.0875\n",
      "     47        \u001b[36m0.4513\u001b[0m  0.0772\n",
      "     48        \u001b[36m0.4416\u001b[0m  0.0854\n",
      "     49        \u001b[36m0.4330\u001b[0m  0.0644\n",
      "     50        \u001b[36m0.4271\u001b[0m  0.0834\n",
      "     51        \u001b[36m0.4181\u001b[0m  0.0668\n",
      "     52        \u001b[36m0.4094\u001b[0m  0.0704\n",
      "     53        \u001b[36m0.4031\u001b[0m  0.0629\n",
      "     54        \u001b[36m0.3953\u001b[0m  0.0684\n",
      "     55        \u001b[36m0.3877\u001b[0m  0.0724\n",
      "     56        \u001b[36m0.3813\u001b[0m  0.0567\n",
      "     57        \u001b[36m0.3736\u001b[0m  0.0760\n",
      "     58        \u001b[36m0.3664\u001b[0m  0.0553\n",
      "     59        \u001b[36m0.3589\u001b[0m  0.0666\n",
      "     60        \u001b[36m0.3553\u001b[0m  0.0667\n",
      "     61        \u001b[36m0.3447\u001b[0m  0.0665\n",
      "     62        \u001b[36m0.3412\u001b[0m  0.0780\n",
      "     63        \u001b[36m0.3309\u001b[0m  0.0904\n",
      "     64        \u001b[36m0.3265\u001b[0m  0.0696\n",
      "     65        \u001b[36m0.3214\u001b[0m  0.0670\n",
      "     66        \u001b[36m0.3189\u001b[0m  0.0667\n",
      "     67        \u001b[36m0.3097\u001b[0m  0.0665\n",
      "     68        \u001b[36m0.3068\u001b[0m  0.0665\n",
      "     69        \u001b[36m0.3018\u001b[0m  0.0666\n",
      "     70        \u001b[36m0.2984\u001b[0m  0.0669\n",
      "     71        \u001b[36m0.2951\u001b[0m  0.0666\n",
      "     72        \u001b[36m0.2905\u001b[0m  0.0704\n",
      "     73        \u001b[36m0.2867\u001b[0m  0.0629\n",
      "     74        \u001b[36m0.2863\u001b[0m  0.0666\n",
      "     75        \u001b[36m0.2781\u001b[0m  0.0895\n",
      "     76        \u001b[36m0.2762\u001b[0m  0.0749\n",
      "     77        0.2817  0.0671\n",
      "     78        \u001b[36m0.2712\u001b[0m  0.0647\n",
      "     79        \u001b[36m0.2691\u001b[0m  0.0495\n",
      "     80        \u001b[36m0.2650\u001b[0m  0.0752\n",
      "     81        \u001b[36m0.2625\u001b[0m  0.0584\n",
      "     82        \u001b[36m0.2617\u001b[0m  0.0597\n",
      "     83        \u001b[36m0.2500\u001b[0m  0.0538\n",
      "     84        0.2514  0.0545\n",
      "     85        \u001b[36m0.2493\u001b[0m  0.0492\n",
      "     86        \u001b[36m0.2444\u001b[0m  0.0538\n",
      "     87        \u001b[36m0.2422\u001b[0m  0.0534\n",
      "     88        \u001b[36m0.2383\u001b[0m  0.0496\n",
      "     89        \u001b[36m0.2367\u001b[0m  0.0516\n",
      "     90        \u001b[36m0.2338\u001b[0m  0.0452\n",
      "     91        \u001b[36m0.2312\u001b[0m  0.0591\n",
      "     92        \u001b[36m0.2308\u001b[0m  0.0507\n",
      "     93        \u001b[36m0.2267\u001b[0m  0.0511\n",
      "     94        \u001b[36m0.2235\u001b[0m  0.0478\n",
      "     95        0.2249  0.0598\n",
      "     96        \u001b[36m0.2202\u001b[0m  0.0479\n",
      "     97        \u001b[36m0.2192\u001b[0m  0.0458\n",
      "     98        \u001b[36m0.2171\u001b[0m  0.0459\n",
      "     99        \u001b[36m0.2157\u001b[0m  0.0487\n",
      "    100        \u001b[36m0.2144\u001b[0m  0.0513\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3684\u001b[0m  0.0477\n",
      "      2       37.3684  0.0480\n",
      "      3       37.3684  0.0499\n",
      "      4       37.3684  0.0495\n",
      "      5       37.3684  0.0497\n",
      "      6       37.3684  0.0458\n",
      "      7       37.3684  0.0482\n",
      "      8       37.3684  0.0508\n",
      "      9       37.3684  0.0491\n",
      "     10       37.3684  0.0492\n",
      "     11       37.3684  0.0479\n",
      "     12       37.3684  0.0468\n",
      "     13       37.3684  0.0492\n",
      "     14       37.3684  0.0489\n",
      "     15       37.3684  0.0492\n",
      "     16       37.3684  0.0489\n",
      "     17       37.3684  0.0449\n",
      "     18       37.3684  0.0489\n",
      "     19       37.3684  0.0554\n",
      "     20       37.3684  0.0473\n",
      "     21       37.3684  0.0462\n",
      "     22       37.3684  0.0502\n",
      "     23       37.3684  0.0442\n",
      "     24       37.3684  0.0415\n",
      "     25       37.3684  0.0435\n",
      "     26       37.3684  0.0799\n",
      "     27       37.3684  0.0634\n",
      "     28       37.3684  0.0397\n",
      "     29       37.3684  0.0511\n",
      "     30       37.3684  0.0630\n",
      "     31       37.3684  0.0532\n",
      "     32       37.3684  0.0645\n",
      "     33       37.3684  0.0529\n",
      "     34       37.3684  0.0610\n",
      "     35       37.3684  0.0470\n",
      "     36       37.3684  0.0479\n",
      "     37       37.3684  0.0460\n",
      "     38       \u001b[36m20.3921\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.6091\u001b[0m  0.0342\n",
      "     40        \u001b[36m0.6082\u001b[0m  0.0490\n",
      "     41        \u001b[36m0.5793\u001b[0m  0.0497\n",
      "     42        \u001b[36m0.5775\u001b[0m  0.0437\n",
      "     43        \u001b[36m0.5665\u001b[0m  0.0448\n",
      "     44        \u001b[36m0.5607\u001b[0m  0.0429\n",
      "     45        \u001b[36m0.5506\u001b[0m  0.0429\n",
      "     46        \u001b[36m0.5465\u001b[0m  0.0425\n",
      "     47        \u001b[36m0.5425\u001b[0m  0.0387\n",
      "     48        \u001b[36m0.5391\u001b[0m  0.0454\n",
      "     49        \u001b[36m0.5369\u001b[0m  0.0416\n",
      "     50        \u001b[36m0.5347\u001b[0m  0.0433\n",
      "     51        \u001b[36m0.5318\u001b[0m  0.0400\n",
      "     52        \u001b[36m0.5290\u001b[0m  0.0419\n",
      "     53        \u001b[36m0.5271\u001b[0m  0.0423\n",
      "     54        \u001b[36m0.5223\u001b[0m  0.0404\n",
      "     55        \u001b[36m0.5210\u001b[0m  0.0448\n",
      "     56        \u001b[36m0.5198\u001b[0m  0.0368\n",
      "     57        \u001b[36m0.5174\u001b[0m  0.0508\n",
      "     58        \u001b[36m0.5147\u001b[0m  0.0452\n",
      "     59        \u001b[36m0.5141\u001b[0m  0.0396\n",
      "     60        \u001b[36m0.5128\u001b[0m  0.0401\n",
      "     61        \u001b[36m0.5113\u001b[0m  0.0407\n",
      "     62        \u001b[36m0.5100\u001b[0m  0.0422\n",
      "     63        \u001b[36m0.5079\u001b[0m  0.0406\n",
      "     64        0.5097  0.0418\n",
      "     65        0.5089  0.0424\n",
      "     66        \u001b[36m0.5066\u001b[0m  0.0418\n",
      "     67        \u001b[36m0.5051\u001b[0m  0.0444\n",
      "     68        0.5070  0.0365\n",
      "     69        0.5056  0.0504\n",
      "     70        0.5056  0.0433\n",
      "     71        \u001b[36m0.5037\u001b[0m  0.0395\n",
      "     72        0.5049  0.0486\n",
      "     73        \u001b[36m0.5029\u001b[0m  0.0450\n",
      "     74        0.5034  0.0367\n",
      "     75        \u001b[36m0.5026\u001b[0m  0.0486\n",
      "     76        \u001b[36m0.5023\u001b[0m  0.0429\n",
      "     77        0.5031  0.0384\n",
      "     78        \u001b[36m0.5001\u001b[0m  0.0483\n",
      "     79        \u001b[36m0.4999\u001b[0m  0.0422\n",
      "     80        \u001b[36m0.4984\u001b[0m  0.0409\n",
      "     81        0.4992  0.0456\n",
      "     82        0.5012  0.0370\n",
      "     83        0.4993  0.0499\n",
      "     84        \u001b[36m0.4981\u001b[0m  0.0434\n",
      "     85        \u001b[36m0.4978\u001b[0m  0.0406\n",
      "     86        \u001b[36m0.4966\u001b[0m  0.0467\n",
      "     87        0.4993  0.0427\n",
      "     88        0.4967  0.0396\n",
      "     89        \u001b[36m0.4965\u001b[0m  0.0465\n",
      "     90        \u001b[36m0.4962\u001b[0m  0.0357\n",
      "     91        \u001b[36m0.4961\u001b[0m  0.0483\n",
      "     92        0.4993  0.0465\n",
      "     93        \u001b[36m0.4953\u001b[0m  0.0353\n",
      "     94        0.4954  0.0509\n",
      "     95        0.4954  0.0458\n",
      "     96        0.4971  0.0374\n",
      "     97        \u001b[36m0.4926\u001b[0m  0.0528\n",
      "     98        0.4946  0.0476\n",
      "     99        0.4942  0.0461\n",
      "    100        0.4953  0.0428\n"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(classificador_sklearn, previsores, classe, cv=3, scoring='accuracy') # nao necessario converter previsores e classe para tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6976329713171818"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media = resultados.mean()\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08414475904617044"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desvio = resultados.std()\n",
    "desvio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module): # necessario herdar nn.Module para que haja integração com o skorch\n",
    "    def __init__(self):\n",
    "        super().__init__() # herdando da classe nn.Module\n",
    "\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight)\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.droupout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X): # interessante pois posso determinar meu próprio forward\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dropout0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.droupout1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_sklearn_dropout = NeuralNetBinaryClassifier(module=classificador_torch,\n",
    "                                                   criterion=nn.BCELoss, \n",
    "                                                   optimizer=torch.optim.Adam, \n",
    "                                                   optimizer__weight_decay=0.0001,\n",
    "                                                   lr=0.001, \n",
    "                                                   max_epochs=100, \n",
    "                                                   batch_size=10, \n",
    "                                                   train_split=False, # iremos configurar manualmente a validação cruzada\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1094\u001b[0m  0.1488\n",
      "      2       37.1094  0.0893\n",
      "      3       37.1094  0.0807\n",
      "      4       37.1094  0.0758\n",
      "      5       37.1094  0.0725\n",
      "      6       37.1094  0.0725\n",
      "      7       37.1094  0.0951\n",
      "      8       37.1094  0.1108\n",
      "      9       37.1094  0.1685\n",
      "     10       37.1094  0.1002\n",
      "     11       37.1094  0.1131\n",
      "     12       37.1094  0.0937\n",
      "     13       37.1094  0.0902\n",
      "     14       37.1094  0.0906\n",
      "     15       37.1094  0.0757\n",
      "     16       37.1094  0.0811\n",
      "     17       37.1094  0.0835\n",
      "     18       37.1094  0.0733\n",
      "     19       37.1094  0.0725\n",
      "     20       37.1094  0.0770\n",
      "     21       37.1094  0.0755\n",
      "     22       37.1094  0.0701\n",
      "     23       37.1094  0.0726\n",
      "     24       37.1094  0.0716\n",
      "     25       37.1094  0.0703\n",
      "     26       37.1094  0.0730\n",
      "     27       \u001b[36m11.7217\u001b[0m  0.0632\n",
      "     28        \u001b[36m0.5957\u001b[0m  0.0797\n",
      "     29        \u001b[36m0.5416\u001b[0m  0.0709\n",
      "     30        \u001b[36m0.5397\u001b[0m  0.0667\n",
      "     31        \u001b[36m0.5150\u001b[0m  0.0530\n",
      "     32        0.5211  0.0666\n",
      "     33        \u001b[36m0.4752\u001b[0m  0.0833\n",
      "     34        0.4803  0.0631\n",
      "     35        \u001b[36m0.4734\u001b[0m  0.0662\n",
      "     36        \u001b[36m0.4615\u001b[0m  0.0701\n",
      "     37        \u001b[36m0.4391\u001b[0m  0.0811\n",
      "     38        \u001b[36m0.4351\u001b[0m  0.0625\n",
      "     39        \u001b[36m0.4305\u001b[0m  0.0665\n",
      "     40        \u001b[36m0.4287\u001b[0m  0.0712\n",
      "     41        \u001b[36m0.4015\u001b[0m  0.0650\n",
      "     42        \u001b[36m0.3908\u001b[0m  0.0637\n",
      "     43        0.4008  0.0713\n",
      "     44        0.4114  0.0648\n",
      "     45        0.3951  0.0640\n",
      "     46        \u001b[36m0.3746\u001b[0m  0.0711\n",
      "     47        \u001b[36m0.3673\u001b[0m  0.0728\n",
      "     48        0.3834  0.0619\n",
      "     49        0.3724  0.0698\n",
      "     50        \u001b[36m0.3424\u001b[0m  0.0664\n",
      "     51        0.3631  0.0579\n",
      "     52        0.3559  0.0725\n",
      "     53        0.3616  0.0607\n",
      "     54        \u001b[36m0.3362\u001b[0m  0.0669\n",
      "     55        0.3484  0.0727\n",
      "     56        \u001b[36m0.3288\u001b[0m  0.0672\n",
      "     57        \u001b[36m0.3195\u001b[0m  0.0649\n",
      "     58        \u001b[36m0.3136\u001b[0m  0.0643\n",
      "     59        0.3322  0.0620\n",
      "     60        0.3249  0.0531\n",
      "     61        \u001b[36m0.3069\u001b[0m  0.0639\n",
      "     62        0.3486  0.0748\n",
      "     63        0.3224  0.0591\n",
      "     64        0.3193  0.0639\n",
      "     65        0.3344  0.0664\n",
      "     66        0.3119  0.0670\n",
      "     67        \u001b[36m0.2966\u001b[0m  0.0666\n",
      "     68        0.3144  0.0523\n",
      "     69        0.3203  0.0715\n",
      "     70        \u001b[36m0.2954\u001b[0m  0.0679\n",
      "     71        0.2995  0.0583\n",
      "     72        \u001b[36m0.2759\u001b[0m  0.0665\n",
      "     73        \u001b[36m0.2615\u001b[0m  0.0941\n",
      "     74        0.2650  0.1117\n",
      "     75        0.2808  0.0818\n",
      "     76        0.2910  0.0810\n",
      "     77        0.2747  0.0906\n",
      "     78        0.2627  0.1495\n",
      "     79        0.2718  0.0981\n",
      "     80        0.2697  0.0986\n",
      "     81        0.2700  0.1075\n",
      "     82        \u001b[36m0.2547\u001b[0m  0.0971\n",
      "     83        0.2735  0.0971\n",
      "     84        \u001b[36m0.2475\u001b[0m  0.0903\n",
      "     85        0.2580  0.1022\n",
      "     86        0.2591  0.1114\n",
      "     87        0.2868  0.1036\n",
      "     88        0.2789  0.0896\n",
      "     89        0.2695  0.0997\n",
      "     90        \u001b[36m0.2460\u001b[0m  0.0903\n",
      "     91        \u001b[36m0.2405\u001b[0m  0.1200\n",
      "     92        0.2634  0.0874\n",
      "     93        0.2553  0.1053\n",
      "     94        0.2653  0.1268\n",
      "     95        0.2533  0.1386\n",
      "     96        0.2429  0.1264\n",
      "     97        0.2556  0.1121\n",
      "     98        0.2760  0.1060\n",
      "     99        0.2675  0.0899\n",
      "    100        0.2407  0.0953\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1094\u001b[0m  0.0947\n",
      "      2       37.1094  0.0934\n",
      "      3       37.1094  0.0906\n",
      "      4       37.1094  0.1015\n",
      "      5       37.1094  0.0923\n",
      "      6       37.1094  0.0836\n",
      "      7       37.1094  0.0945\n",
      "      8       37.1094  0.0855\n",
      "      9       37.1094  0.0829\n",
      "     10       37.1094  0.0859\n",
      "     11       37.1094  0.0869\n",
      "     12       37.1094  0.0874\n",
      "     13       37.1094  0.0894\n",
      "     14       37.1094  0.0825\n",
      "     15       37.1094  0.0887\n",
      "     16       37.1094  0.0798\n",
      "     17       37.1094  0.1025\n",
      "     18       37.1094  0.0822\n",
      "     19       37.1094  0.0975\n",
      "     20       37.1094  0.0834\n",
      "     21       37.1094  0.0705\n",
      "     22       37.1094  0.0637\n",
      "     23       37.1094  0.0754\n",
      "     24       37.1094  0.0591\n",
      "     25       \u001b[36m20.1782\u001b[0m  0.0744\n",
      "     26        \u001b[36m0.5922\u001b[0m  0.0645\n",
      "     27        \u001b[36m0.5704\u001b[0m  0.0809\n",
      "     28        \u001b[36m0.5702\u001b[0m  0.0907\n",
      "     29        \u001b[36m0.5478\u001b[0m  0.0818\n",
      "     30        \u001b[36m0.5463\u001b[0m  0.0754\n",
      "     31        \u001b[36m0.5212\u001b[0m  0.0734\n",
      "     32        \u001b[36m0.5065\u001b[0m  0.0837\n",
      "     33        \u001b[36m0.4991\u001b[0m  0.0677\n",
      "     34        \u001b[36m0.4918\u001b[0m  0.0822\n",
      "     35        \u001b[36m0.4782\u001b[0m  0.0698\n",
      "     36        \u001b[36m0.4597\u001b[0m  0.1245\n",
      "     37        \u001b[36m0.4501\u001b[0m  0.0723\n",
      "     38        0.4515  0.0881\n",
      "     39        \u001b[36m0.4441\u001b[0m  0.0709\n",
      "     40        \u001b[36m0.4174\u001b[0m  0.0667\n",
      "     41        \u001b[36m0.3782\u001b[0m  0.0798\n",
      "     42        0.3830  0.0700\n",
      "     43        \u001b[36m0.3693\u001b[0m  0.0696\n",
      "     44        \u001b[36m0.3515\u001b[0m  0.0693\n",
      "     45        \u001b[36m0.3323\u001b[0m  0.0686\n",
      "     46        \u001b[36m0.3253\u001b[0m  0.0566\n",
      "     47        \u001b[36m0.2928\u001b[0m  0.0662\n",
      "     48        0.3136  0.0764\n",
      "     49        \u001b[36m0.2640\u001b[0m  0.0578\n",
      "     50        0.3020  0.0634\n",
      "     51        0.2766  0.0831\n",
      "     52        \u001b[36m0.2553\u001b[0m  0.0529\n",
      "     53        0.2875  0.0744\n",
      "     54        0.2909  0.0541\n",
      "     55        \u001b[36m0.2482\u001b[0m  0.0700\n",
      "     56        0.2893  0.0715\n",
      "     57        0.2566  0.0580\n",
      "     58        0.2484  0.0711\n",
      "     59        0.2585  0.0626\n",
      "     60        0.2591  0.0683\n",
      "     61        0.2530  0.0606\n",
      "     62        0.2485  0.0518\n",
      "     63        0.2564  0.0667\n",
      "     64        \u001b[36m0.2416\u001b[0m  0.0666\n",
      "     65        0.2510  0.0667\n",
      "     66        \u001b[36m0.2185\u001b[0m  0.0717\n",
      "     67        0.2584  0.0698\n",
      "     68        0.2291  0.0683\n",
      "     69        0.2363  0.0753\n",
      "     70        0.2363  0.0774\n",
      "     71        \u001b[36m0.1932\u001b[0m  0.0668\n",
      "     72        0.2403  0.0634\n",
      "     73        0.2458  0.0835\n",
      "     74        0.2218  0.0721\n",
      "     75        0.2234  0.0767\n",
      "     76        0.2491  0.0912\n",
      "     77        0.2115  0.0938\n",
      "     78        0.2720  0.0905\n",
      "     79        0.2311  0.0716\n",
      "     80        0.2034  0.0831\n",
      "     81        0.1955  0.0793\n",
      "     82        0.2101  0.1047\n",
      "     83        0.2310  0.0991\n",
      "     84        0.2086  0.1223\n",
      "     85        0.2069  0.1136\n",
      "     86        0.2021  0.1122\n",
      "     87        0.2167  0.0906\n",
      "     88        0.2301  0.1091\n",
      "     89        0.2164  0.0952\n",
      "     90        0.2052  0.0892\n",
      "     91        \u001b[36m0.1894\u001b[0m  0.0979\n",
      "     92        0.2131  0.1134\n",
      "     93        0.2102  0.1039\n",
      "     94        0.1949  0.1087\n",
      "     95        0.2027  0.1263\n",
      "     96        0.2242  0.1035\n",
      "     97        0.2001  0.1334\n",
      "     98        0.2318  0.1298\n",
      "     99        0.2137  0.1072\n",
      "    100        0.1939  0.1186\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.1254\n",
      "      2       37.3047  0.1099\n",
      "      3       37.3047  0.1457\n",
      "      4       37.3047  0.1368\n",
      "      5       37.3047  0.1065\n",
      "      6       37.3047  0.0937\n",
      "      7       37.3047  0.0912\n",
      "      8       37.3047  0.0968\n",
      "      9       37.3047  0.1036\n",
      "     10       37.3047  0.0883\n",
      "     11       37.3047  0.0899\n",
      "     12       37.3047  0.0907\n",
      "     13       37.3047  0.0942\n",
      "     14       37.3047  0.0951\n",
      "     15       37.3047  0.0889\n",
      "     16       37.3047  0.0836\n",
      "     17       37.3047  0.0856\n",
      "     18       37.3047  0.0857\n",
      "     19       37.3047  0.0890\n",
      "     20       37.3047  0.0855\n",
      "     21       37.3047  0.0828\n",
      "     22       37.3047  0.1459\n",
      "     23       37.3047  0.1029\n",
      "     24       37.3047  0.1202\n",
      "     25       37.3047  0.1023\n",
      "     26       \u001b[36m11.2358\u001b[0m  0.0954\n",
      "     27        \u001b[36m0.5961\u001b[0m  0.0996\n",
      "     28        0.6272  0.0942\n",
      "     29        0.6238  0.0887\n",
      "     30        0.6048  0.0899\n",
      "     31        \u001b[36m0.5786\u001b[0m  0.0985\n",
      "     32        \u001b[36m0.5549\u001b[0m  0.1447\n",
      "     33        \u001b[36m0.5421\u001b[0m  0.2018\n",
      "     34        \u001b[36m0.5354\u001b[0m  0.1848\n",
      "     35        \u001b[36m0.5228\u001b[0m  0.1603\n",
      "     36        \u001b[36m0.5115\u001b[0m  0.1801\n",
      "     37        0.5116  0.1692\n",
      "     38        \u001b[36m0.4777\u001b[0m  0.1494\n",
      "     39        \u001b[36m0.4638\u001b[0m  0.1693\n",
      "     40        \u001b[36m0.4613\u001b[0m  0.1605\n",
      "     41        \u001b[36m0.4579\u001b[0m  0.1365\n",
      "     42        \u001b[36m0.4371\u001b[0m  0.1617\n",
      "     43        \u001b[36m0.4236\u001b[0m  0.1642\n",
      "     44        \u001b[36m0.4080\u001b[0m  0.1805\n",
      "     45        \u001b[36m0.4059\u001b[0m  0.1767\n",
      "     46        0.4101  0.1892\n",
      "     47        \u001b[36m0.3963\u001b[0m  0.1780\n",
      "     48        \u001b[36m0.3795\u001b[0m  0.1703\n",
      "     49        \u001b[36m0.3732\u001b[0m  0.1895\n",
      "     50        \u001b[36m0.3716\u001b[0m  0.2087\n",
      "     51        \u001b[36m0.3582\u001b[0m  0.1816\n",
      "     52        \u001b[36m0.3429\u001b[0m  0.1970\n",
      "     53        0.3625  0.2172\n",
      "     54        \u001b[36m0.3351\u001b[0m  0.1968\n",
      "     55        0.3489  0.1988\n",
      "     56        0.3462  0.1956\n",
      "     57        0.3395  0.1911\n",
      "     58        0.3527  0.1782\n",
      "     59        \u001b[36m0.3319\u001b[0m  0.1451\n",
      "     60        \u001b[36m0.3319\u001b[0m  0.1342\n",
      "     61        \u001b[36m0.3028\u001b[0m  0.1985\n",
      "     62        0.3310  0.1797\n",
      "     63        \u001b[36m0.2958\u001b[0m  0.1788\n",
      "     64        0.2973  0.1832\n",
      "     65        \u001b[36m0.2741\u001b[0m  0.1954\n",
      "     66        0.2874  0.1519\n",
      "     67        0.2850  0.1108\n",
      "     68        0.2889  0.0799\n",
      "     69        0.2792  0.0820\n",
      "     70        0.2874  0.0682\n",
      "     71        0.2841  0.0899\n",
      "     72        0.2838  0.1194\n",
      "     73        0.2851  0.0736\n",
      "     74        0.2942  0.0524\n",
      "     75        0.2958  0.0685\n",
      "     76        0.2746  0.0818\n",
      "     77        \u001b[36m0.2546\u001b[0m  0.1294\n",
      "     78        0.2771  0.0789\n",
      "     79        0.2812  0.0944\n",
      "     80        0.2751  0.0598\n",
      "     81        0.2748  0.0600\n",
      "     82        0.2977  0.0645\n",
      "     83        0.2967  0.0529\n",
      "     84        0.2886  0.0689\n",
      "     85        0.2854  0.0658\n",
      "     86        0.2556  0.1019\n",
      "     87        \u001b[36m0.2506\u001b[0m  0.0628\n",
      "     88        \u001b[36m0.2463\u001b[0m  0.0667\n",
      "     89        0.2546  0.0777\n",
      "     90        \u001b[36m0.2423\u001b[0m  0.0518\n",
      "     91        0.2711  0.0663\n",
      "     92        0.2692  0.0641\n",
      "     93        0.2471  0.0505\n",
      "     94        \u001b[36m0.2336\u001b[0m  0.0596\n",
      "     95        0.2589  0.0592\n",
      "     96        0.2602  0.0591\n",
      "     97        0.2701  0.0642\n",
      "     98        0.2540  0.0596\n",
      "     99        0.2659  0.0644\n",
      "    100        0.2547  0.0919\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0679\n",
      "      2       37.3047  0.0524\n",
      "      3       37.3047  0.0554\n",
      "      4       37.3047  0.0578\n",
      "      5       37.3047  0.0588\n",
      "      6       37.3047  0.0565\n",
      "      7       37.3047  0.0495\n",
      "      8       37.3047  0.0667\n",
      "      9       37.3047  0.0556\n",
      "     10       37.3047  0.0538\n",
      "     11       37.3047  0.0669\n",
      "     12       37.3047  0.0523\n",
      "     13       37.3047  0.0531\n",
      "     14       37.3047  0.0580\n",
      "     15       37.3047  0.0700\n",
      "     16       37.3047  0.0530\n",
      "     17       37.3047  0.0524\n",
      "     18       37.3047  0.0433\n",
      "     19       37.3047  0.0629\n",
      "     20       37.3047  0.0591\n",
      "     21       37.3047  0.0561\n",
      "     22       37.3047  0.0610\n",
      "     23       37.3047  0.0595\n",
      "     24       37.3047  0.0513\n",
      "     25       37.3047  0.0549\n",
      "     26       \u001b[36m27.9089\u001b[0m  0.0535\n",
      "     27        \u001b[36m0.6170\u001b[0m  0.0551\n",
      "     28        \u001b[36m0.5615\u001b[0m  0.0541\n",
      "     29        \u001b[36m0.5501\u001b[0m  0.0596\n",
      "     30        \u001b[36m0.5491\u001b[0m  0.0684\n",
      "     31        \u001b[36m0.5296\u001b[0m  0.0497\n",
      "     32        \u001b[36m0.5220\u001b[0m  0.0668\n",
      "     33        \u001b[36m0.4955\u001b[0m  0.0547\n",
      "     34        \u001b[36m0.4796\u001b[0m  0.0499\n",
      "     35        0.5003  0.0470\n",
      "     36        \u001b[36m0.4722\u001b[0m  0.0501\n",
      "     37        0.4907  0.0497\n",
      "     38        \u001b[36m0.4448\u001b[0m  0.0553\n",
      "     39        0.4594  0.0547\n",
      "     40        0.4555  0.0522\n",
      "     41        \u001b[36m0.3978\u001b[0m  0.0555\n",
      "     42        0.4096  0.0524\n",
      "     43        \u001b[36m0.3874\u001b[0m  0.0525\n",
      "     44        0.3890  0.0419\n",
      "     45        \u001b[36m0.3810\u001b[0m  0.0541\n",
      "     46        \u001b[36m0.3806\u001b[0m  0.0513\n",
      "     47        \u001b[36m0.3452\u001b[0m  0.0541\n",
      "     48        0.3728  0.0510\n",
      "     49        0.3845  0.0529\n",
      "     50        0.3507  0.0582\n",
      "     51        0.3507  0.0518\n",
      "     52        \u001b[36m0.3416\u001b[0m  0.0411\n",
      "     53        \u001b[36m0.3326\u001b[0m  0.0516\n",
      "     54        0.3374  0.0624\n",
      "     55        \u001b[36m0.3226\u001b[0m  0.0500\n",
      "     56        \u001b[36m0.3194\u001b[0m  0.0590\n",
      "     57        \u001b[36m0.3079\u001b[0m  0.0437\n",
      "     58        0.3334  0.0542\n",
      "     59        \u001b[36m0.3024\u001b[0m  0.0522\n",
      "     60        0.3054  0.0519\n",
      "     61        \u001b[36m0.2839\u001b[0m  0.0418\n",
      "     62        0.2981  0.0648\n",
      "     63        0.3234  0.0530\n",
      "     64        0.3069  0.0542\n",
      "     65        0.2892  0.0413\n",
      "     66        0.2880  0.0843\n",
      "     67        0.2924  0.0655\n",
      "     68        \u001b[36m0.2623\u001b[0m  0.0499\n",
      "     69        0.2740  0.0511\n",
      "     70        0.2690  0.0701\n",
      "     71        0.3054  0.0604\n",
      "     72        0.2755  0.0541\n",
      "     73        0.2668  0.0564\n",
      "     74        0.2697  0.0571\n",
      "     75        0.2757  0.0556\n",
      "     76        0.2853  0.0529\n",
      "     77        \u001b[36m0.2407\u001b[0m  0.0440\n",
      "     78        0.2576  0.0512\n",
      "     79        \u001b[36m0.2299\u001b[0m  0.0521\n",
      "     80        0.2796  0.0554\n",
      "     81        0.2387  0.0442\n",
      "     82        0.2507  0.0570\n",
      "     83        0.2561  0.0533\n",
      "     84        0.2580  0.0547\n",
      "     85        0.2400  0.0536\n",
      "     86        0.2621  0.0534\n",
      "     87        0.2385  0.0554\n",
      "     88        0.2706  0.0582\n",
      "     89        0.2434  0.0603\n",
      "     90        0.2404  0.0612\n",
      "     91        \u001b[36m0.2207\u001b[0m  0.0614\n",
      "     92        0.2331  0.0561\n",
      "     93        \u001b[36m0.2093\u001b[0m  0.0710\n",
      "     94        0.2342  0.0572\n",
      "     95        \u001b[36m0.2037\u001b[0m  0.0565\n",
      "     96        0.2191  0.0572\n",
      "     97        0.2211  0.0549\n",
      "     98        0.2307  0.0554\n",
      "     99        0.2312  0.0553\n",
      "    100        0.2637  0.0520\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0626\n",
      "      2       37.3047  0.0538\n",
      "      3       37.3047  0.0722\n",
      "      4       37.3047  0.0591\n",
      "      5       37.3047  0.0815\n",
      "      6       37.3047  0.0697\n",
      "      7       37.3047  0.0660\n",
      "      8       37.3047  0.0643\n",
      "      9       37.3047  0.0578\n",
      "     10       37.3047  0.0653\n",
      "     11       37.3047  0.0567\n",
      "     12       37.3047  0.0562\n",
      "     13       37.3047  0.0612\n",
      "     14       37.3047  0.0563\n",
      "     15       37.3047  0.0567\n",
      "     16       37.3047  0.0565\n",
      "     17       37.3047  0.0433\n",
      "     18       37.3047  0.0695\n",
      "     19       37.3047  0.0540\n",
      "     20       37.3047  0.0550\n",
      "     21       37.3047  0.0578\n",
      "     22       37.3047  0.0539\n",
      "     23       37.3047  0.0534\n",
      "     24       37.3047  0.0541\n",
      "     25       37.3047  0.0559\n",
      "     26       \u001b[36m36.7877\u001b[0m  0.0552\n",
      "     27        \u001b[36m3.4690\u001b[0m  0.0545\n",
      "     28        \u001b[36m0.6168\u001b[0m  0.0537\n",
      "     29        \u001b[36m0.6073\u001b[0m  0.0531\n",
      "     30        \u001b[36m0.5923\u001b[0m  0.0536\n",
      "     31        \u001b[36m0.5882\u001b[0m  0.0532\n",
      "     32        \u001b[36m0.5696\u001b[0m  0.0536\n",
      "     33        \u001b[36m0.5553\u001b[0m  0.0571\n",
      "     34        \u001b[36m0.5319\u001b[0m  0.0569\n",
      "     35        \u001b[36m0.5227\u001b[0m  0.0577\n",
      "     36        \u001b[36m0.5106\u001b[0m  0.0600\n",
      "     37        \u001b[36m0.4972\u001b[0m  0.0629\n",
      "     38        \u001b[36m0.4920\u001b[0m  0.0566\n",
      "     39        \u001b[36m0.4713\u001b[0m  0.0749\n",
      "     40        \u001b[36m0.4598\u001b[0m  0.0606\n",
      "     41        \u001b[36m0.4356\u001b[0m  0.0618\n",
      "     42        \u001b[36m0.4269\u001b[0m  0.0519\n",
      "     43        0.4369  0.0666\n",
      "     44        \u001b[36m0.4216\u001b[0m  0.0665\n",
      "     45        0.4307  0.0611\n",
      "     46        \u001b[36m0.4108\u001b[0m  0.0587\n",
      "     47        \u001b[36m0.4030\u001b[0m  0.0558\n",
      "     48        \u001b[36m0.3877\u001b[0m  0.0712\n",
      "     49        0.3917  0.0602\n",
      "     50        0.3896  0.0667\n",
      "     51        \u001b[36m0.3748\u001b[0m  0.0637\n",
      "     52        \u001b[36m0.3711\u001b[0m  0.1024\n",
      "     53        \u001b[36m0.3640\u001b[0m  0.0709\n",
      "     54        \u001b[36m0.3476\u001b[0m  0.0619\n",
      "     55        0.3663  0.0792\n",
      "     56        \u001b[36m0.3365\u001b[0m  0.0682\n",
      "     57        0.3570  0.0714\n",
      "     58        0.3617  0.0746\n",
      "     59        \u001b[36m0.3195\u001b[0m  0.0706\n",
      "     60        \u001b[36m0.2902\u001b[0m  0.0751\n",
      "     61        0.3448  0.0728\n",
      "     62        0.3067  0.0743\n",
      "     63        0.2938  0.0772\n",
      "     64        0.3038  0.0729\n",
      "     65        0.2991  0.0707\n",
      "     66        0.2913  0.0691\n",
      "     67        0.3188  0.0726\n",
      "     68        0.2928  0.0668\n",
      "     69        \u001b[36m0.2879\u001b[0m  0.0833\n",
      "     70        \u001b[36m0.2776\u001b[0m  0.0717\n",
      "     71        \u001b[36m0.2696\u001b[0m  0.0727\n",
      "     72        0.2923  0.0756\n",
      "     73        0.3010  0.0705\n",
      "     74        \u001b[36m0.2488\u001b[0m  0.0715\n",
      "     75        0.2766  0.0706\n",
      "     76        0.2968  0.0631\n",
      "     77        0.2840  0.0803\n",
      "     78        \u001b[36m0.2309\u001b[0m  0.0692\n",
      "     79        0.2541  0.0593\n",
      "     80        0.2828  0.0841\n",
      "     81        0.2759  0.0725\n",
      "     82        0.2398  0.0706\n",
      "     83        0.2768  0.0751\n",
      "     84        0.2416  0.0718\n",
      "     85        0.2638  0.0737\n",
      "     86        0.2617  0.0875\n",
      "     87        0.2547  0.0706\n",
      "     88        0.2608  0.0694\n",
      "     89        0.2580  0.0700\n",
      "     90        0.2507  0.0676\n",
      "     91        0.2427  0.0825\n",
      "     92        0.2434  0.0789\n",
      "     93        0.2409  0.0734\n",
      "     94        0.2642  0.0608\n",
      "     95        0.2578  0.0672\n",
      "     96        0.2471  0.0709\n",
      "     97        \u001b[36m0.2224\u001b[0m  0.0649\n",
      "     98        0.2488  0.0684\n",
      "     99        0.2701  0.0624\n",
      "    100        0.2408  0.0615\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0545\n",
      "      2       37.3047  0.0590\n",
      "      3       37.3047  0.0644\n",
      "      4       37.3047  0.0499\n",
      "      5       37.3047  0.0609\n",
      "      6       37.3047  0.0556\n",
      "      7       37.3047  0.0464\n",
      "      8       37.3047  0.0640\n",
      "      9       37.3047  0.0534\n",
      "     10       37.3047  0.0469\n",
      "     11       37.3047  0.0628\n",
      "     12       37.3047  0.0556\n",
      "     13       37.3047  0.0531\n",
      "     14       37.3047  0.0610\n",
      "     15       37.3047  0.0513\n",
      "     16       37.3047  0.0541\n",
      "     17       37.3047  0.0448\n",
      "     18       37.3047  0.0639\n",
      "     19       37.3047  0.0536\n",
      "     20       37.3047  0.0547\n",
      "     21       37.3047  0.0536\n",
      "     22       37.3047  0.0511\n",
      "     23       37.3047  0.0473\n",
      "     24       37.3047  0.0570\n",
      "     25       37.3047  0.0453\n",
      "     26       37.3047  0.0504\n",
      "     27       37.3047  0.0648\n",
      "     28       \u001b[36m11.5925\u001b[0m  0.0519\n",
      "     29        \u001b[36m0.6130\u001b[0m  0.0551\n",
      "     30        \u001b[36m0.5921\u001b[0m  0.0516\n",
      "     31        \u001b[36m0.5902\u001b[0m  0.0541\n",
      "     32        \u001b[36m0.5680\u001b[0m  0.0538\n",
      "     33        \u001b[36m0.5647\u001b[0m  0.0546\n",
      "     34        \u001b[36m0.5304\u001b[0m  0.0524\n",
      "     35        0.5481  0.0528\n",
      "     36        \u001b[36m0.5123\u001b[0m  0.0858\n",
      "     37        0.5136  0.0613\n",
      "     38        0.5223  0.0609\n",
      "     39        \u001b[36m0.4865\u001b[0m  0.0520\n",
      "     40        \u001b[36m0.4811\u001b[0m  0.0663\n",
      "     41        \u001b[36m0.4718\u001b[0m  0.0588\n",
      "     42        \u001b[36m0.4550\u001b[0m  0.0525\n",
      "     43        0.4618  0.0567\n",
      "     44        \u001b[36m0.4151\u001b[0m  0.0550\n",
      "     45        0.4310  0.0582\n",
      "     46        0.4565  0.0638\n",
      "     47        0.4305  0.0728\n",
      "     48        0.4198  0.0693\n",
      "     49        0.4197  0.0703\n",
      "     50        0.4163  0.0664\n",
      "     51        \u001b[36m0.4140\u001b[0m  0.0668\n",
      "     52        \u001b[36m0.3950\u001b[0m  0.0666\n",
      "     53        0.4090  0.0512\n",
      "     54        0.4258  0.0663\n",
      "     55        \u001b[36m0.3851\u001b[0m  0.0678\n",
      "     56        0.4256  0.0592\n",
      "     57        0.4042  0.0573\n",
      "     58        0.4492  0.0573\n",
      "     59        0.3877  0.0550\n",
      "     60        0.3976  0.0614\n",
      "     61        0.3963  0.0538\n",
      "     62        0.4211  0.0507\n",
      "     63        0.3937  0.0616\n",
      "     64        \u001b[36m0.3780\u001b[0m  0.0539\n",
      "     65        0.4080  0.0564\n",
      "     66        0.3916  0.0544\n",
      "     67        0.3923  0.0563\n",
      "     68        0.4110  0.0539\n",
      "     69        \u001b[36m0.3685\u001b[0m  0.0554\n",
      "     70        0.3977  0.0552\n",
      "     71        0.3966  0.0539\n",
      "     72        0.3974  0.0542\n",
      "     73        0.3923  0.0555\n",
      "     74        0.3836  0.0519\n",
      "     75        0.3908  0.0552\n",
      "     76        \u001b[36m0.3642\u001b[0m  0.0531\n",
      "     77        0.3739  0.0583\n",
      "     78        0.3665  0.0528\n",
      "     79        0.3902  0.0525\n",
      "     80        0.3751  0.0644\n",
      "     81        0.3959  0.0521\n",
      "     82        0.3877  0.0568\n",
      "     83        0.3931  0.0531\n",
      "     84        0.3924  0.0549\n",
      "     85        0.3746  0.0538\n",
      "     86        0.3872  0.0556\n",
      "     87        \u001b[36m0.3605\u001b[0m  0.0551\n",
      "     88        0.3741  0.0545\n",
      "     89        0.3761  0.0408\n",
      "     90        0.3740  0.0530\n",
      "     91        0.3656  0.0567\n",
      "     92        0.3805  0.0540\n",
      "     93        \u001b[36m0.3524\u001b[0m  0.0555\n",
      "     94        0.3863  0.0536\n",
      "     95        0.3968  0.0404\n",
      "     96        \u001b[36m0.3444\u001b[0m  0.0560\n",
      "     97        0.3635  0.0549\n",
      "     98        0.3773  0.0528\n",
      "     99        0.3897  0.0571\n",
      "    100        0.3789  0.0534\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0523\n",
      "      2       37.3047  0.0557\n",
      "      3       37.3047  0.0543\n",
      "      4       37.3047  0.0537\n",
      "      5       37.3047  0.0548\n",
      "      6       37.3047  0.0532\n",
      "      7       37.3047  0.0521\n",
      "      8       37.3047  0.0553\n",
      "      9       37.3047  0.0536\n",
      "     10       37.3047  0.0407\n",
      "     11       37.3047  0.0572\n",
      "     12       37.3047  0.0554\n",
      "     13       37.3047  0.0533\n",
      "     14       37.3047  0.0540\n",
      "     15       37.3047  0.0926\n",
      "     16       37.3047  0.0501\n",
      "     17       37.3047  0.0667\n",
      "     18       37.3047  0.0664\n",
      "     19       37.3047  0.0502\n",
      "     20       37.3047  0.0681\n",
      "     21       37.3047  0.0572\n",
      "     22       37.3047  0.0552\n",
      "     23       37.3047  0.0604\n",
      "     24       37.3047  0.0554\n",
      "     25       37.3047  0.0573\n",
      "     26       37.3047  0.0545\n",
      "     27       \u001b[36m16.7733\u001b[0m  0.0541\n",
      "     28        \u001b[36m0.6321\u001b[0m  0.0549\n",
      "     29        \u001b[36m0.6048\u001b[0m  0.0553\n",
      "     30        \u001b[36m0.5783\u001b[0m  0.0506\n",
      "     31        0.5871  0.0573\n",
      "     32        0.5786  0.0568\n",
      "     33        \u001b[36m0.5772\u001b[0m  0.0587\n",
      "     34        \u001b[36m0.5707\u001b[0m  0.0551\n",
      "     35        \u001b[36m0.5485\u001b[0m  0.0541\n",
      "     36        \u001b[36m0.5441\u001b[0m  0.0559\n",
      "     37        \u001b[36m0.5256\u001b[0m  0.0553\n",
      "     38        \u001b[36m0.5099\u001b[0m  0.0542\n",
      "     39        \u001b[36m0.4862\u001b[0m  0.0532\n",
      "     40        \u001b[36m0.4810\u001b[0m  0.0517\n",
      "     41        0.4841  0.0545\n",
      "     42        \u001b[36m0.4668\u001b[0m  0.0543\n",
      "     43        0.4747  0.0535\n",
      "     44        \u001b[36m0.4618\u001b[0m  0.0550\n",
      "     45        \u001b[36m0.4432\u001b[0m  0.0542\n",
      "     46        0.4684  0.0545\n",
      "     47        0.4570  0.0561\n",
      "     48        \u001b[36m0.4420\u001b[0m  0.0581\n",
      "     49        \u001b[36m0.4187\u001b[0m  0.0506\n",
      "     50        0.4312  0.0584\n",
      "     51        \u001b[36m0.4183\u001b[0m  0.0540\n",
      "     52        0.4403  0.0546\n",
      "     53        0.4291  0.0555\n",
      "     54        0.4413  0.0526\n",
      "     55        0.4211  0.0451\n",
      "     56        0.4231  0.0539\n",
      "     57        0.4485  0.0552\n",
      "     58        0.4203  0.0549\n",
      "     59        0.4327  0.0553\n",
      "     60        0.4311  0.0559\n",
      "     61        \u001b[36m0.4182\u001b[0m  0.0635\n",
      "     62        \u001b[36m0.3958\u001b[0m  0.0548\n",
      "     63        0.4252  0.0664\n",
      "     64        0.4172  0.0667\n",
      "     65        0.4225  0.0668\n",
      "     66        0.4044  0.0497\n",
      "     67        0.4034  0.0700\n",
      "     68        \u001b[36m0.3920\u001b[0m  0.0802\n",
      "     69        0.4195  0.0727\n",
      "     70        \u001b[36m0.3908\u001b[0m  0.0600\n",
      "     71        0.4415  0.0662\n",
      "     72        0.4427  0.0667\n",
      "     73        0.4266  0.0673\n",
      "     74        \u001b[36m0.3744\u001b[0m  0.0645\n",
      "     75        0.4005  0.0530\n",
      "     76        0.4352  0.0638\n",
      "     77        0.3812  0.0668\n",
      "     78        0.4229  0.0664\n",
      "     79        0.4155  0.0665\n",
      "     80        0.3893  0.0668\n",
      "     81        0.3927  0.0666\n",
      "     82        0.3840  0.0670\n",
      "     83        0.4040  0.0668\n",
      "     84        0.3993  0.0651\n",
      "     85        0.3957  0.0669\n",
      "     86        0.3935  0.0694\n",
      "     87        0.4221  0.0675\n",
      "     88        0.4428  0.0687\n",
      "     89        0.4079  0.0814\n",
      "     90        0.3814  0.0665\n",
      "     91        0.4026  0.0666\n",
      "     92        0.4038  0.1010\n",
      "     93        0.4254  0.0829\n",
      "     94        \u001b[36m0.3661\u001b[0m  0.0696\n",
      "     95        0.3888  0.0676\n",
      "     96        \u001b[36m0.3660\u001b[0m  0.0664\n",
      "     97        0.3679  0.0838\n",
      "     98        0.3948  0.0663\n",
      "     99        0.4133  0.0825\n",
      "    100        0.3970  0.0669\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0687\n",
      "      2       37.3047  0.0803\n",
      "      3       37.3047  0.0664\n",
      "      4       37.3047  0.0700\n",
      "      5       37.3047  0.0634\n",
      "      6       37.3047  0.0832\n",
      "      7       37.3047  0.0681\n",
      "      8       37.3047  0.0663\n",
      "      9       37.3047  0.0647\n",
      "     10       37.3047  0.0667\n",
      "     11       37.3047  0.0687\n",
      "     12       37.3047  0.0867\n",
      "     13       37.3047  0.0644\n",
      "     14       37.3047  0.0832\n",
      "     15       37.3047  0.0843\n",
      "     16       37.3047  0.1000\n",
      "     17       37.3047  0.0833\n",
      "     18       37.3047  0.0999\n",
      "     19       37.3047  0.0834\n",
      "     20       37.3047  0.0861\n",
      "     21       37.3047  0.0808\n",
      "     22       \u001b[36m37.1406\u001b[0m  0.0830\n",
      "     23       37.3047  0.0835\n",
      "     24       37.3047  0.0667\n",
      "     25       37.3047  0.0833\n",
      "     26       37.3047  0.0863\n",
      "     27       \u001b[36m36.9520\u001b[0m  0.0837\n",
      "     28       \u001b[36m11.6150\u001b[0m  0.0798\n",
      "     29        \u001b[36m0.6074\u001b[0m  0.0836\n",
      "     30        \u001b[36m0.5963\u001b[0m  0.0833\n",
      "     31        \u001b[36m0.5625\u001b[0m  0.1000\n",
      "     32        0.5665  0.0832\n",
      "     33        \u001b[36m0.5501\u001b[0m  0.0833\n",
      "     34        \u001b[36m0.5251\u001b[0m  0.0878\n",
      "     35        0.5403  0.0789\n",
      "     36        \u001b[36m0.5065\u001b[0m  0.0991\n",
      "     37        0.5439  0.0864\n",
      "     38        \u001b[36m0.5054\u001b[0m  0.0946\n",
      "     39        0.5166  0.0829\n",
      "     40        \u001b[36m0.4854\u001b[0m  0.0667\n",
      "     41        0.5143  0.0722\n",
      "     42        \u001b[36m0.4813\u001b[0m  0.0716\n",
      "     43        0.4966  0.0638\n",
      "     44        \u001b[36m0.4652\u001b[0m  0.0831\n",
      "     45        \u001b[36m0.4526\u001b[0m  0.0725\n",
      "     46        0.4850  0.1001\n",
      "     47        \u001b[36m0.4235\u001b[0m  0.1002\n",
      "     48        0.4431  0.0772\n",
      "     49        \u001b[36m0.4062\u001b[0m  0.0832\n",
      "     50        0.4272  0.0833\n",
      "     51        0.4235  0.0696\n",
      "     52        \u001b[36m0.3974\u001b[0m  0.0637\n",
      "     53        0.4011  0.0667\n",
      "     54        \u001b[36m0.3862\u001b[0m  0.0720\n",
      "     55        0.3989  0.0612\n",
      "     56        0.4025  0.0623\n",
      "     57        \u001b[36m0.3651\u001b[0m  0.0609\n",
      "     58        0.3786  0.0594\n",
      "     59        0.3867  0.0501\n",
      "     60        0.3817  0.0662\n",
      "     61        0.3772  0.0635\n",
      "     62        0.3775  0.0586\n",
      "     63        0.4021  0.0546\n",
      "     64        0.3723  0.0573\n",
      "     65        \u001b[36m0.3505\u001b[0m  0.0540\n",
      "     66        0.3780  0.0536\n",
      "     67        0.3680  0.0564\n",
      "     68        0.3614  0.0537\n",
      "     69        \u001b[36m0.3271\u001b[0m  0.0552\n",
      "     70        0.3337  0.0528\n",
      "     71        0.3309  0.0547\n",
      "     72        0.3404  0.0536\n",
      "     73        0.3355  0.0490\n",
      "     74        0.3365  0.0583\n",
      "     75        0.3562  0.0542\n",
      "     76        0.3328  0.0534\n",
      "     77        \u001b[36m0.3162\u001b[0m  0.0520\n",
      "     78        0.3436  0.0927\n",
      "     79        0.3421  0.0502\n",
      "     80        0.3515  0.0667\n",
      "     81        0.3370  0.0651\n",
      "     82        \u001b[36m0.3111\u001b[0m  0.0531\n",
      "     83        0.3318  0.0632\n",
      "     84        0.3447  0.0615\n",
      "     85        0.3437  0.0532\n",
      "     86        0.3571  0.0609\n",
      "     87        \u001b[36m0.3094\u001b[0m  0.0541\n",
      "     88        \u001b[36m0.2978\u001b[0m  0.0592\n",
      "     89        0.3151  0.0523\n",
      "     90        0.3400  0.0556\n",
      "     91        0.3258  0.0539\n",
      "     92        0.3515  0.0547\n",
      "     93        \u001b[36m0.2957\u001b[0m  0.0569\n",
      "     94        \u001b[36m0.2860\u001b[0m  0.0501\n",
      "     95        0.3542  0.0559\n",
      "     96        0.3078  0.0545\n",
      "     97        0.3030  0.0532\n",
      "     98        0.3183  0.0522\n",
      "     99        0.3450  0.0414\n",
      "    100        0.3256  0.0523\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3047\u001b[0m  0.0505\n",
      "      2       37.3047  0.0509\n",
      "      3       37.3047  0.0526\n",
      "      4       37.3047  0.0517\n",
      "      5       37.3047  0.0558\n",
      "      6       37.3047  0.0549\n",
      "      7       37.3047  0.0554\n",
      "      8       37.3047  0.0541\n",
      "      9       37.3047  0.0549\n",
      "     10       37.3047  0.0522\n",
      "     11       37.3047  0.0544\n",
      "     12       37.3047  0.0517\n",
      "     13       37.3047  0.0544\n",
      "     14       37.3047  0.0501\n",
      "     15       37.3047  0.0638\n",
      "     16       37.3047  0.0548\n",
      "     17       37.3047  0.0534\n",
      "     18       37.3047  0.0540\n",
      "     19       37.3047  0.0548\n",
      "     20       \u001b[36m30.9965\u001b[0m  0.0544\n",
      "     21        \u001b[36m3.3415\u001b[0m  0.0560\n",
      "     22        \u001b[36m0.6214\u001b[0m  0.0535\n",
      "     23        \u001b[36m0.5404\u001b[0m  0.0544\n",
      "     24        0.5422  0.0544\n",
      "     25        \u001b[36m0.5256\u001b[0m  0.0554\n",
      "     26        \u001b[36m0.5050\u001b[0m  0.0550\n",
      "     27        \u001b[36m0.5039\u001b[0m  0.0553\n",
      "     28        \u001b[36m0.4974\u001b[0m  0.0550\n",
      "     29        \u001b[36m0.4857\u001b[0m  0.0559\n",
      "     30        0.5036  0.0537\n",
      "     31        \u001b[36m0.4747\u001b[0m  0.0547\n",
      "     32        \u001b[36m0.4745\u001b[0m  0.0547\n",
      "     33        0.4785  0.0559\n",
      "     34        0.4798  0.0541\n",
      "     35        \u001b[36m0.4426\u001b[0m  0.0534\n",
      "     36        0.4534  0.0562\n",
      "     37        0.4480  0.0541\n",
      "     38        0.4614  0.0570\n",
      "     39        \u001b[36m0.4274\u001b[0m  0.0553\n",
      "     40        0.4318  0.0603\n",
      "     41        0.4483  0.0559\n",
      "     42        \u001b[36m0.4089\u001b[0m  0.0427\n",
      "     43        0.4118  0.0692\n",
      "     44        0.4217  0.0553\n",
      "     45        0.4105  0.0559\n",
      "     46        \u001b[36m0.3956\u001b[0m  0.0497\n",
      "     47        \u001b[36m0.3688\u001b[0m  0.0684\n",
      "     48        0.4131  0.0610\n",
      "     49        0.3822  0.0599\n",
      "     50        0.3744  0.0547\n",
      "     51        0.3762  0.0524\n",
      "     52        \u001b[36m0.3551\u001b[0m  0.0564\n",
      "     53        0.3706  0.0557\n",
      "     54        0.3747  0.0583\n",
      "     55        0.3568  0.0543\n",
      "     56        0.3739  0.0568\n",
      "     57        0.3631  0.0550\n",
      "     58        0.3603  0.0898\n",
      "     59        \u001b[36m0.3323\u001b[0m  0.0498\n",
      "     60        \u001b[36m0.3292\u001b[0m  0.0759\n",
      "     61        0.3299  0.0630\n",
      "     62        \u001b[36m0.3275\u001b[0m  0.0578\n",
      "     63        \u001b[36m0.3272\u001b[0m  0.0502\n",
      "     64        \u001b[36m0.3137\u001b[0m  0.0668\n",
      "     65        \u001b[36m0.3017\u001b[0m  0.0704\n",
      "     66        0.3040  0.0628\n",
      "     67        0.3265  0.0817\n",
      "     68        0.3116  0.0549\n",
      "     69        0.3402  0.0622\n",
      "     70        0.3157  0.0668\n",
      "     71        0.3128  0.0733\n",
      "     72        0.3050  0.0601\n",
      "     73        \u001b[36m0.2981\u001b[0m  0.0666\n",
      "     74        \u001b[36m0.2807\u001b[0m  0.0635\n",
      "     75        0.2891  0.0615\n",
      "     76        0.2952  0.0566\n",
      "     77        0.2997  0.0613\n",
      "     78        \u001b[36m0.2740\u001b[0m  0.0515\n",
      "     79        \u001b[36m0.2673\u001b[0m  0.0665\n",
      "     80        0.2837  0.0590\n",
      "     81        0.2674  0.0578\n",
      "     82        0.3007  0.0584\n",
      "     83        0.2749  0.0501\n",
      "     84        \u001b[36m0.2609\u001b[0m  0.0569\n",
      "     85        0.2775  0.0632\n",
      "     86        \u001b[36m0.2571\u001b[0m  0.0638\n",
      "     87        0.2699  0.0669\n",
      "     88        0.2790  0.0632\n",
      "     89        0.2836  0.0526\n",
      "     90        0.2741  0.0683\n",
      "     91        0.2793  0.0610\n",
      "     92        \u001b[36m0.2504\u001b[0m  0.0570\n",
      "     93        0.2683  0.0607\n",
      "     94        0.2754  0.0600\n",
      "     95        0.2534  0.0581\n",
      "     96        \u001b[36m0.2450\u001b[0m  0.0473\n",
      "     97        0.2635  0.0659\n",
      "     98        0.2503  0.0568\n",
      "     99        0.2538  0.0568\n",
      "    100        \u001b[36m0.2366\u001b[0m  0.0561\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2320\u001b[0m  0.0541\n",
      "      2       37.2320  0.0536\n",
      "      3       37.2320  0.0574\n",
      "      4       37.2320  0.0553\n",
      "      5       37.2320  0.0533\n",
      "      6       37.2320  0.0539\n",
      "      7       37.2320  0.0547\n",
      "      8       37.2320  0.0567\n",
      "      9       37.2320  0.0542\n",
      "     10       37.2320  0.0530\n",
      "     11       37.2320  0.0638\n",
      "     12       37.2320  0.0583\n",
      "     13       37.2320  0.0534\n",
      "     14       37.2320  0.0544\n",
      "     15       37.2320  0.0529\n",
      "     16       37.2320  0.0553\n",
      "     17       37.2320  0.0524\n",
      "     18       37.2320  0.0547\n",
      "     19       37.2320  0.0530\n",
      "     20       37.2320  0.0560\n",
      "     21       37.2320  0.0532\n",
      "     22       37.2320  0.0539\n",
      "     23       37.2320  0.0539\n",
      "     24       37.2320  0.0556\n",
      "     25       37.2320  0.0520\n",
      "     26       \u001b[36m30.8529\u001b[0m  0.0547\n",
      "     27        \u001b[36m0.7238\u001b[0m  0.0399\n",
      "     28        \u001b[36m0.6123\u001b[0m  0.0701\n",
      "     29        \u001b[36m0.5691\u001b[0m  0.0519\n",
      "     30        \u001b[36m0.5278\u001b[0m  0.0536\n",
      "     31        \u001b[36m0.5257\u001b[0m  0.0561\n",
      "     32        0.5555  0.0542\n",
      "     33        \u001b[36m0.5161\u001b[0m  0.0538\n",
      "     34        \u001b[36m0.4789\u001b[0m  0.0521\n",
      "     35        0.5014  0.0542\n",
      "     36        \u001b[36m0.4662\u001b[0m  0.0531\n",
      "     37        0.4777  0.0519\n",
      "     38        \u001b[36m0.4569\u001b[0m  0.0502\n",
      "     39        0.4762  0.0530\n",
      "     40        \u001b[36m0.4201\u001b[0m  0.0536\n",
      "     41        \u001b[36m0.4111\u001b[0m  0.0541\n",
      "     42        \u001b[36m0.3980\u001b[0m  0.0521\n",
      "     43        \u001b[36m0.3871\u001b[0m  0.0564\n",
      "     44        \u001b[36m0.3742\u001b[0m  0.0529\n",
      "     45        0.3817  0.0531\n",
      "     46        \u001b[36m0.3722\u001b[0m  0.0540\n",
      "     47        \u001b[36m0.3359\u001b[0m  0.0542\n",
      "     48        \u001b[36m0.3266\u001b[0m  0.0535\n",
      "     49        0.3484  0.0614\n",
      "     50        0.3345  0.0793\n",
      "     51        \u001b[36m0.3259\u001b[0m  0.0695\n",
      "     52        \u001b[36m0.3203\u001b[0m  0.0636\n",
      "     53        0.3643  0.0666\n",
      "     54        \u001b[36m0.3103\u001b[0m  0.0501\n",
      "     55        \u001b[36m0.3016\u001b[0m  0.0665\n",
      "     56        \u001b[36m0.2977\u001b[0m  0.0501\n",
      "     57        0.3037  0.0667\n",
      "     58        0.3076  0.0591\n",
      "     59        0.2981  0.0545\n",
      "     60        \u001b[36m0.2791\u001b[0m  0.0566\n",
      "     61        \u001b[36m0.2789\u001b[0m  0.0538\n",
      "     62        0.2809  0.0521\n",
      "     63        0.2827  0.0514\n",
      "     64        \u001b[36m0.2665\u001b[0m  0.0587\n",
      "     65        \u001b[36m0.2582\u001b[0m  0.0543\n",
      "     66        0.2817  0.0560\n",
      "     67        \u001b[36m0.2474\u001b[0m  0.0540\n",
      "     68        0.2823  0.0554\n",
      "     69        0.2605  0.0706\n",
      "     70        0.2526  0.0631\n",
      "     71        0.2708  0.0625\n",
      "     72        0.2522  0.0657\n",
      "     73        0.2492  0.0670\n",
      "     74        0.2847  0.0641\n",
      "     75        0.2647  0.0742\n",
      "     76        \u001b[36m0.2344\u001b[0m  0.0690\n",
      "     77        0.2543  0.0707\n",
      "     78        0.2633  0.0728\n",
      "     79        0.2622  0.0674\n",
      "     80        0.2485  0.0698\n",
      "     81        0.2443  0.0715\n",
      "     82        0.2476  0.0725\n",
      "     83        0.3032  0.0788\n",
      "     84        \u001b[36m0.2206\u001b[0m  0.0731\n",
      "     85        0.2482  0.0708\n",
      "     86        0.2554  0.0724\n",
      "     87        0.2622  0.0743\n",
      "     88        0.2379  0.0764\n",
      "     89        0.2629  0.0735\n",
      "     90        0.2333  0.0796\n",
      "     91        0.2456  0.0734\n",
      "     92        0.2663  0.0721\n",
      "     93        0.2225  0.0751\n",
      "     94        0.2369  0.0752\n",
      "     95        0.2275  0.0747\n",
      "     96        0.2465  0.0784\n",
      "     97        \u001b[36m0.2119\u001b[0m  0.0769\n",
      "     98        0.2377  0.0682\n",
      "     99        0.2741  0.0821\n",
      "    100        0.2384  0.0660\n"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(classificador_sklearn_dropout, previsores, classe, cv=10, scoring='accuracy') # nao necessario converter previsores e classe para tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84210526, 0.84210526, 0.87719298, 0.96491228, 0.87719298,\n",
       "       0.92982456, 0.87719298, 0.89473684, 0.9122807 , 0.89285714])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8910401002506264"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media = resultados.mean()\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.035768601795661784"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desvio = resultados.std()\n",
    "desvio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 3: Classificação binária Brest Cancer com tuning de parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2: Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "previores = pd.read_csv('./Bases/Bases/entradas_breast.csv')\n",
    "classe = pd.read_csv('./Bases/Bases/saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = np.array(previsores, dtype=np.float32)\n",
    "classe = np.array(classe, dtype=np.float32).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3: Classe para estrutura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module): # necessario herdar nn.Module para que haja integração com o skorch\n",
    "    def __init__(self, activation, neurons, initializer):\n",
    "        super().__init__() # herdando da classe nn.Module\n",
    "\n",
    "        self.dense0 = nn.Linear(30, neurons)\n",
    "        initializer(self.dense0.weight)\n",
    "        self.activation0 = activation\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense1 = nn.Linear(neurons, neurons)\n",
    "        initializer(self.dense1.weight)\n",
    "        self.activation1 = activation\n",
    "        self.droupout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dense2 = nn.Linear(neurons, 1)\n",
    "        initializer(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X): # interessante pois posso determinar meu próprio forward\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dropout0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.droupout1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4: Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
    "                                                  lr = 0.001,\n",
    "                                                  optimizer__weight_decay=0.0001,\n",
    "                                                  train_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5: Tuning dos parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': [30],\n",
    "          'max_epochs': [100],\n",
    "          'optimizer': [torch.optim.Adam],\n",
    "          'criterion': [nn.BCELoss],\n",
    "          'module__neurons': [16], # formato com 'module__' na frente pois são parâmetros do otimizador\n",
    "          'module__activation': [F.relu, F.tanh],\n",
    "          'module__initializer': [nn.init.uniform_, nn.init.normal_]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(classificador_sklearn, params, scoring='accuracy', cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3239\u001b[0m  0.0568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2       37.3239  0.0543\n",
      "      3       37.3239  0.0450\n",
      "      4       37.3239  0.0342\n",
      "      5       37.3239  0.0375\n",
      "      6       37.3239  0.0335\n",
      "      7       37.3239  0.0308\n",
      "      8       37.3239  0.0340\n",
      "      9       37.3239  0.0271\n",
      "     10       37.3239  0.0309\n",
      "     11       37.3239  0.0268\n",
      "     12       37.3239  0.0270\n",
      "     13       37.3239  0.0169\n",
      "     14       37.3239  0.0263\n",
      "     15       37.3239  0.0242\n",
      "     16       37.3239  0.0236\n",
      "     17       37.3239  0.0295\n",
      "     18       37.3239  0.0320\n",
      "     19       37.3239  0.0473\n",
      "     20       37.3239  0.0307\n",
      "     21       37.3239  0.0247\n",
      "     22       37.3239  0.0209\n",
      "     23       37.3239  0.0177\n",
      "     24       37.3239  0.0372\n",
      "     25       37.3239  0.0235\n",
      "     26       37.3239  0.0214\n",
      "     27       37.3239  0.0164\n",
      "     28       37.3239  0.0259\n",
      "     29       37.3239  0.0242\n",
      "     30       37.3239  0.0275\n",
      "     31       37.3239  0.0281\n",
      "     32       37.3239  0.0178\n",
      "     33       37.3239  0.0366\n",
      "     34       37.3239  0.0289\n",
      "     35       37.3239  0.0263\n",
      "     36       37.3239  0.0249\n",
      "     37       37.3239  0.0222\n",
      "     38       37.3239  0.0241\n",
      "     39       37.3239  0.0224\n",
      "     40       37.3239  0.0254\n",
      "     41       37.3239  0.0210\n",
      "     42       37.3239  0.0204\n",
      "     43       37.3239  0.0357\n",
      "     44       37.3239  0.0243\n",
      "     45       37.3239  0.0289\n",
      "     46       37.3239  0.0300\n",
      "     47       37.3239  0.0267\n",
      "     48       37.3239  0.0252\n",
      "     49       37.3239  0.0269\n",
      "     50       37.3239  0.0270\n",
      "     51       37.3239  0.0297\n",
      "     52       37.3239  0.0360\n",
      "     53       37.3239  0.0166\n",
      "     54       37.3239  0.0186\n",
      "     55       37.3239  0.0347\n",
      "     56       37.3239  0.0134\n",
      "     57       37.3239  0.0180\n",
      "     58       37.3239  0.0158\n",
      "     59       37.3239  0.0218\n",
      "     60       37.3239  0.0397\n",
      "     61       37.3239  0.0260\n",
      "     62       37.3239  0.0319\n",
      "     63       37.3239  0.0108\n",
      "     64       37.3239  0.0342\n",
      "     65       37.3239  0.0159\n",
      "     66       37.3239  0.0319\n",
      "     67       37.3239  0.0258\n",
      "     68       37.3239  0.0376\n",
      "     69       37.3239  0.0311\n",
      "     70       37.3239  0.0264\n",
      "     71       37.3239  0.0280\n",
      "     72       37.3239  0.0291\n",
      "     73       37.3239  0.0222\n",
      "     74       37.3239  0.0293\n",
      "     75       37.3239  0.0255\n",
      "     76       37.3239  0.0252\n",
      "     77       37.3239  0.0687\n",
      "     78       37.3239  0.0172\n",
      "     79       37.3239  0.0396\n",
      "     80       37.3239  0.0130\n",
      "     81       37.3239  0.0385\n",
      "     82       37.3239  0.0278\n",
      "     83       37.3239  0.0353\n",
      "     84       37.3239  0.0251\n",
      "     85       37.3239  0.0286\n",
      "     86       37.3239  0.0136\n",
      "     87       37.3239  0.0242\n",
      "     88       37.3239  0.0245\n",
      "     89       37.3239  0.0232\n",
      "     90       37.3239  0.0142\n",
      "     91       37.3239  0.0210\n",
      "     92       \u001b[36m32.9401\u001b[0m  0.0237\n",
      "     93        \u001b[36m1.1945\u001b[0m  0.0233\n",
      "     94        \u001b[36m0.6229\u001b[0m  0.0200\n",
      "     95        \u001b[36m0.6197\u001b[0m  0.0260\n",
      "     96        \u001b[36m0.6179\u001b[0m  0.0203\n",
      "     97        \u001b[36m0.6037\u001b[0m  0.0294\n",
      "     98        \u001b[36m0.5990\u001b[0m  0.0218\n",
      "     99        0.6021  0.0200\n",
      "    100        \u001b[36m0.5979\u001b[0m  0.0268\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2365\u001b[0m  0.0049\n",
      "      2       37.2365  0.0370\n",
      "      3       37.2365  0.0162\n",
      "      4       37.2365  0.0166\n",
      "      5       37.2365  0.0319\n",
      "      6       37.2365  0.0166\n",
      "      7       37.2365  0.0175\n",
      "      8       37.2365  0.0163\n",
      "      9       37.2365  0.0208\n",
      "     10       37.2365  0.0180\n",
      "     11       37.2365  0.0187\n",
      "     12       37.2365  0.0184\n",
      "     13       37.2365  0.0200\n",
      "     14       37.2365  0.0168\n",
      "     15       37.2365  0.0180\n",
      "     16       37.2365  0.0303\n",
      "     17       37.2365  0.0138\n",
      "     18       37.2365  0.0201\n",
      "     19       37.2365  0.0249\n",
      "     20       37.2365  0.0203\n",
      "     21       37.2365  0.0201\n",
      "     22       37.2365  0.0194\n",
      "     23       37.2365  0.0197\n",
      "     24       37.2365  0.0249\n",
      "     25       37.2365  0.0233\n",
      "     26       37.2365  0.0329\n",
      "     27       37.2365  0.0188\n",
      "     28       37.2365  0.0238\n",
      "     29       37.2365  0.0301\n",
      "     30       37.2365  0.0293\n",
      "     31       37.2365  0.0194\n",
      "     32       37.2365  0.0204\n",
      "     33       37.2365  0.0219\n",
      "     34       37.2365  0.0215\n",
      "     35       37.2365  0.0166\n",
      "     36       37.2365  0.0252\n",
      "     37       37.2365  0.0214\n",
      "     38       37.2365  0.0240\n",
      "     39       37.2365  0.0245\n",
      "     40       37.2365  0.0253\n",
      "     41       37.2365  0.0219\n",
      "     42       37.2365  0.0190\n",
      "     43       37.2365  0.0269\n",
      "     44       37.2365  0.0211\n",
      "     45       37.2365  0.0284\n",
      "     46       37.2365  0.0192\n",
      "     47       37.2365  0.0345\n",
      "     48       37.2365  0.0178\n",
      "     49       37.2365  0.0367\n",
      "     50       37.2365  0.0221\n",
      "     51       37.2365  0.0228\n",
      "     52       37.2365  0.0129\n",
      "     53       37.2365  0.0566\n",
      "     54       37.2365  0.0439\n",
      "     55       37.2365  0.0200\n",
      "     56       37.2365  0.0277\n",
      "     57       37.2365  0.0350\n",
      "     58       37.2365  0.0196\n",
      "     59       37.2365  0.0261\n",
      "     60       37.2365  0.0202\n",
      "     61       37.2365  0.0298\n",
      "     62       37.2365  0.0329\n",
      "     63       37.2365  0.0251\n",
      "     64       37.2365  0.0221\n",
      "     65       37.2365  0.0204\n",
      "     66       37.2365  0.0248\n",
      "     67       37.2365  0.0246\n",
      "     68       37.2365  0.0122\n",
      "     69       37.2365  0.0204\n",
      "     70       37.2365  0.0224\n",
      "     71       37.2365  0.0108\n",
      "     72       37.2365  0.0261\n",
      "     73       37.2365  0.0235\n",
      "     74       37.2366  0.0216\n",
      "     75       37.2365  0.0117\n",
      "     76       37.2365  0.0329\n",
      "     77       37.2365  0.0162\n",
      "     78       37.2365  0.0213\n",
      "     79       37.2365  0.0153\n",
      "     80       37.2365  0.0319\n",
      "     81       37.2365  0.0138\n",
      "     82       37.2365  0.0195\n",
      "     83       37.2365  0.0188\n",
      "     84       37.2365  0.0348\n",
      "     85       37.2365  0.0187\n",
      "     86       37.2365  0.0196\n",
      "     87       37.2365  0.0194\n",
      "     88       37.2365  0.0215\n",
      "     89       37.2365  0.0150\n",
      "     90       \u001b[36m33.2563\u001b[0m  0.0197\n",
      "     91        \u001b[36m0.9786\u001b[0m  0.0188\n",
      "     92        \u001b[36m0.6078\u001b[0m  0.0139\n",
      "     93        \u001b[36m0.5841\u001b[0m  0.0236\n",
      "     94        \u001b[36m0.5451\u001b[0m  0.0126\n",
      "     95        \u001b[36m0.5275\u001b[0m  0.0201\n",
      "     96        \u001b[36m0.5153\u001b[0m  0.0129\n",
      "     97        \u001b[36m0.5050\u001b[0m  0.0217\n",
      "     98        0.5310  0.0187\n",
      "     99        \u001b[36m0.4987\u001b[0m  0.0188\n",
      "    100        \u001b[36m0.4900\u001b[0m  0.0188\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2365\u001b[0m  0.0160\n",
      "      2       37.2365  0.0206\n",
      "      3       37.2365  0.0159\n",
      "      4       37.2365  0.0168\n",
      "      5       37.2365  0.0245\n",
      "      6       37.2365  0.0170\n",
      "      7       37.2365  0.0217\n",
      "      8       37.2365  0.0146\n",
      "      9       37.2365  0.0167\n",
      "     10       37.2365  0.0359\n",
      "     11       37.2365  0.0100\n",
      "     12       37.2365  0.0213\n",
      "     13       37.2365  0.0228\n",
      "     14       37.2365  0.0078\n",
      "     15       37.2365  0.0303\n",
      "     16       37.2365  0.0201\n",
      "     17       37.2365  0.0188\n",
      "     18       37.2365  0.0184\n",
      "     19       37.2365  0.0290\n",
      "     20       37.2365  0.0129\n",
      "     21       37.2365  0.0197\n",
      "     22       37.2365  0.0271\n",
      "     23       37.2365  0.0225\n",
      "     24       37.2365  0.0197\n",
      "     25       37.2365  0.0200\n",
      "     26       37.2365  0.0170\n",
      "     27       37.2365  0.0216\n",
      "     28       37.2365  0.0150\n",
      "     29       37.2365  0.0317\n",
      "     30       37.2365  0.0199\n",
      "     31       37.2365  0.0213\n",
      "     32       37.2365  0.0223\n",
      "     33       37.2365  0.0138\n",
      "     34       37.2365  0.0200\n",
      "     35       37.2365  0.0166\n",
      "     36       37.2365  0.0161\n",
      "     37       37.2365  0.0157\n",
      "     38       37.2365  0.0206\n",
      "     39       37.2365  0.0161\n",
      "     40       37.2365  0.0265\n",
      "     41       37.2365  0.0215\n",
      "     42       37.2365  0.0166\n",
      "     43       37.2365  0.0170\n",
      "     44       37.2365  0.0157\n",
      "     45       37.2365  0.0317\n",
      "     46       37.2365  0.0145\n",
      "     47       37.2365  0.0215\n",
      "     48       37.2365  0.0247\n",
      "     49       37.2365  0.0260\n",
      "     50       37.2365  0.0104\n",
      "     51       37.2365  0.0218\n",
      "     52       37.2365  0.0235\n",
      "     53       37.2365  0.0200\n",
      "     54       37.2365  0.0187\n",
      "     55       37.2365  0.0202\n",
      "     56       37.2365  0.0177\n",
      "     57       37.2365  0.0095\n",
      "     58       37.2365  0.0294\n",
      "     59       37.2365  0.0202\n",
      "     60       37.2365  0.0166\n",
      "     61       37.2365  0.0163\n",
      "     62       37.2365  0.0352\n",
      "     63       37.2365  0.0107\n",
      "     64       37.2365  0.0197\n",
      "     65       37.2365  0.0285\n",
      "     66       37.2365  0.0216\n",
      "     67       37.2365  0.0154\n",
      "     68       37.2365  0.0256\n",
      "     69       37.2365  0.0198\n",
      "     70       37.2365  0.0180\n",
      "     71       37.2365  0.0199\n",
      "     72       37.2365  0.0205\n",
      "     73       37.2365  0.0184\n",
      "     74       37.2365  0.0191\n",
      "     75       37.2365  0.0209\n",
      "     76       37.2365  0.0209\n",
      "     77       37.2365  0.0209\n",
      "     78       37.2365  0.0198\n",
      "     79       37.2365  0.0177\n",
      "     80       37.2365  0.0157\n",
      "     81       37.2365  0.0340\n",
      "     82       \u001b[36m26.2238\u001b[0m  0.0116\n",
      "     83        \u001b[36m0.7562\u001b[0m  0.0198\n",
      "     84        \u001b[36m0.5990\u001b[0m  0.0284\n",
      "     85        \u001b[36m0.5701\u001b[0m  0.0221\n",
      "     86        \u001b[36m0.5465\u001b[0m  0.0198\n",
      "     87        \u001b[36m0.5142\u001b[0m  0.0213\n",
      "     88        \u001b[36m0.5046\u001b[0m  0.0214\n",
      "     89        0.5124  0.0534\n",
      "     90        \u001b[36m0.4843\u001b[0m  0.0234\n",
      "     91        0.5086  0.0221\n",
      "     92        0.5016  0.0251\n",
      "     93        \u001b[36m0.4563\u001b[0m  0.0225\n",
      "     94        0.5003  0.0297\n",
      "     95        \u001b[36m0.4190\u001b[0m  0.0174\n",
      "     96        0.4510  0.0271\n",
      "     97        0.4296  0.0220\n",
      "     98        0.4658  0.0148\n",
      "     99        \u001b[36m0.4014\u001b[0m  0.0340\n",
      "    100        0.4188  0.0216\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2365\u001b[0m  0.0155\n",
      "      2       37.2365  0.0173\n",
      "      3       37.2365  0.0318\n",
      "      4       37.2365  0.0194\n",
      "      5       37.2365  0.0146\n",
      "      6       37.2365  0.0325\n",
      "      7       37.2365  0.0182\n",
      "      8       37.2365  0.0232\n",
      "      9       37.2365  0.0202\n",
      "     10       37.2365  0.0222\n",
      "     11       37.2365  0.0197\n",
      "     12       37.2365  0.0230\n",
      "     13       37.2365  0.0200\n",
      "     14       37.2365  0.0249\n",
      "     15       37.2365  0.0218\n",
      "     16       37.2365  0.0144\n",
      "     17       37.2365  0.0201\n",
      "     18       37.2365  0.0162\n",
      "     19       37.2365  0.0341\n",
      "     20       37.2365  0.0115\n",
      "     21       37.2365  0.0200\n",
      "     22       37.2365  0.0273\n",
      "     23       37.2365  0.0194\n",
      "     24       37.2365  0.0188\n",
      "     25       37.2365  0.0216\n",
      "     26       37.2365  0.0249\n",
      "     27       37.2365  0.0151\n",
      "     28       37.2365  0.0200\n",
      "     29       37.2365  0.0339\n",
      "     30       37.2365  0.0115\n",
      "     31       37.2365  0.0256\n",
      "     32       37.2365  0.0179\n",
      "     33       37.2365  0.0082\n",
      "     34       37.2365  0.0311\n",
      "     35       37.2365  0.0187\n",
      "     36       37.2365  0.0168\n",
      "     37       37.2365  0.0158\n",
      "     38       37.2365  0.0335\n",
      "     39       37.2365  0.0153\n",
      "     40       37.2365  0.0175\n",
      "     41       37.2365  0.0169\n",
      "     42       37.2365  0.0304\n",
      "     43       37.2365  0.0150\n",
      "     44       37.2365  0.0218\n",
      "     45       37.2365  0.0179\n",
      "     46       37.2365  0.0177\n",
      "     47       37.2365  0.0029\n",
      "     48       37.2365  0.0203\n",
      "     49       37.2365  0.0191\n",
      "     50       37.2365  0.0144\n",
      "     51       37.2365  0.0308\n",
      "     52       37.2365  0.0149\n",
      "     53       37.2365  0.0198\n",
      "     54       37.2365  0.0233\n",
      "     55       37.2365  0.0087\n",
      "     56       37.2368  0.0160\n",
      "     57       37.2365  0.0206\n",
      "     58       37.2365  0.0162\n",
      "     59       37.2365  0.0317\n",
      "     60       37.2365  0.0139\n",
      "     61       37.2365  0.0204\n",
      "     62       37.2365  0.0291\n",
      "     63       37.2365  0.0219\n",
      "     64       37.2365  0.0094\n",
      "     65       37.2365  0.0196\n",
      "     66       37.2365  0.0160\n",
      "     67       37.2365  0.0202\n",
      "     68       37.2365  0.0180\n",
      "     69       37.2365  0.0323\n",
      "     70       37.2365  0.0116\n",
      "     71       37.2365  0.0198\n",
      "     72       37.2365  0.0305\n",
      "     73       37.2365  0.0220\n",
      "     74       37.2365  0.0144\n",
      "     75       37.2365  0.0340\n",
      "     76       37.2365  0.0118\n",
      "     77       37.2365  0.0204\n",
      "     78       \u001b[36m37.0397\u001b[0m  0.0296\n",
      "     79       \u001b[36m23.7274\u001b[0m  0.0389\n",
      "     80        \u001b[36m6.9877\u001b[0m  0.0358\n",
      "     81        \u001b[36m0.5927\u001b[0m  0.0287\n",
      "     82        0.6134  0.0297\n",
      "     83        \u001b[36m0.5925\u001b[0m  0.0242\n",
      "     84        \u001b[36m0.5912\u001b[0m  0.0255\n",
      "     85        \u001b[36m0.5904\u001b[0m  0.0202\n",
      "     86        \u001b[36m0.5609\u001b[0m  0.0255\n",
      "     87        \u001b[36m0.5455\u001b[0m  0.0204\n",
      "     88        \u001b[36m0.5164\u001b[0m  0.0324\n",
      "     89        0.5452  0.0323\n",
      "     90        0.5543  0.0221\n",
      "     91        0.5340  0.0262\n",
      "     92        0.5259  0.0169\n",
      "     93        0.5227  0.0186\n",
      "     94        0.5390  0.0339\n",
      "     95        0.5174  0.0221\n",
      "     96        0.5380  0.0235\n",
      "     97        \u001b[36m0.5148\u001b[0m  0.0271\n",
      "     98        \u001b[36m0.5048\u001b[0m  0.0232\n",
      "     99        \u001b[36m0.5026\u001b[0m  0.0211\n",
      "    100        0.5027  0.0255\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m50.5779\u001b[0m  0.0368\n",
      "      2       54.1285  0.0208\n",
      "      3       53.7536  0.0258\n",
      "      4       51.9999  0.0199\n",
      "      5       \u001b[36m50.2347\u001b[0m  0.0249\n",
      "      6       \u001b[36m49.7592\u001b[0m  0.0227\n",
      "      7       52.0615  0.0209\n",
      "      8       50.3101  0.0251\n",
      "      9       \u001b[36m47.9949\u001b[0m  0.0193\n",
      "     10       \u001b[36m46.5779\u001b[0m  0.0259\n",
      "     11       47.7729  0.0217\n",
      "     12       49.2628  0.0211\n",
      "     13       50.0000  0.0287\n",
      "     14       47.8528  0.0182\n",
      "     15       47.4178  0.0156\n",
      "     16       \u001b[36m46.0149\u001b[0m  0.0356\n",
      "     17       48.5126  0.0100\n",
      "     18       54.0298  0.0348\n",
      "     19       46.0254  0.0140\n",
      "     20       48.8263  0.0205\n",
      "     21       46.8941  0.0332\n",
      "     22       48.3246  0.0226\n",
      "     23       48.5915  0.0231\n",
      "     24       \u001b[36m44.4591\u001b[0m  0.0182\n",
      "     25       46.8379  0.0206\n",
      "     26       47.1178  0.0244\n",
      "     27       51.4164  0.0223\n",
      "     28       46.6038  0.0272\n",
      "     29       47.6666  0.0224\n",
      "     30       48.6358  0.0095\n",
      "     31       49.0082  0.0338\n",
      "     32       46.4529  0.0193\n",
      "     33       50.5648  0.0251\n",
      "     34       49.2556  0.0226\n",
      "     35       49.3225  0.0198\n",
      "     36       49.6120  0.0270\n",
      "     37       48.4984  0.0204\n",
      "     38       53.7583  0.0297\n",
      "     39       47.4379  0.0210\n",
      "     40       46.8305  0.0180\n",
      "     41       46.7540  0.0229\n",
      "     42       45.6665  0.0093\n",
      "     43       45.7765  0.0360\n",
      "     44       46.2460  0.0203\n",
      "     45       45.0114  0.0261\n",
      "     46       46.6064  0.0207\n",
      "     47       45.8710  0.0300\n",
      "     48       48.8879  0.0218\n",
      "     49       46.5189  0.0079\n",
      "     50       \u001b[36m43.7057\u001b[0m  0.0224\n",
      "     51       48.9427  0.0161\n",
      "     52       48.4195  0.0227\n",
      "     53       45.7308  0.0268\n",
      "     54       44.2106  0.0205\n",
      "     55       49.8804  0.0320\n",
      "     56       51.6243  0.0218\n",
      "     57       49.5833  0.0224\n",
      "     58       47.1831  0.0205\n",
      "     59       46.4056  0.0292\n",
      "     60       51.1737  0.0209\n",
      "     61       45.6916  0.0128\n",
      "     62       45.0788  0.0171\n",
      "     63       53.4626  0.0268\n",
      "     64       49.7537  0.0211\n",
      "     65       45.8844  0.0377\n",
      "     66       46.1240  0.0503\n",
      "     67       52.2215  0.0294\n",
      "     68       53.0155  0.0293\n",
      "     69       49.3590  0.0209\n",
      "     70       50.5149  0.0285\n",
      "     71       47.8833  0.0225\n",
      "     72       45.5555  0.0291\n",
      "     73       47.1499  0.0356\n",
      "     74       \u001b[36m40.9267\u001b[0m  0.0210\n",
      "     75       45.3469  0.0204\n",
      "     76       50.5795  0.0330\n",
      "     77       51.4834  0.0223\n",
      "     78       49.1174  0.0169\n",
      "     79       43.6620  0.0369\n",
      "     80       48.8724  0.0206\n",
      "     81       47.1831  0.0259\n",
      "     82       49.8744  0.0340\n",
      "     83       43.6990  0.0167\n",
      "     84       48.2474  0.0418\n",
      "     85       43.5903  0.0230\n",
      "     86       46.7391  0.0441\n",
      "     87       43.8952  0.0231\n",
      "     88       41.7852  0.0285\n",
      "     89       42.9960  0.0302\n",
      "     90       42.5809  0.0128\n",
      "     91       46.6121  0.0387\n",
      "     92       45.3066  0.0298\n",
      "     93       47.4100  0.0172\n",
      "     94       \u001b[36m40.0031\u001b[0m  0.0373\n",
      "     95       41.1811  0.0256\n",
      "     96       42.7488  0.0235\n",
      "     97       41.0798  0.0247\n",
      "     98       41.2042  0.0281\n",
      "     99       43.5376  0.0198\n",
      "    100       48.9036  0.0368\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m59.2506\u001b[0m  0.0247\n",
      "      2       59.9532  0.0232\n",
      "      3       60.8899  0.0182\n",
      "      4       59.2506  0.0362\n",
      "      5       \u001b[36m59.0164\u001b[0m  0.0303\n",
      "      6       59.9532  0.0157\n",
      "      7       \u001b[36m58.5776\u001b[0m  0.0348\n",
      "      8       \u001b[36m58.3138\u001b[0m  0.0190\n",
      "      9       60.1874  0.0296\n",
      "     10       59.0164  0.0132\n",
      "     11       60.4232  0.0366\n",
      "     12       \u001b[36m57.8472\u001b[0m  0.0268\n",
      "     13       59.5208  0.0184\n",
      "     14       61.7633  0.0307\n",
      "     15       61.4126  0.0222\n",
      "     16       61.1241  0.0270\n",
      "     17       59.0164  0.0182\n",
      "     18       59.5088  0.0353\n",
      "     19       59.4848  0.0198\n",
      "     20       \u001b[36m57.6118\u001b[0m  0.0255\n",
      "     21       59.2506  0.0184\n",
      "     22       59.7209  0.0338\n",
      "     23       \u001b[36m57.3787\u001b[0m  0.0158\n",
      "     24       59.9532  0.0314\n",
      "     25       60.4231  0.0207\n",
      "     26       58.7838  0.0273\n",
      "     27       60.2246  0.0192\n",
      "     28       61.4140  0.0320\n",
      "     29       60.8899  0.0205\n",
      "     30       57.9195  0.0269\n",
      "     31       \u001b[36m55.5873\u001b[0m  0.0195\n",
      "     32       59.7207  0.0361\n",
      "     33       57.3786  0.0161\n",
      "     34       58.5480  0.0274\n",
      "     35       57.8454  0.0199\n",
      "     36       57.1429  0.0367\n",
      "     37       61.5138  0.0234\n",
      "     38       58.7822  0.0202\n",
      "     39       59.8073  0.0265\n",
      "     40       60.4365  0.0235\n",
      "     41       58.3879  0.0215\n",
      "     42       58.1695  0.0079\n",
      "     43       59.8048  0.0384\n",
      "     44       56.2437  0.0223\n",
      "     45       58.3746  0.0236\n",
      "     46       57.8818  0.0170\n",
      "     47       57.6112  0.0324\n",
      "     48       59.7190  0.0633\n",
      "     49       56.1668  0.0395\n",
      "     50       56.9087  0.0272\n",
      "     51       \u001b[36m54.3662\u001b[0m  0.0380\n",
      "     52       55.4196  0.0305\n",
      "     53       \u001b[36m54.3340\u001b[0m  0.0109\n",
      "     54       \u001b[36m53.8293\u001b[0m  0.0225\n",
      "     55       59.1062  0.0233\n",
      "     56       55.9719  0.0278\n",
      "     57       56.7032  0.0410\n",
      "     58       58.5482  0.0249\n",
      "     59       58.1414  0.0208\n",
      "     60       56.6745  0.0222\n",
      "     61       55.0708  0.0310\n",
      "     62       57.2663  0.0346\n",
      "     63       56.2061  0.0267\n",
      "     64       55.7282  0.0258\n",
      "     65       54.2249  0.0274\n",
      "     66       62.2077  0.0282\n",
      "     67       55.9719  0.0178\n",
      "     68       55.1181  0.0386\n",
      "     69       55.7377  0.0268\n",
      "     70       55.7377  0.0182\n",
      "     71       57.3770  0.0372\n",
      "     72       56.4403  0.0235\n",
      "     73       53.8642  0.0223\n",
      "     74       56.7603  0.0275\n",
      "     75       57.9805  0.0231\n",
      "     76       55.0351  0.0249\n",
      "     77       58.5841  0.0208\n",
      "     78       54.9450  0.0272\n",
      "     79       57.2277  0.0207\n",
      "     80       56.9591  0.0269\n",
      "     81       55.3521  0.0223\n",
      "     82       55.5035  0.0331\n",
      "     83       56.3007  0.0274\n",
      "     84       59.3267  0.0197\n",
      "     85       57.1429  0.0354\n",
      "     86       58.5480  0.0185\n",
      "     87       54.5096  0.0177\n",
      "     88       59.0164  0.0344\n",
      "     89       58.4919  0.0205\n",
      "     90       59.4296  0.0352\n",
      "     91       54.0984  0.0103\n",
      "     92       \u001b[36m53.8163\u001b[0m  0.0263\n",
      "     93       55.3971  0.0069\n",
      "     94       \u001b[36m52.2248\u001b[0m  0.0184\n",
      "     95       55.9719  0.0321\n",
      "     96       54.8437  0.0332\n",
      "     97       59.7190  0.0139\n",
      "     98       57.1447  0.0185\n",
      "     99       56.9371  0.0368\n",
      "    100       53.2745  0.0166\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m60.1874\u001b[0m  0.0179\n",
      "      2       60.8899  0.0320\n",
      "      3       61.1241  0.0203\n",
      "      4       62.7635  0.0365\n",
      "      5       60.9783  0.0089\n",
      "      6       61.7711  0.0348\n",
      "      7       62.7635  0.0287\n",
      "      8       60.3066  0.0222\n",
      "      9       60.6557  0.0322\n",
      "     10       60.7944  0.0184\n",
      "     11       61.5925  0.0274\n",
      "     12       61.0944  0.0350\n",
      "     13       61.3583  0.0212\n",
      "     14       62.0609  0.0272\n",
      "     15       \u001b[36m59.0655\u001b[0m  0.0239\n",
      "     16       61.3683  0.0231\n",
      "     17       59.7190  0.0263\n",
      "     18       \u001b[36m58.3141\u001b[0m  0.0179\n",
      "     19       61.0454  0.0314\n",
      "     20       61.3283  0.0248\n",
      "     21       60.9577  0.0230\n",
      "     22       63.9344  0.0181\n",
      "     23       60.8957  0.0361\n",
      "     24       60.9179  0.0176\n",
      "     25       62.0625  0.0174\n",
      "     26       59.9532  0.0184\n",
      "     27       58.9828  0.0405\n",
      "     28       60.6557  0.0240\n",
      "     29       62.2951  0.0187\n",
      "     30       61.2467  0.0342\n",
      "     31       61.2487  0.0520\n",
      "     32       60.4215  0.0271\n",
      "     33       61.6697  0.0320\n",
      "     34       61.8703  0.0345\n",
      "     35       61.7681  0.0273\n",
      "     36       61.1865  0.0254\n",
      "     37       61.1241  0.0299\n",
      "     38       \u001b[36m58.0634\u001b[0m  0.0268\n",
      "     39       \u001b[36m57.6294\u001b[0m  0.0186\n",
      "     40       63.1752  0.0335\n",
      "     41       63.1465  0.0195\n",
      "     42       61.8267  0.0269\n",
      "     43       60.5361  0.0175\n",
      "     44       59.2506  0.0227\n",
      "     45       59.0049  0.0326\n",
      "     46       58.4472  0.0123\n",
      "     47       58.8033  0.0236\n",
      "     48       57.7229  0.0268\n",
      "     49       58.6011  0.0140\n",
      "     50       58.4446  0.0227\n",
      "     51       58.7837  0.0282\n",
      "     52       58.6948  0.0218\n",
      "     53       59.6989  0.0328\n",
      "     54       59.4865  0.0289\n",
      "     55       59.2506  0.0152\n",
      "     56       60.7714  0.0374\n",
      "     57       61.1241  0.0257\n",
      "     58       59.7766  0.0181\n",
      "     59       \u001b[36m57.0984\u001b[0m  0.0246\n",
      "     60       58.4769  0.0205\n",
      "     61       \u001b[36m56.4403\u001b[0m  0.0254\n",
      "     62       61.8267  0.0267\n",
      "     63       60.6557  0.0234\n",
      "     64       60.5064  0.0273\n",
      "     65       59.5991  0.0253\n",
      "     66       58.4040  0.0241\n",
      "     67       61.1242  0.0252\n",
      "     68       59.7521  0.0179\n",
      "     69       60.5936  0.0260\n",
      "     70       59.9532  0.0222\n",
      "     71       56.5134  0.0260\n",
      "     72       59.5495  0.0252\n",
      "     73       \u001b[36m56.1760\u001b[0m  0.0123\n",
      "     74       58.1806  0.0324\n",
      "     75       56.1931  0.0226\n",
      "     76       56.6945  0.0194\n",
      "     77       60.2208  0.0162\n",
      "     78       58.7822  0.0277\n",
      "     79       \u001b[36m52.9274\u001b[0m  0.0203\n",
      "     80       56.3488  0.0204\n",
      "     81       59.9799  0.0215\n",
      "     82       56.5351  0.0200\n",
      "     83       57.6112  0.0206\n",
      "     84       53.6459  0.0237\n",
      "     85       58.7822  0.0210\n",
      "     86       56.6209  0.0207\n",
      "     87       56.4725  0.0199\n",
      "     88       53.5035  0.0056\n",
      "     89       54.9667  0.0366\n",
      "     90       59.8412  0.0131\n",
      "     91       55.7377  0.0269\n",
      "     92       54.2467  0.0203\n",
      "     93       56.0128  0.0185\n",
      "     94       54.2085  0.0148\n",
      "     95       53.1777  0.0292\n",
      "     96       55.7469  0.0231\n",
      "     97       53.0527  0.0180\n",
      "     98       \u001b[36m51.6489\u001b[0m  0.0213\n",
      "     99       \u001b[36m51.0556\u001b[0m  0.0066\n",
      "    100       53.0631  0.0311\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m55.9719\u001b[0m  0.0209\n",
      "      2       \u001b[36m54.5833\u001b[0m  0.0199\n",
      "      3       56.6745  0.0192\n",
      "      4       57.8809  0.0193\n",
      "      5       54.7368  0.0190\n",
      "      6       \u001b[36m54.3326\u001b[0m  0.0177\n",
      "      7       55.5035  0.0078\n",
      "      8       \u001b[36m54.3326\u001b[0m  0.0194\n",
      "      9       56.4403  0.0353\n",
      "     10       54.3326  0.0360\n",
      "     11       55.9719  0.0093\n",
      "     12       55.5041  0.0347\n",
      "     13       \u001b[36m54.2388\u001b[0m  0.0125\n",
      "     14       55.8833  0.0337\n",
      "     15       54.3328  0.0227\n",
      "     16       55.5500  0.0208\n",
      "     17       \u001b[36m51.7564\u001b[0m  0.0236\n",
      "     18       51.9906  0.0205\n",
      "     19       54.1580  0.0207\n",
      "     20       52.8819  0.0224\n",
      "     21       56.6283  0.0204\n",
      "     22       54.8009  0.0209\n",
      "     23       54.5667  0.0237\n",
      "     24       53.5427  0.0270\n",
      "     25       56.4403  0.0212\n",
      "     26       53.7204  0.0191\n",
      "     27       52.2458  0.0168\n",
      "     28       \u001b[36m50.6416\u001b[0m  0.0181\n",
      "     29       54.9943  0.0208\n",
      "     30       53.1616  0.0202\n",
      "     31       52.3106  0.0207\n",
      "     32       52.0093  0.0244\n",
      "     33       \u001b[36m48.5283\u001b[0m  0.0214\n",
      "     34       51.7564  0.0199\n",
      "     35       \u001b[36m48.1886\u001b[0m  0.0211\n",
      "     36       53.3958  0.0221\n",
      "     37       52.2248  0.0202\n",
      "     38       \u001b[36m48.1808\u001b[0m  0.0191\n",
      "     39       53.7771  0.0202\n",
      "     40       50.0726  0.0204\n",
      "     41       48.5592  0.0234\n",
      "     42       51.0539  0.0208\n",
      "     43       51.3445  0.0212\n",
      "     44       49.8510  0.0204\n",
      "     45       48.5286  0.0210\n",
      "     46       49.1803  0.0155\n",
      "     47       \u001b[36m48.0094\u001b[0m  0.0185\n",
      "     48       51.2881  0.0161\n",
      "     49       48.4778  0.0305\n",
      "     50       50.5855  0.0204\n",
      "     51       49.7821  0.0171\n",
      "     52       49.1803  0.0285\n",
      "     53       48.2436  0.0183\n",
      "     54       \u001b[36m47.0726\u001b[0m  0.0216\n",
      "     55       49.4145  0.0201\n",
      "     56       51.2881  0.0187\n",
      "     57       47.9647  0.0199\n",
      "     58       51.0539  0.0119\n",
      "     59       50.9558  0.0165\n",
      "     60       \u001b[36m43.7282\u001b[0m  0.0324\n",
      "     61       50.5855  0.0183\n",
      "     62       51.0229  0.0245\n",
      "     63       47.6930  0.0084\n",
      "     64       50.9148  0.0176\n",
      "     65       48.9461  0.0210\n",
      "     66       48.0094  0.0238\n",
      "     67       48.9461  0.0199\n",
      "     68       45.2094  0.0195\n",
      "     69       \u001b[36m43.5597\u001b[0m  0.0157\n",
      "     70       47.7752  0.0289\n",
      "     71       47.6450  0.0212\n",
      "     72       51.0539  0.0186\n",
      "     73       48.9461  0.0175\n",
      "     74       45.9034  0.0212\n",
      "     75       49.1121  0.0078\n",
      "     76       47.7752  0.0181\n",
      "     77       47.0386  0.0158\n",
      "     78       51.5222  0.0166\n",
      "     79       49.8829  0.0168\n",
      "     80       45.6342  0.0192\n",
      "     81       45.3544  0.0194\n",
      "     82       49.8829  0.0157\n",
      "     83       49.4914  0.0326\n",
      "     84       48.9461  0.0148\n",
      "     85       51.0539  0.0200\n",
      "     86       46.3289  0.0147\n",
      "     87       49.6487  0.0162\n",
      "     88       49.8829  0.0322\n",
      "     89       48.8941  0.0235\n",
      "     90       49.5578  0.0132\n",
      "     91       44.7307  0.0469\n",
      "     92       46.1358  0.0337\n",
      "     93       46.3700  0.0180\n",
      "     94       43.7939  0.0154\n",
      "     95       46.4925  0.0327\n",
      "     96       45.6622  0.0206\n",
      "     97       46.8384  0.0230\n",
      "     98       45.1991  0.0209\n",
      "     99       47.7752  0.0206\n",
      "    100       49.4145  0.0240\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.2809\u001b[0m  0.0735\n",
      "      2        \u001b[36m2.1916\u001b[0m  0.0184\n",
      "      3        \u001b[36m2.1298\u001b[0m  0.0219\n",
      "      4        \u001b[36m2.0148\u001b[0m  0.0204\n",
      "      5        \u001b[36m1.9646\u001b[0m  0.0135\n",
      "      6        \u001b[36m1.9035\u001b[0m  0.0144\n",
      "      7        \u001b[36m1.7989\u001b[0m  0.0335\n",
      "      8        \u001b[36m1.7662\u001b[0m  0.0141\n",
      "      9        \u001b[36m1.6995\u001b[0m  0.0198\n",
      "     10        \u001b[36m1.6283\u001b[0m  0.0156\n",
      "     11        \u001b[36m1.5383\u001b[0m  0.0312\n",
      "     12        \u001b[36m1.4785\u001b[0m  0.0183\n",
      "     13        \u001b[36m1.3666\u001b[0m  0.0167\n",
      "     14        \u001b[36m1.3385\u001b[0m  0.0310\n",
      "     15        \u001b[36m1.2494\u001b[0m  0.0150\n",
      "     16        \u001b[36m1.2169\u001b[0m  0.0180\n",
      "     17        \u001b[36m1.0940\u001b[0m  0.0165\n",
      "     18        \u001b[36m1.0301\u001b[0m  0.0335\n",
      "     19        \u001b[36m0.9250\u001b[0m  0.0185\n",
      "     20        \u001b[36m0.8097\u001b[0m  0.0132\n",
      "     21        \u001b[36m0.7722\u001b[0m  0.0165\n",
      "     22        \u001b[36m0.6938\u001b[0m  0.0310\n",
      "     23        0.7270  0.0153\n",
      "     24        0.7232  0.0209\n",
      "     25        0.7121  0.0304\n",
      "     26        0.7133  0.0176\n",
      "     27        \u001b[36m0.6807\u001b[0m  0.0166\n",
      "     28        0.7220  0.0211\n",
      "     29        0.7112  0.0084\n",
      "     30        0.7161  0.0302\n",
      "     31        0.7127  0.0180\n",
      "     32        0.7061  0.0171\n",
      "     33        0.7165  0.0162\n",
      "     34        0.7223  0.0198\n",
      "     35        0.7213  0.0172\n",
      "     36        0.7127  0.0213\n",
      "     37        0.7063  0.0225\n",
      "     38        0.6949  0.0199\n",
      "     39        0.7003  0.0208\n",
      "     40        \u001b[36m0.6756\u001b[0m  0.0200\n",
      "     41        0.7178  0.0185\n",
      "     42        0.6957  0.0184\n",
      "     43        0.6919  0.0166\n",
      "     44        0.7015  0.0187\n",
      "     45        0.7255  0.0297\n",
      "     46        0.6846  0.0180\n",
      "     47        0.7101  0.0166\n",
      "     48        0.6938  0.0167\n",
      "     49        0.6965  0.0290\n",
      "     50        0.6967  0.0191\n",
      "     51        0.6864  0.0169\n",
      "     52        0.7120  0.0167\n",
      "     53        0.6966  0.0172\n",
      "     54        0.6767  0.0174\n",
      "     55        0.7125  0.0182\n",
      "     56        0.7018  0.0164\n",
      "     57        0.6959  0.0314\n",
      "     58        0.6878  0.0151\n",
      "     59        0.6974  0.0199\n",
      "     60        0.6857  0.0148\n",
      "     61        0.7306  0.0321\n",
      "     62        0.6887  0.0178\n",
      "     63        0.7181  0.0169\n",
      "     64        0.7222  0.0324\n",
      "     65        0.7023  0.0150\n",
      "     66        0.6917  0.0181\n",
      "     67        0.7046  0.0280\n",
      "     68        0.7063  0.0202\n",
      "     69        0.6941  0.0407\n",
      "     70        0.6941  0.0303\n",
      "     71        \u001b[36m0.6700\u001b[0m  0.0078\n",
      "     72        0.6924  0.0249\n",
      "     73        0.6990  0.0214\n",
      "     74        0.7085  0.0204\n",
      "     75        0.6912  0.0261\n",
      "     76        0.6794  0.0220\n",
      "     77        0.7051  0.0138\n",
      "     78        0.7080  0.0178\n",
      "     79        0.6983  0.0322\n",
      "     80        0.7019  0.0190\n",
      "     81        0.7099  0.0305\n",
      "     82        0.7061  0.0230\n",
      "     83        0.7076  0.0188\n",
      "     84        0.7092  0.0075\n",
      "     85        0.6964  0.0226\n",
      "     86        0.6952  0.0120\n",
      "     87        0.6845  0.0311\n",
      "     88        0.6809  0.0199\n",
      "     89        0.6786  0.0220\n",
      "     90        0.6852  0.0217\n",
      "     91        0.6753  0.0153\n",
      "     92        0.6785  0.0178\n",
      "     93        0.7035  0.0172\n",
      "     94        0.6931  0.0343\n",
      "     95        0.6889  0.0126\n",
      "     96        0.6874  0.0182\n",
      "     97        0.6876  0.0295\n",
      "     98        0.6913  0.0189\n",
      "     99        0.6926  0.0167\n",
      "    100        0.6937  0.0320\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.9761\u001b[0m  0.0167\n",
      "      2        \u001b[36m2.8839\u001b[0m  0.0080\n",
      "      3        \u001b[36m2.7644\u001b[0m  0.0352\n",
      "      4        \u001b[36m2.7564\u001b[0m  0.0200\n",
      "      5        \u001b[36m2.6580\u001b[0m  0.0213\n",
      "      6        \u001b[36m2.5868\u001b[0m  0.0220\n",
      "      7        \u001b[36m2.5029\u001b[0m  0.0143\n",
      "      8        \u001b[36m2.3698\u001b[0m  0.0179\n",
      "      9        \u001b[36m2.3172\u001b[0m  0.0294\n",
      "     10        \u001b[36m2.2727\u001b[0m  0.0203\n",
      "     11        \u001b[36m2.2180\u001b[0m  0.0198\n",
      "     12        \u001b[36m2.2104\u001b[0m  0.0211\n",
      "     13        \u001b[36m2.0790\u001b[0m  0.0210\n",
      "     14        \u001b[36m1.9614\u001b[0m  0.0213\n",
      "     15        \u001b[36m1.9378\u001b[0m  0.0131\n",
      "     16        \u001b[36m1.8687\u001b[0m  0.0297\n",
      "     17        \u001b[36m1.7737\u001b[0m  0.0178\n",
      "     18        \u001b[36m1.6689\u001b[0m  0.0209\n",
      "     19        \u001b[36m1.5068\u001b[0m  0.0153\n",
      "     20        \u001b[36m1.4114\u001b[0m  0.0319\n",
      "     21        \u001b[36m1.2033\u001b[0m  0.0175\n",
      "     22        \u001b[36m1.0163\u001b[0m  0.0216\n",
      "     23        \u001b[36m0.8790\u001b[0m  0.0198\n",
      "     24        \u001b[36m0.7518\u001b[0m  0.0227\n",
      "     25        \u001b[36m0.7487\u001b[0m  0.0177\n",
      "     26        \u001b[36m0.7135\u001b[0m  0.0231\n",
      "     27        \u001b[36m0.7125\u001b[0m  0.0227\n",
      "     28        0.7475  0.0167\n",
      "     29        0.7185  0.0205\n",
      "     30        0.7519  0.0231\n",
      "     31        0.7298  0.0179\n",
      "     32        \u001b[36m0.6900\u001b[0m  0.0167\n",
      "     33        0.7275  0.0166\n",
      "     34        0.7323  0.0322\n",
      "     35        0.7511  0.0184\n",
      "     36        0.7710  0.0160\n",
      "     37        0.7259  0.0325\n",
      "     38        0.7386  0.0179\n",
      "     39        0.7170  0.0161\n",
      "     40        0.7065  0.0147\n",
      "     41        0.7268  0.0333\n",
      "     42        0.7421  0.0138\n",
      "     43        0.7341  0.0185\n",
      "     44        0.7420  0.0344\n",
      "     45        0.7033  0.0183\n",
      "     46        0.7213  0.0220\n",
      "     47        0.7036  0.0188\n",
      "     48        0.7279  0.0203\n",
      "     49        0.7395  0.0367\n",
      "     50        0.7514  0.0380\n",
      "     51        0.7318  0.0221\n",
      "     52        0.7206  0.0210\n",
      "     53        0.7508  0.0218\n",
      "     54        0.6908  0.0203\n",
      "     55        0.7121  0.0266\n",
      "     56        0.7383  0.0178\n",
      "     57        0.7137  0.0250\n",
      "     58        0.7201  0.0271\n",
      "     59        0.7394  0.0208\n",
      "     60        0.7567  0.0105\n",
      "     61        0.7083  0.0347\n",
      "     62        0.7363  0.0162\n",
      "     63        0.7374  0.0245\n",
      "     64        0.7542  0.0212\n",
      "     65        0.7049  0.0375\n",
      "     66        0.7225  0.0326\n",
      "     67        0.7099  0.0398\n",
      "     68        0.7214  0.0174\n",
      "     69        0.7148  0.0193\n",
      "     70        0.6960  0.0359\n",
      "     71        \u001b[36m0.6858\u001b[0m  0.0332\n",
      "     72        0.7198  0.0254\n",
      "     73        \u001b[36m0.6755\u001b[0m  0.0198\n",
      "     74        0.7024  0.0308\n",
      "     75        0.6870  0.0259\n",
      "     76        \u001b[36m0.6713\u001b[0m  0.0259\n",
      "     77        0.7065  0.0098\n",
      "     78        0.6903  0.0376\n",
      "     79        0.6987  0.0198\n",
      "     80        0.6998  0.0279\n",
      "     81        0.6740  0.0139\n",
      "     82        0.6766  0.0206\n",
      "     83        0.7067  0.0290\n",
      "     84        0.6765  0.0223\n",
      "     85        0.6774  0.0295\n",
      "     86        0.6873  0.0217\n",
      "     87        0.6959  0.0189\n",
      "     88        0.6861  0.0244\n",
      "     89        0.6988  0.0117\n",
      "     90        0.6868  0.0235\n",
      "     91        \u001b[36m0.6711\u001b[0m  0.0159\n",
      "     92        0.7104  0.0197\n",
      "     93        0.6987  0.0310\n",
      "     94        0.7009  0.0218\n",
      "     95        0.6791  0.0099\n",
      "     96        0.6843  0.0355\n",
      "     97        0.6872  0.0238\n",
      "     98        0.6964  0.0228\n",
      "     99        0.6805  0.0221\n",
      "    100        0.6880  0.0197\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3373\u001b[0m  0.0212\n",
      "      2        \u001b[36m3.3092\u001b[0m  0.0172\n",
      "      3        \u001b[36m3.2200\u001b[0m  0.0270\n",
      "      4        \u001b[36m3.1840\u001b[0m  0.0255\n",
      "      5        \u001b[36m3.0432\u001b[0m  0.0194\n",
      "      6        \u001b[36m2.9898\u001b[0m  0.0220\n",
      "      7        \u001b[36m2.9151\u001b[0m  0.0243\n",
      "      8        \u001b[36m2.8027\u001b[0m  0.0066\n",
      "      9        \u001b[36m2.7045\u001b[0m  0.0327\n",
      "     10        \u001b[36m2.6970\u001b[0m  0.0180\n",
      "     11        \u001b[36m2.5535\u001b[0m  0.0233\n",
      "     12        \u001b[36m2.5340\u001b[0m  0.0102\n",
      "     13        \u001b[36m2.4269\u001b[0m  0.0336\n",
      "     14        \u001b[36m2.3508\u001b[0m  0.0188\n",
      "     15        \u001b[36m2.2604\u001b[0m  0.0324\n",
      "     16        \u001b[36m2.2157\u001b[0m  0.0373\n",
      "     17        \u001b[36m2.0626\u001b[0m  0.0251\n",
      "     18        \u001b[36m1.9761\u001b[0m  0.0254\n",
      "     19        \u001b[36m1.7320\u001b[0m  0.0223\n",
      "     20        \u001b[36m1.4637\u001b[0m  0.0218\n",
      "     21        \u001b[36m1.1259\u001b[0m  0.0247\n",
      "     22        \u001b[36m0.8361\u001b[0m  0.0141\n",
      "     23        \u001b[36m0.7254\u001b[0m  0.0176\n",
      "     24        0.7374  0.0205\n",
      "     25        0.7405  0.0233\n",
      "     26        \u001b[36m0.7004\u001b[0m  0.0208\n",
      "     27        0.7159  0.0225\n",
      "     28        0.7546  0.0205\n",
      "     29        \u001b[36m0.6971\u001b[0m  0.0230\n",
      "     30        0.7011  0.0231\n",
      "     31        0.7114  0.0074\n",
      "     32        0.7178  0.0339\n",
      "     33        0.7026  0.0157\n",
      "     34        0.6975  0.0162\n",
      "     35        0.7063  0.0338\n",
      "     36        0.7019  0.0199\n",
      "     37        0.7090  0.0147\n",
      "     38        0.7074  0.0326\n",
      "     39        0.7141  0.0128\n",
      "     40        0.7207  0.0195\n",
      "     41        \u001b[36m0.6835\u001b[0m  0.0329\n",
      "     42        0.7020  0.0228\n",
      "     43        0.7199  0.0341\n",
      "     44        0.7085  0.0175\n",
      "     45        0.7056  0.0334\n",
      "     46        0.7214  0.0235\n",
      "     47        \u001b[36m0.6812\u001b[0m  0.0310\n",
      "     48        0.7227  0.0114\n",
      "     49        0.7179  0.0388\n",
      "     50        0.6949  0.0203\n",
      "     51        0.6834  0.0280\n",
      "     52        0.7180  0.0126\n",
      "     53        0.6987  0.0358\n",
      "     54        0.7353  0.0207\n",
      "     55        0.7090  0.0289\n",
      "     56        0.6919  0.0123\n",
      "     57        0.6921  0.0213\n",
      "     58        0.7027  0.0160\n",
      "     59        0.7002  0.0166\n",
      "     60        0.6992  0.0202\n",
      "     61        0.7154  0.0315\n",
      "     62        0.6982  0.0211\n",
      "     63        \u001b[36m0.6786\u001b[0m  0.0208\n",
      "     64        0.7061  0.0208\n",
      "     65        0.7112  0.0196\n",
      "     66        0.7026  0.0193\n",
      "     67        0.6861  0.0264\n",
      "     68        0.6919  0.0204\n",
      "     69        0.6853  0.0161\n",
      "     70        0.7059  0.0209\n",
      "     71        0.6817  0.0147\n",
      "     72        0.6920  0.0170\n",
      "     73        0.6937  0.0310\n",
      "     74        0.6962  0.0150\n",
      "     75        \u001b[36m0.6782\u001b[0m  0.0196\n",
      "     76        0.7106  0.0184\n",
      "     77        0.6824  0.0323\n",
      "     78        0.6848  0.0155\n",
      "     79        0.6993  0.0289\n",
      "     80        0.6901  0.0217\n",
      "     81        0.7027  0.0162\n",
      "     82        0.6896  0.0236\n",
      "     83        0.6915  0.0197\n",
      "     84        0.6915  0.0377\n",
      "     85        0.6945  0.0262\n",
      "     86        0.6951  0.0275\n",
      "     87        0.6975  0.0159\n",
      "     88        0.7166  0.0260\n",
      "     89        0.7003  0.0188\n",
      "     90        \u001b[36m0.6767\u001b[0m  0.0345\n",
      "     91        0.7061  0.0209\n",
      "     92        0.6912  0.0228\n",
      "     93        0.6926  0.0202\n",
      "     94        0.7010  0.0340\n",
      "     95        0.6831  0.0248\n",
      "     96        0.6897  0.0282\n",
      "     97        0.6809  0.0286\n",
      "     98        0.6929  0.0232\n",
      "     99        0.6927  0.0227\n",
      "    100        0.7070  0.0305\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3196\u001b[0m  0.0198\n",
      "      2        \u001b[36m3.2624\u001b[0m  0.0286\n",
      "      3        \u001b[36m3.1609\u001b[0m  0.0227\n",
      "      4        \u001b[36m3.0911\u001b[0m  0.0151\n",
      "      5        \u001b[36m3.0108\u001b[0m  0.0305\n",
      "      6        \u001b[36m2.9642\u001b[0m  0.0226\n",
      "      7        \u001b[36m2.9036\u001b[0m  0.0252\n",
      "      8        \u001b[36m2.8658\u001b[0m  0.0254\n",
      "      9        \u001b[36m2.7239\u001b[0m  0.0269\n",
      "     10        \u001b[36m2.6261\u001b[0m  0.0114\n",
      "     11        \u001b[36m2.5992\u001b[0m  0.0379\n",
      "     12        \u001b[36m2.4761\u001b[0m  0.0109\n",
      "     13        \u001b[36m2.4122\u001b[0m  0.0340\n",
      "     14        \u001b[36m2.3308\u001b[0m  0.0216\n",
      "     15        \u001b[36m2.3018\u001b[0m  0.0260\n",
      "     16        \u001b[36m2.1664\u001b[0m  0.0187\n",
      "     17        \u001b[36m2.0666\u001b[0m  0.0291\n",
      "     18        \u001b[36m1.9745\u001b[0m  0.0214\n",
      "     19        \u001b[36m1.8588\u001b[0m  0.0109\n",
      "     20        \u001b[36m1.6358\u001b[0m  0.0350\n",
      "     21        \u001b[36m1.4055\u001b[0m  0.0240\n",
      "     22        \u001b[36m1.0482\u001b[0m  0.0224\n",
      "     23        \u001b[36m0.8379\u001b[0m  0.0210\n",
      "     24        \u001b[36m0.7518\u001b[0m  0.0196\n",
      "     25        \u001b[36m0.7393\u001b[0m  0.0218\n",
      "     26        \u001b[36m0.7203\u001b[0m  0.0242\n",
      "     27        0.7386  0.0208\n",
      "     28        0.7401  0.0211\n",
      "     29        \u001b[36m0.7104\u001b[0m  0.0208\n",
      "     30        0.7200  0.0180\n",
      "     31        0.7159  0.0165\n",
      "     32        \u001b[36m0.7073\u001b[0m  0.0333\n",
      "     33        0.7106  0.0206\n",
      "     34        0.7081  0.0214\n",
      "     35        0.7254  0.0213\n",
      "     36        0.7533  0.0162\n",
      "     37        \u001b[36m0.7071\u001b[0m  0.0157\n",
      "     38        \u001b[36m0.6818\u001b[0m  0.0198\n",
      "     39        0.6979  0.0172\n",
      "     40        0.7437  0.0253\n",
      "     41        0.7271  0.0217\n",
      "     42        0.7184  0.0273\n",
      "     43        0.7035  0.0207\n",
      "     44        0.7267  0.0188\n",
      "     45        0.7422  0.0199\n",
      "     46        0.6958  0.0191\n",
      "     47        0.6901  0.0196\n",
      "     48        0.7254  0.0200\n",
      "     49        0.6906  0.0201\n",
      "     50        0.7228  0.0204\n",
      "     51        0.7297  0.0207\n",
      "     52        0.7167  0.0210\n",
      "     53        0.7390  0.0198\n",
      "     54        0.7368  0.0254\n",
      "     55        0.7352  0.0089\n",
      "     56        0.7221  0.0242\n",
      "     57        0.7377  0.0123\n",
      "     58        0.7134  0.0338\n",
      "     59        0.7228  0.0200\n",
      "     60        0.7328  0.0099\n",
      "     61        0.7011  0.0361\n",
      "     62        0.7045  0.0170\n",
      "     63        0.6988  0.0228\n",
      "     64        0.6988  0.0218\n",
      "     65        0.7221  0.0331\n",
      "     66        0.7235  0.0321\n",
      "     67        0.7051  0.0314\n",
      "     68        0.7272  0.0324\n",
      "     69        0.7021  0.0234\n",
      "     70        0.6855  0.0205\n",
      "     71        0.7067  0.0196\n",
      "     72        0.7222  0.0234\n",
      "     73        0.7139  0.0247\n",
      "     74        0.6998  0.0219\n",
      "     75        0.6947  0.0226\n",
      "     76        0.7033  0.0213\n",
      "     77        0.6945  0.0223\n",
      "     78        0.7064  0.0242\n",
      "     79        0.7167  0.0149\n",
      "     80        \u001b[36m0.6754\u001b[0m  0.0267\n",
      "     81        0.6983  0.0211\n",
      "     82        0.7216  0.0197\n",
      "     83        0.7290  0.0148\n",
      "     84        0.6950  0.0337\n",
      "     85        0.7104  0.0208\n",
      "     86        0.7084  0.0243\n",
      "     87        0.6992  0.0212\n",
      "     88        0.7101  0.0157\n",
      "     89        0.7258  0.0270\n",
      "     90        0.7177  0.0187\n",
      "     91        0.6903  0.0164\n",
      "     92        0.7001  0.0174\n",
      "     93        0.7072  0.0163\n",
      "     94        0.6979  0.0254\n",
      "     95        0.7139  0.0091\n",
      "     96        0.6908  0.0222\n",
      "     97        0.6948  0.0156\n",
      "     98        0.6934  0.0175\n",
      "     99        0.6913  0.0177\n",
      "    100        0.6931  0.0171\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.5246\u001b[0m  0.0185\n",
      "      2        1.5449  0.0091\n",
      "      3        \u001b[36m1.3703\u001b[0m  0.0258\n",
      "      4        \u001b[36m1.2453\u001b[0m  0.0259\n",
      "      5        1.2999  0.0151\n",
      "      6        1.2590  0.0179\n",
      "      7        \u001b[36m1.1870\u001b[0m  0.0161\n",
      "      8        \u001b[36m1.0592\u001b[0m  0.0162\n",
      "      9        1.1834  0.0187\n",
      "     10        \u001b[36m1.0048\u001b[0m  0.0345\n",
      "     11        1.0376  0.0189\n",
      "     12        1.0840  0.0276\n",
      "     13        \u001b[36m0.9956\u001b[0m  0.0222\n",
      "     14        \u001b[36m0.9623\u001b[0m  0.0098\n",
      "     15        0.9656  0.0336\n",
      "     16        0.9795  0.0337\n",
      "     17        \u001b[36m0.9012\u001b[0m  0.0357\n",
      "     18        0.9168  0.0293\n",
      "     19        1.0102  0.0336\n",
      "     20        0.9764  0.0269\n",
      "     21        0.9933  0.0298\n",
      "     22        0.9871  0.0266\n",
      "     23        0.9763  0.0289\n",
      "     24        0.9415  0.0172\n",
      "     25        0.9582  0.0365\n",
      "     26        \u001b[36m0.8605\u001b[0m  0.0295\n",
      "     27        0.9661  0.0245\n",
      "     28        0.9387  0.0269\n",
      "     29        0.9431  0.0305\n",
      "     30        0.8746  0.0118\n",
      "     31        0.9169  0.0380\n",
      "     32        \u001b[36m0.8114\u001b[0m  0.0247\n",
      "     33        1.0351  0.0223\n",
      "     34        0.9081  0.0227\n",
      "     35        0.9150  0.0188\n",
      "     36        0.8472  0.0355\n",
      "     37        0.8352  0.0283\n",
      "     38        0.8409  0.0186\n",
      "     39        0.9312  0.0472\n",
      "     40        0.8285  0.0688\n",
      "     41        0.8939  0.0341\n",
      "     42        0.8381  0.0250\n",
      "     43        0.8981  0.0377\n",
      "     44        0.8896  0.0287\n",
      "     45        \u001b[36m0.7846\u001b[0m  0.0249\n",
      "     46        0.8196  0.0270\n",
      "     47        0.8087  0.0305\n",
      "     48        0.9102  0.0130\n",
      "     49        0.8709  0.0434\n",
      "     50        0.9235  0.0339\n",
      "     51        0.8050  0.0248\n",
      "     52        0.8439  0.0348\n",
      "     53        1.0006  0.0277\n",
      "     54        0.8978  0.0199\n",
      "     55        0.8494  0.0352\n",
      "     56        0.8798  0.0253\n",
      "     57        0.8808  0.0209\n",
      "     58        0.9850  0.0365\n",
      "     59        0.8254  0.0266\n",
      "     60        0.8536  0.0138\n",
      "     61        0.8520  0.0386\n",
      "     62        0.8700  0.0265\n",
      "     63        0.9644  0.0219\n",
      "     64        0.9000  0.0370\n",
      "     65        0.8149  0.0244\n",
      "     66        0.8773  0.0279\n",
      "     67        0.8665  0.0263\n",
      "     68        0.7897  0.0293\n",
      "     69        0.8103  0.0312\n",
      "     70        0.9252  0.0233\n",
      "     71        0.8622  0.0248\n",
      "     72        \u001b[36m0.7769\u001b[0m  0.0276\n",
      "     73        0.8748  0.0284\n",
      "     74        \u001b[36m0.7527\u001b[0m  0.0290\n",
      "     75        0.7997  0.0240\n",
      "     76        0.8276  0.0182\n",
      "     77        0.8492  0.0385\n",
      "     78        0.7689  0.0268\n",
      "     79        0.8588  0.0351\n",
      "     80        0.8115  0.0288\n",
      "     81        0.9032  0.0211\n",
      "     82        0.7896  0.0344\n",
      "     83        0.8870  0.0324\n",
      "     84        0.7988  0.0108\n",
      "     85        0.8079  0.0348\n",
      "     86        0.7712  0.0301\n",
      "     87        0.8148  0.0184\n",
      "     88        0.7734  0.0317\n",
      "     89        0.8193  0.0209\n",
      "     90        0.7928  0.0269\n",
      "     91        0.8312  0.0285\n",
      "     92        0.7779  0.0237\n",
      "     93        0.7773  0.0271\n",
      "     94        0.7859  0.0182\n",
      "     95        \u001b[36m0.7262\u001b[0m  0.0184\n",
      "     96        0.7683  0.0339\n",
      "     97        0.8631  0.0367\n",
      "     98        0.7832  0.0276\n",
      "     99        0.8168  0.0390\n",
      "    100        0.8172  0.0324\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.4288\u001b[0m  0.0511\n",
      "      2        1.6834  0.0426\n",
      "      3        1.5542  0.0357\n",
      "      4        1.6043  0.0507\n",
      "      5        \u001b[36m1.4125\u001b[0m  0.0273\n",
      "      6        1.4768  0.0284\n",
      "      7        \u001b[36m1.3702\u001b[0m  0.0356\n",
      "      8        1.4935  0.0360\n",
      "      9        \u001b[36m1.3107\u001b[0m  0.0308\n",
      "     10        \u001b[36m1.1857\u001b[0m  0.0635\n",
      "     11        1.3447  0.0381\n",
      "     12        1.2934  0.0307\n",
      "     13        1.3286  0.0295\n",
      "     14        1.2641  0.0233\n",
      "     15        1.3563  0.0245\n",
      "     16        1.2244  0.0410\n",
      "     17        \u001b[36m1.1844\u001b[0m  0.0313\n",
      "     18        1.2287  0.0268\n",
      "     19        \u001b[36m1.1057\u001b[0m  0.0419\n",
      "     20        1.1662  0.0381\n",
      "     21        1.2267  0.0369\n",
      "     22        \u001b[36m1.0851\u001b[0m  0.0333\n",
      "     23        1.1530  0.0260\n",
      "     24        1.1125  0.0394\n",
      "     25        1.0865  0.0321\n",
      "     26        1.1626  0.0359\n",
      "     27        1.1737  0.0262\n",
      "     28        1.1361  0.0355\n",
      "     29        1.3018  0.0322\n",
      "     30        1.1438  0.0146\n",
      "     31        1.1493  0.0402\n",
      "     32        1.0991  0.0192\n",
      "     33        1.1417  0.0414\n",
      "     34        \u001b[36m1.0684\u001b[0m  0.0304\n",
      "     35        1.1481  0.0253\n",
      "     36        \u001b[36m1.0268\u001b[0m  0.0219\n",
      "     37        1.1690  0.0309\n",
      "     38        1.0540  0.0098\n",
      "     39        1.0664  0.0377\n",
      "     40        1.0571  0.0304\n",
      "     41        1.2116  0.0233\n",
      "     42        1.0742  0.0261\n",
      "     43        \u001b[36m1.0197\u001b[0m  0.0232\n",
      "     44        1.1122  0.0264\n",
      "     45        1.1163  0.0145\n",
      "     46        1.1452  0.0359\n",
      "     47        \u001b[36m0.9705\u001b[0m  0.0140\n",
      "     48        1.0324  0.0256\n",
      "     49        1.0133  0.0255\n",
      "     50        0.9881  0.0215\n",
      "     51        1.1170  0.0219\n",
      "     52        1.0087  0.0218\n",
      "     53        1.0435  0.0257\n",
      "     54        1.0524  0.0216\n",
      "     55        0.9830  0.0221\n",
      "     56        1.0064  0.0243\n",
      "     57        \u001b[36m0.9482\u001b[0m  0.0155\n",
      "     58        \u001b[36m0.9266\u001b[0m  0.0164\n",
      "     59        0.9802  0.0211\n",
      "     60        \u001b[36m0.9179\u001b[0m  0.0093\n",
      "     61        0.9623  0.0374\n",
      "     62        0.9891  0.0119\n",
      "     63        1.0601  0.0197\n",
      "     64        0.9827  0.0264\n",
      "     65        0.9610  0.0194\n",
      "     66        \u001b[36m0.8459\u001b[0m  0.0194\n",
      "     67        0.9134  0.0207\n",
      "     68        1.0350  0.0197\n",
      "     69        0.8908  0.0159\n",
      "     70        1.0180  0.0202\n",
      "     71        0.9117  0.0247\n",
      "     72        0.9635  0.0399\n",
      "     73        0.9500  0.0482\n",
      "     74        1.0008  0.0351\n",
      "     75        1.0136  0.0275\n",
      "     76        0.9443  0.0189\n",
      "     77        0.9343  0.0380\n",
      "     78        0.9456  0.0252\n",
      "     79        0.8811  0.0346\n",
      "     80        1.0136  0.0150\n",
      "     81        0.9225  0.0401\n",
      "     82        \u001b[36m0.8411\u001b[0m  0.0285\n",
      "     83        0.9275  0.0116\n",
      "     84        0.8752  0.0442\n",
      "     85        0.9639  0.0296\n",
      "     86        1.0301  0.0270\n",
      "     87        0.8837  0.0160\n",
      "     88        0.9360  0.0246\n",
      "     89        0.8652  0.0266\n",
      "     90        0.8705  0.0217\n",
      "     91        0.9520  0.0345\n",
      "     92        0.9519  0.0360\n",
      "     93        0.9299  0.0233\n",
      "     94        0.8953  0.0336\n",
      "     95        0.8677  0.0198\n",
      "     96        0.8829  0.0181\n",
      "     97        0.9597  0.0345\n",
      "     98        0.9739  0.0208\n",
      "     99        0.9443  0.0269\n",
      "    100        0.8757  0.0335\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.8255\u001b[0m  0.0299\n",
      "      2        \u001b[36m1.8021\u001b[0m  0.0279\n",
      "      3        1.8310  0.0239\n",
      "      4        \u001b[36m1.6799\u001b[0m  0.0223\n",
      "      5        \u001b[36m1.5248\u001b[0m  0.0189\n",
      "      6        1.5313  0.0340\n",
      "      7        \u001b[36m1.3376\u001b[0m  0.0181\n",
      "      8        1.3865  0.0265\n",
      "      9        1.4844  0.0197\n",
      "     10        1.4396  0.0339\n",
      "     11        \u001b[36m1.1323\u001b[0m  0.0137\n",
      "     12        1.4154  0.0342\n",
      "     13        1.2704  0.0147\n",
      "     14        1.3788  0.0372\n",
      "     15        1.4063  0.0235\n",
      "     16        1.3015  0.0216\n",
      "     17        1.2523  0.0258\n",
      "     18        1.1529  0.0217\n",
      "     19        1.2080  0.0166\n",
      "     20        \u001b[36m1.1151\u001b[0m  0.0206\n",
      "     21        1.1646  0.0304\n",
      "     22        1.1300  0.0163\n",
      "     23        1.2730  0.0179\n",
      "     24        1.1866  0.0346\n",
      "     25        1.1754  0.0182\n",
      "     26        \u001b[36m1.0946\u001b[0m  0.0222\n",
      "     27        1.1709  0.0063\n",
      "     28        1.1586  0.0199\n",
      "     29        1.1837  0.0187\n",
      "     30        1.0952  0.0232\n",
      "     31        1.1887  0.0222\n",
      "     32        \u001b[36m1.0409\u001b[0m  0.0194\n",
      "     33        1.1061  0.0126\n",
      "     34        1.0538  0.0509\n",
      "     35        1.0790  0.0359\n",
      "     36        1.0898  0.0090\n",
      "     37        1.0917  0.0346\n",
      "     38        1.1977  0.0200\n",
      "     39        1.1133  0.0258\n",
      "     40        1.1704  0.0197\n",
      "     41        1.0446  0.0246\n",
      "     42        \u001b[36m1.0319\u001b[0m  0.0063\n",
      "     43        1.0800  0.0210\n",
      "     44        \u001b[36m0.9747\u001b[0m  0.0143\n",
      "     45        0.9912  0.0301\n",
      "     46        1.1038  0.0184\n",
      "     47        0.9749  0.0159\n",
      "     48        1.0599  0.0188\n",
      "     49        \u001b[36m0.9256\u001b[0m  0.0174\n",
      "     50        1.0546  0.0263\n",
      "     51        0.9771  0.0241\n",
      "     52        0.9658  0.0182\n",
      "     53        1.0309  0.0160\n",
      "     54        0.9663  0.0228\n",
      "     55        \u001b[36m0.9210\u001b[0m  0.0060\n",
      "     56        0.9957  0.0174\n",
      "     57        0.9879  0.0205\n",
      "     58        0.9350  0.0142\n",
      "     59        \u001b[36m0.9060\u001b[0m  0.0323\n",
      "     60        0.9539  0.0156\n",
      "     61        0.9423  0.0174\n",
      "     62        0.9499  0.0199\n",
      "     63        0.9339  0.0282\n",
      "     64        \u001b[36m0.8829\u001b[0m  0.0163\n",
      "     65        0.9758  0.0181\n",
      "     66        0.9121  0.0164\n",
      "     67        0.9466  0.0316\n",
      "     68        0.9482  0.0166\n",
      "     69        0.9359  0.0154\n",
      "     70        \u001b[36m0.8781\u001b[0m  0.0167\n",
      "     71        0.9396  0.0164\n",
      "     72        0.9338  0.0195\n",
      "     73        0.9082  0.0171\n",
      "     74        0.9933  0.0208\n",
      "     75        \u001b[36m0.8709\u001b[0m  0.0171\n",
      "     76        0.8716  0.0262\n",
      "     77        0.9074  0.0182\n",
      "     78        0.8915  0.0176\n",
      "     79        0.8869  0.0153\n",
      "     80        1.0085  0.0303\n",
      "     81        0.9222  0.0185\n",
      "     82        0.8876  0.0165\n",
      "     83        0.9177  0.0176\n",
      "     84        0.9368  0.0154\n",
      "     85        0.9355  0.0298\n",
      "     86        \u001b[36m0.8221\u001b[0m  0.0176\n",
      "     87        \u001b[36m0.7742\u001b[0m  0.0175\n",
      "     88        0.7990  0.0181\n",
      "     89        0.8104  0.0345\n",
      "     90        0.8701  0.0291\n",
      "     91        0.8503  0.0162\n",
      "     92        0.8612  0.0421\n",
      "     93        0.8793  0.0416\n",
      "     94        0.9591  0.0297\n",
      "     95        0.7762  0.0322\n",
      "     96        0.8876  0.0706\n",
      "     97        0.8628  0.0352\n",
      "     98        0.9141  0.0344\n",
      "     99        0.8082  0.0229\n",
      "    100        0.8859  0.0382\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.4622\u001b[0m  0.0313\n",
      "      2        1.4907  0.0110\n",
      "      3        \u001b[36m1.4427\u001b[0m  0.0377\n",
      "      4        \u001b[36m1.2655\u001b[0m  0.0212\n",
      "      5        1.4335  0.0291\n",
      "      6        1.3982  0.0341\n",
      "      7        1.3788  0.0257\n",
      "      8        \u001b[36m1.1584\u001b[0m  0.0274\n",
      "      9        1.5132  0.0308\n",
      "     10        1.2699  0.0230\n",
      "     11        1.3916  0.0370\n",
      "     12        1.3008  0.0208\n",
      "     13        1.3031  0.0232\n",
      "     14        1.2249  0.0207\n",
      "     15        1.1991  0.0362\n",
      "     16        1.2466  0.0260\n",
      "     17        1.3791  0.0159\n",
      "     18        \u001b[36m1.1580\u001b[0m  0.0317\n",
      "     19        \u001b[36m1.0833\u001b[0m  0.0228\n",
      "     20        1.1677  0.0206\n",
      "     21        1.2073  0.0214\n",
      "     22        1.2276  0.0165\n",
      "     23        1.2438  0.0326\n",
      "     24        1.1495  0.0208\n",
      "     25        1.1764  0.0116\n",
      "     26        \u001b[36m1.0538\u001b[0m  0.0345\n",
      "     27        1.0912  0.0132\n",
      "     28        1.1152  0.0183\n",
      "     29        1.1372  0.0204\n",
      "     30        1.1117  0.0112\n",
      "     31        1.0925  0.0347\n",
      "     32        1.0862  0.0142\n",
      "     33        1.1117  0.0239\n",
      "     34        1.0791  0.0125\n",
      "     35        1.1785  0.0317\n",
      "     36        1.1159  0.0200\n",
      "     37        \u001b[36m0.9856\u001b[0m  0.0235\n",
      "     38        1.0623  0.0226\n",
      "     39        1.1049  0.0195\n",
      "     40        1.0740  0.0234\n",
      "     41        1.1083  0.0231\n",
      "     42        1.0555  0.0143\n",
      "     43        1.1170  0.0216\n",
      "     44        1.0656  0.0035\n",
      "     45        1.1267  0.0225\n",
      "     46        1.0716  0.0345\n",
      "     47        1.0201  0.0420\n",
      "     48        1.0118  0.0193\n",
      "     49        1.0137  0.0358\n",
      "     50        1.0481  0.0209\n",
      "     51        1.0037  0.0278\n",
      "     52        \u001b[36m0.9062\u001b[0m  0.0219\n",
      "     53        1.0464  0.0254\n",
      "     54        1.0073  0.0200\n",
      "     55        1.0085  0.0250\n",
      "     56        1.1083  0.0226\n",
      "     57        0.9639  0.0247\n",
      "     58        0.9733  0.0201\n",
      "     59        1.0711  0.0258\n",
      "     60        0.9404  0.0210\n",
      "     61        0.9569  0.0245\n",
      "     62        0.9733  0.0241\n",
      "     63        0.9694  0.0167\n",
      "     64        0.9556  0.0315\n",
      "     65        \u001b[36m0.8457\u001b[0m  0.0234\n",
      "     66        0.9833  0.0237\n",
      "     67        1.0510  0.0236\n",
      "     68        1.0174  0.0251\n",
      "     69        0.9049  0.0186\n",
      "     70        0.8928  0.0273\n",
      "     71        0.8714  0.0308\n",
      "     72        0.9087  0.0211\n",
      "     73        0.9249  0.0178\n",
      "     74        0.9048  0.0272\n",
      "     75        0.8855  0.0179\n",
      "     76        0.9120  0.0273\n",
      "     77        \u001b[36m0.8385\u001b[0m  0.0242\n",
      "     78        0.9289  0.0212\n",
      "     79        0.8735  0.0218\n",
      "     80        0.9228  0.0222\n",
      "     81        0.9625  0.0228\n",
      "     82        0.8553  0.0184\n",
      "     83        0.9407  0.0269\n",
      "     84        0.8491  0.0231\n",
      "     85        0.9145  0.0166\n",
      "     86        0.8714  0.0310\n",
      "     87        0.8805  0.0180\n",
      "     88        0.8725  0.0265\n",
      "     89        0.8812  0.0197\n",
      "     90        0.8810  0.0138\n",
      "     91        0.8666  0.0214\n",
      "     92        0.9596  0.0264\n",
      "     93        \u001b[36m0.8159\u001b[0m  0.0238\n",
      "     94        0.8233  0.0218\n",
      "     95        0.8189  0.0229\n",
      "     96        0.8214  0.0238\n",
      "     97        0.8291  0.0223\n",
      "     98        \u001b[36m0.7985\u001b[0m  0.0181\n",
      "     99        \u001b[36m0.7848\u001b[0m  0.0262\n",
      "    100        0.8564  0.0208\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2583\u001b[0m  0.0308\n",
      "      2       37.2583  0.0108\n",
      "      3       37.2583  0.0348\n",
      "      4       37.2583  0.0341\n",
      "      5       37.2583  0.0324\n",
      "      6       37.2583  0.0291\n",
      "      7       37.2583  0.0208\n",
      "      8       37.2583  0.0323\n",
      "      9       37.2583  0.0279\n",
      "     10       37.2583  0.0319\n",
      "     11       37.2583  0.0218\n",
      "     12       37.2583  0.0284\n",
      "     13       37.2583  0.0630\n",
      "     14       37.2583  0.0399\n",
      "     15       37.2583  0.0308\n",
      "     16       37.2583  0.0328\n",
      "     17       37.2583  0.0340\n",
      "     18       37.2583  0.0309\n",
      "     19       37.2583  0.0305\n",
      "     20       37.2583  0.0350\n",
      "     21       37.2583  0.0296\n",
      "     22       37.2583  0.0310\n",
      "     23       37.2583  0.0266\n",
      "     24       37.2583  0.0332\n",
      "     25       37.2583  0.0253\n",
      "     26       37.2583  0.0353\n",
      "     27       37.2583  0.0316\n",
      "     28       37.2583  0.0254\n",
      "     29       37.2583  0.0301\n",
      "     30       37.2583  0.0247\n",
      "     31       37.2583  0.0386\n",
      "     32       37.2583  0.0297\n",
      "     33       37.2583  0.0289\n",
      "     34       37.2583  0.0261\n",
      "     35       37.2583  0.0251\n",
      "     36       37.2583  0.0342\n",
      "     37       37.2583  0.0283\n",
      "     38       37.2583  0.0269\n",
      "     39       37.2583  0.0285\n",
      "     40       37.2583  0.0257\n",
      "     41       37.2583  0.0267\n",
      "     42       37.2583  0.0343\n",
      "     43       37.2583  0.0152\n",
      "     44       37.2583  0.0395\n",
      "     45       37.2583  0.0274\n",
      "     46       37.2583  0.0272\n",
      "     47       37.2583  0.0282\n",
      "     48       37.2583  0.0307\n",
      "     49       37.2583  0.0296\n",
      "     50       37.2583  0.0339\n",
      "     51       37.2583  0.0273\n",
      "     52       37.2583  0.0256\n",
      "     53       37.2583  0.0320\n",
      "     54       37.2583  0.0351\n",
      "     55       37.2583  0.0308\n",
      "     56       37.2583  0.0275\n",
      "     57       37.2583  0.0314\n",
      "     58       37.2583  0.0368\n",
      "     59       37.2583  0.0273\n",
      "     60       37.2583  0.0193\n",
      "     61       37.2583  0.0234\n",
      "     62       37.2583  0.0381\n",
      "     63       \u001b[36m36.9477\u001b[0m  0.0276\n",
      "     64       \u001b[36m16.2129\u001b[0m  0.0376\n",
      "     65        \u001b[36m0.6884\u001b[0m  0.0329\n",
      "     66        \u001b[36m0.6361\u001b[0m  0.0308\n",
      "     67        \u001b[36m0.5678\u001b[0m  0.0327\n",
      "     68        \u001b[36m0.5632\u001b[0m  0.0318\n",
      "     69        0.5793  0.0263\n",
      "     70        \u001b[36m0.5396\u001b[0m  0.0356\n",
      "     71        \u001b[36m0.5370\u001b[0m  0.0309\n",
      "     72        0.5391  0.0303\n",
      "     73        \u001b[36m0.5316\u001b[0m  0.0388\n",
      "     74        \u001b[36m0.5304\u001b[0m  0.0244\n",
      "     75        \u001b[36m0.5126\u001b[0m  0.0398\n",
      "     76        \u001b[36m0.5095\u001b[0m  0.0307\n",
      "     77        \u001b[36m0.5074\u001b[0m  0.0490\n",
      "     78        \u001b[36m0.4873\u001b[0m  0.0582\n",
      "     79        0.4948  0.0349\n",
      "     80        0.4989  0.0411\n",
      "     81        0.4896  0.0405\n",
      "     82        0.4886  0.0371\n",
      "     83        \u001b[36m0.4819\u001b[0m  0.0353\n",
      "     84        \u001b[36m0.4713\u001b[0m  0.0377\n",
      "     85        \u001b[36m0.4689\u001b[0m  0.0369\n",
      "     86        \u001b[36m0.4667\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4628\u001b[0m  0.0321\n",
      "     88        \u001b[36m0.4356\u001b[0m  0.0348\n",
      "     89        0.4530  0.0398\n",
      "     90        0.4491  0.0268\n",
      "     91        0.4400  0.0489\n",
      "     92        \u001b[36m0.4206\u001b[0m  0.0312\n",
      "     93        0.4302  0.0313\n",
      "     94        0.4332  0.0308\n",
      "     95        0.4304  0.0316\n",
      "     96        \u001b[36m0.4036\u001b[0m  0.0322\n",
      "     97        0.4247  0.0307\n",
      "     98        0.4125  0.0280\n",
      "     99        \u001b[36m0.3992\u001b[0m  0.0241\n",
      "    100        0.4061  0.0309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=4,\n",
       "             estimator=NeuralNetBinaryClassifier(_params_to_validate={&#x27;optimizer__weight_decay&#x27;}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class &#x27;skorch.dataset.Dataset&#x27;&gt;, device=&#x27;cpu&#x27;, iterator_train=&lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;, iterator_valid=&lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;, lr=0.001, max_epochs=10, mo...\n",
       "                         &#x27;criterion&#x27;: [&lt;class &#x27;torch.nn.modules.loss.BCELoss&#x27;&gt;],\n",
       "                         &#x27;max_epochs&#x27;: [100],\n",
       "                         &#x27;module__activation&#x27;: [&lt;function relu at 0x0000018F1024A700&gt;,\n",
       "                                                &lt;function tanh at 0x0000018F1024B100&gt;],\n",
       "                         &#x27;module__initializer&#x27;: [&lt;function uniform_ at 0x0000018F102C3560&gt;,\n",
       "                                                 &lt;function normal_ at 0x0000018F102C3600&gt;],\n",
       "                         &#x27;module__neurons&#x27;: [16],\n",
       "                         &#x27;optimizer&#x27;: [&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=4,\n",
       "             estimator=NeuralNetBinaryClassifier(_params_to_validate={&#x27;optimizer__weight_decay&#x27;}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class &#x27;skorch.dataset.Dataset&#x27;&gt;, device=&#x27;cpu&#x27;, iterator_train=&lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;, iterator_valid=&lt;class &#x27;torch.utils.data.dataloader.DataLoader&#x27;&gt;, lr=0.001, max_epochs=10, mo...\n",
       "                         &#x27;criterion&#x27;: [&lt;class &#x27;torch.nn.modules.loss.BCELoss&#x27;&gt;],\n",
       "                         &#x27;max_epochs&#x27;: [100],\n",
       "                         &#x27;module__activation&#x27;: [&lt;function relu at 0x0000018F1024A700&gt;,\n",
       "                                                &lt;function tanh at 0x0000018F1024B100&gt;],\n",
       "                         &#x27;module__initializer&#x27;: [&lt;function uniform_ at 0x0000018F102C3560&gt;,\n",
       "                                                 &lt;function normal_ at 0x0000018F102C3600&gt;],\n",
       "                         &#x27;module__neurons&#x27;: [16],\n",
       "                         &#x27;optimizer&#x27;: [&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: NeuralNetBinaryClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;skorch.classifier.NeuralNetBinaryClassifier&#x27;&gt;[initialized](\n",
       "  module_=classificador_torch(\n",
       "    (dense0): Linear(in_features=30, out_features=16, bias=True)\n",
       "    (dropout0): Dropout(p=0.2, inplace=False)\n",
       "    (dense1): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (droupout1): Dropout(p=0.2, inplace=False)\n",
       "    (dense2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (output): Sigmoid()\n",
       "  ),\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">NeuralNetBinaryClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;skorch.classifier.NeuralNetBinaryClassifier&#x27;&gt;[initialized](\n",
       "  module_=classificador_torch(\n",
       "    (dense0): Linear(in_features=30, out_features=16, bias=True)\n",
       "    (dropout0): Dropout(p=0.2, inplace=False)\n",
       "    (dense1): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (droupout1): Dropout(p=0.2, inplace=False)\n",
       "    (dense2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (output): Sigmoid()\n",
       "  ),\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=NeuralNetBinaryClassifier(_params_to_validate={'optimizer__weight_decay'}, batch_size=128, callbacks=None, compile=False, dataset=<class 'skorch.dataset.Dataset'>, device='cpu', iterator_train=<class 'torch.utils.data.dataloader.DataLoader'>, iterator_valid=<class 'torch.utils.data.dataloader.DataLoader'>, lr=0.001, max_epochs=10, mo...\n",
       "                         'criterion': [<class 'torch.nn.modules.loss.BCELoss'>],\n",
       "                         'max_epochs': [100],\n",
       "                         'module__activation': [<function relu at 0x0000018F1024A700>,\n",
       "                                                <function tanh at 0x0000018F1024B100>],\n",
       "                         'module__initializer': [<function uniform_ at 0x0000018F102C3560>,\n",
       "                                                 <function normal_ at 0x0000018F102C3600>],\n",
       "                         'module__neurons': [16],\n",
       "                         'optimizer': [<class 'torch.optim.adam.Adam'>]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(previsores, classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_precisao = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'batch_size': 30,\n",
       "  'criterion': torch.nn.modules.loss.BCELoss,\n",
       "  'max_epochs': 100,\n",
       "  'module__activation': <function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>,\n",
       "  'module__initializer': <function torch.nn.init.uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0, generator: Optional[torch._C.Generator] = None) -> torch.Tensor>,\n",
       "  'module__neurons': 16,\n",
       "  'optimizer': torch.optim.adam.Adam},\n",
       " 0.7242194425293016)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhores_parametros, melhor_precisao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 4: Classificação binária Breast Cancer - classificar somente um registro e salvar o classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2: Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('./Bases/Bases/entradas_breast.csv')\n",
    "classes = pd.read_csv('./Bases/Bases/saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = torch.tensor(np.array(previsores), dtype=torch.float)\n",
    "classes = torch.tensor(np.array(classes), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3: Transformação dos dados para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(previsores, classes), batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4: Contrução do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.normal_(self.dense0.weight, mean=0.0, std=0.05) # configuração de inicialização do keras\n",
    "\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.normal_(self.dense1.weight, mean=0.0, std=0.05)\n",
    "        self.activation1 = nn.ReLU()\n",
    "\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.output = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        \n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "    \n",
    "        X = self.dense2(X)\n",
    "        X = self.activation2(X)\n",
    "        X = self.dropout(X)\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = classificador_torch()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classificador.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5: Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs} loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 loss: 1.140583071269487\n",
      "Epoch 2/100 loss: 0.5777509092238912\n",
      "Epoch 3/100 loss: 0.5758622628554964\n",
      "Epoch 4/100 loss: 0.5723089523482741\n",
      "Epoch 5/100 loss: 0.5596633380965182\n",
      "Epoch 6/100 loss: 0.5855091428547575\n",
      "Epoch 7/100 loss: 0.5629152927482337\n",
      "Epoch 8/100 loss: 0.5732014309941676\n",
      "Epoch 9/100 loss: 0.5554155179283076\n",
      "Epoch 10/100 loss: 0.5554048549710658\n",
      "Epoch 11/100 loss: 0.5558865248111257\n",
      "Epoch 12/100 loss: 0.5583988386288024\n",
      "Epoch 13/100 loss: 0.5564874081235183\n",
      "Epoch 14/100 loss: 0.5494867824671561\n",
      "Epoch 15/100 loss: 0.5334616235473699\n",
      "Epoch 16/100 loss: 0.5455839926736397\n",
      "Epoch 17/100 loss: 0.543208205386212\n",
      "Epoch 18/100 loss: 0.5328075138100407\n",
      "Epoch 19/100 loss: 0.5414915916166807\n",
      "Epoch 20/100 loss: 0.543293564466008\n",
      "Epoch 21/100 loss: 0.5233326891535207\n",
      "Epoch 22/100 loss: 0.5144120041738477\n",
      "Epoch 23/100 loss: 0.5180973138725549\n",
      "Epoch 24/100 loss: 0.5149904615000674\n",
      "Epoch 25/100 loss: 0.5424551111564302\n",
      "Epoch 26/100 loss: 0.5295284986495972\n",
      "Epoch 27/100 loss: 0.5202702994932208\n",
      "Epoch 28/100 loss: 0.4973242476321103\n",
      "Epoch 29/100 loss: 0.5203702149161121\n",
      "Epoch 30/100 loss: 0.5019282970512122\n",
      "Epoch 31/100 loss: 0.5189605203636906\n",
      "Epoch 32/100 loss: 0.48583860669219703\n",
      "Epoch 33/100 loss: 0.5206023543550257\n",
      "Epoch 34/100 loss: 0.503544080675694\n",
      "Epoch 35/100 loss: 0.5088072162971162\n",
      "Epoch 36/100 loss: 0.5192548480995914\n",
      "Epoch 37/100 loss: 0.5132377037876531\n",
      "Epoch 38/100 loss: 0.4873688895451395\n",
      "Epoch 39/100 loss: 0.5061217447121938\n",
      "Epoch 40/100 loss: 0.4845103652853715\n",
      "Epoch 41/100 loss: 0.47749612297405275\n",
      "Epoch 42/100 loss: 0.4920784109517148\n",
      "Epoch 43/100 loss: 0.4881211571526109\n",
      "Epoch 44/100 loss: 0.470612069232422\n",
      "Epoch 45/100 loss: 0.48144154276764184\n",
      "Epoch 46/100 loss: 0.5118655128437176\n",
      "Epoch 47/100 loss: 0.4659065023848885\n",
      "Epoch 48/100 loss: 0.46934778726937476\n",
      "Epoch 49/100 loss: 0.4942725734752521\n",
      "Epoch 50/100 loss: 0.46388038679173116\n",
      "Epoch 51/100 loss: 0.4628470336136065\n",
      "Epoch 52/100 loss: 0.4750204888875024\n",
      "Epoch 53/100 loss: 0.47756715537163247\n",
      "Epoch 54/100 loss: 0.4550177528147112\n",
      "Epoch 55/100 loss: 0.4674504652880786\n",
      "Epoch 56/100 loss: 0.478698710861959\n",
      "Epoch 57/100 loss: 0.46794072081122484\n",
      "Epoch 58/100 loss: 0.477340791309089\n",
      "Epoch 59/100 loss: 0.46284341341570806\n",
      "Epoch 60/100 loss: 0.4714888336888531\n",
      "Epoch 61/100 loss: 0.5047596677353507\n",
      "Epoch 62/100 loss: 0.46465046363964413\n",
      "Epoch 63/100 loss: 0.46408241961086005\n",
      "Epoch 64/100 loss: 0.48708599073845044\n",
      "Epoch 65/100 loss: 0.461411397185242\n",
      "Epoch 66/100 loss: 0.44462634858332184\n",
      "Epoch 67/100 loss: 0.46373867727162543\n",
      "Epoch 68/100 loss: 0.46618240026005525\n",
      "Epoch 69/100 loss: 0.4486637604341172\n",
      "Epoch 70/100 loss: 0.4399755367061548\n",
      "Epoch 71/100 loss: 0.44551468653637066\n",
      "Epoch 72/100 loss: 0.44999788152544123\n",
      "Epoch 73/100 loss: 0.45166952359048945\n",
      "Epoch 74/100 loss: 0.4267861678412086\n",
      "Epoch 75/100 loss: 0.41617307432910855\n",
      "Epoch 76/100 loss: 0.42345632625776425\n",
      "Epoch 77/100 loss: 0.44889440740409653\n",
      "Epoch 78/100 loss: 0.4316743282895339\n",
      "Epoch 79/100 loss: 0.4347405786577024\n",
      "Epoch 80/100 loss: 0.44853289948220837\n",
      "Epoch 81/100 loss: 0.42704559758044125\n",
      "Epoch 82/100 loss: 0.4353164774284028\n",
      "Epoch 83/100 loss: 0.4249459058046341\n",
      "Epoch 84/100 loss: 0.44618014403079687\n",
      "Epoch 85/100 loss: 0.4171801622499499\n",
      "Epoch 86/100 loss: 0.42023985859072\n",
      "Epoch 87/100 loss: 0.46270649579533357\n",
      "Epoch 88/100 loss: 0.4365831061936261\n",
      "Epoch 89/100 loss: 0.4123042053297946\n",
      "Epoch 90/100 loss: 0.4301082878782038\n",
      "Epoch 91/100 loss: 0.4172466499240775\n",
      "Epoch 92/100 loss: 0.4335081825653712\n",
      "Epoch 93/100 loss: 0.43834282039550315\n",
      "Epoch 94/100 loss: 0.4193752123075619\n",
      "Epoch 95/100 loss: 0.426212362552944\n",
      "Epoch 96/100 loss: 0.4320440153803742\n",
      "Epoch 97/100 loss: 0.4241548261621542\n",
      "Epoch 98/100 loss: 0.4127466459023325\n",
      "Epoch 99/100 loss: 0.4333557217267522\n",
      "Epoch 100/100 loss: 0.4217975944803472\n"
     ]
    }
   ],
   "source": [
    "train(classificador, train_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6: Classificar somente um registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo = torch.tensor([15.80, 8.34, 118, 900, 0.10, 0.26, 0.08, 0.134, 0.178, \n",
    "                     0.20, 0.05, 1098, 0.87, 4500, 145.2, 0.005, 0.04, 0.05, 0.015, \n",
    "                     0.03, 0.007, 23.15, 16.64, 178.5, 2018, 0.14, 0.185, 0.84, 158, 0.363], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.eval()\n",
    "previsao = classificador(novo)\n",
    "previsao = previsao.detach()\n",
    "previsao = previsao.numpy()\n",
    "previsao = previsao > 0.5\n",
    "previsao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 7: Salvar o classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dense0.weight',\n",
       "              tensor([[-2.9874e-01, -2.4318e-02, -2.8736e-01, -6.0441e-02,  9.2354e-02,\n",
       "                        1.2011e-01,  1.1482e-01, -2.1392e-01, -1.8156e-01, -6.7864e-02,\n",
       "                       -8.2370e-02, -5.9511e-02,  4.1297e-02,  1.0247e-01, -2.9426e-03,\n",
       "                       -8.9640e-02, -1.2226e-01, -1.1409e-01, -4.9019e-02,  1.6793e-01,\n",
       "                       -4.3635e-02,  4.4012e-02, -9.6899e-02,  6.4170e-02,  9.2050e-02,\n",
       "                       -1.0689e-01,  1.0891e-01, -8.0071e-03, -5.0805e-03,  1.2771e-01],\n",
       "                      [ 3.0755e-02, -3.1563e-02,  1.0958e-01,  2.5881e-02,  1.1664e-02,\n",
       "                        2.1585e-02, -1.1671e-01,  1.7554e-01,  3.4069e-01,  1.8274e-01,\n",
       "                        1.0541e-02,  5.2146e-02, -8.5857e-02,  1.2377e-02,  6.8551e-03,\n",
       "                       -6.6760e-03,  9.6605e-02, -3.8297e-02,  5.5081e-02, -9.7353e-02,\n",
       "                        1.9366e-02, -1.7553e-01, -3.7163e-02, -4.2237e-02, -2.3373e-01,\n",
       "                        7.1189e-02, -2.1782e-01,  4.7659e-02,  1.6429e-01, -6.3003e-02],\n",
       "                      [ 4.9967e-02,  8.0892e-02,  2.2192e-01,  1.4613e-01, -9.8111e-02,\n",
       "                        1.0391e-02, -1.2304e-01,  8.8150e-02,  2.7843e-01,  3.0207e-01,\n",
       "                        1.5305e-01,  1.0834e-02, -2.0710e-02, -1.2913e-01,  1.2625e-04,\n",
       "                        4.7569e-02,  1.6991e-01,  3.7408e-03,  1.5768e-01, -1.0286e-01,\n",
       "                        9.2780e-02, -8.2548e-02,  9.9204e-02, -5.6333e-02, -1.4469e-01,\n",
       "                        9.3571e-02, -2.0798e-01,  2.1017e-01,  4.2581e-02, -1.3277e-01],\n",
       "                      [-4.2734e-01, -1.2327e-01, -4.6343e-01, -5.3005e-03,  2.6898e-01,\n",
       "                       -4.7689e-02,  1.2500e-01, -2.6270e-01, -1.2325e-01,  2.9475e-03,\n",
       "                        9.3693e-02, -6.1424e-02,  8.6546e-03,  2.8584e-01,  1.4413e-03,\n",
       "                       -1.2435e-01, -2.2699e-01, -2.9913e-01,  7.3461e-04,  3.1369e-01,\n",
       "                       -1.4192e-01, -2.8628e-02, -2.0760e-01,  1.7534e-01,  1.0859e-01,\n",
       "                       -4.1695e-02,  3.4224e-02,  5.4715e-02,  2.9671e-03,  3.8898e-01],\n",
       "                      [-3.2812e-01, -9.1117e-02, -2.5910e-01, -8.8728e-02,  1.3875e-01,\n",
       "                       -4.8991e-02,  9.6003e-02, -2.0751e-01, -8.5803e-02, -6.0683e-02,\n",
       "                       -7.1936e-03, -1.9585e-02,  5.1971e-03,  1.4059e-01, -1.5283e-04,\n",
       "                       -1.2096e-01, -1.8908e-01, -3.5522e-01,  5.9391e-03,  3.1942e-01,\n",
       "                       -2.2382e-01,  1.2354e-01, -2.8997e-01,  1.5828e-01,  2.8433e-01,\n",
       "                       -3.1202e-02,  6.6569e-02, -8.4207e-02,  4.6430e-03,  2.5388e-01],\n",
       "                      [-1.0176e-01, -7.4125e-02, -1.1683e-02, -2.0187e-02, -8.7531e-02,\n",
       "                        1.3622e-02, -5.4220e-02, -4.2454e-02,  7.8827e-02,  1.1133e-01,\n",
       "                        2.1610e-02,  2.9047e-02,  9.1480e-03,  2.6509e-03, -1.4393e-03,\n",
       "                        1.5133e-02,  2.8915e-03, -4.1525e-02, -5.0242e-02, -2.7919e-02,\n",
       "                       -5.0075e-02, -1.7744e-02, -9.2409e-02,  4.1364e-02, -2.2209e-01,\n",
       "                       -9.3783e-02,  7.6536e-02,  3.2758e-02,  3.2598e-02, -1.6228e-01],\n",
       "                      [-1.6661e-01, -8.0217e-02, -1.1349e-01,  1.9623e-02,  4.6204e-03,\n",
       "                       -4.4442e-02, -5.8220e-02, -1.5612e-01,  2.2321e-01,  6.3275e-02,\n",
       "                        1.9530e-01, -4.3959e-03, -2.9385e-02, -1.0485e-01,  8.3811e-04,\n",
       "                       -6.2494e-02,  1.1070e-01, -1.6919e-01,  6.5585e-02,  9.5728e-02,\n",
       "                       -8.5827e-02, -1.0011e-01, -7.3816e-02,  1.5433e-01, -5.8502e-02,\n",
       "                        1.8834e-02, -6.9894e-02,  1.2320e-01, -6.4824e-02,  2.3403e-01],\n",
       "                      [-5.9566e-02, -8.8390e-02, -1.2549e-01,  1.9986e-02, -1.3198e-01,\n",
       "                        2.4052e-01, -1.2270e-01, -1.0511e-01,  2.6809e-01,  1.0866e-01,\n",
       "                        1.3738e-01,  2.2189e-02, -3.6283e-02, -8.3957e-02,  3.3513e-03,\n",
       "                       -4.6779e-02,  7.7518e-02, -1.2778e-01,  1.3518e-01, -4.2379e-02,\n",
       "                       -6.3630e-02, -2.0326e-01, -9.4790e-02,  9.3485e-02, -2.0508e-01,\n",
       "                       -3.4582e-02, -1.8295e-01,  1.4679e-01,  1.9059e-01, -7.4412e-02],\n",
       "                      [-3.4995e-03,  1.3597e-01, -6.8708e-02, -8.8499e-03,  1.2801e-01,\n",
       "                        1.6700e-01, -9.5022e-02,  2.2541e-01,  2.6925e-01, -8.1418e-02,\n",
       "                        7.8096e-02,  4.7086e-02,  1.0911e-02,  1.2308e-02,  9.9886e-04,\n",
       "                        9.3777e-02,  1.1744e-01, -9.4767e-02, -2.0384e-01, -7.3802e-02,\n",
       "                       -3.7567e-02, -2.2496e-02, -1.2320e-01, -2.4796e-03, -2.9595e-01,\n",
       "                        1.0044e-01, -1.4939e-01,  1.5951e-01, -1.4292e-01,  3.4325e-02],\n",
       "                      [-3.1491e-02,  2.3832e-03, -1.4274e-01, -1.0578e-01,  1.6987e-01,\n",
       "                        4.4722e-02, -1.7289e-01,  1.8527e-01,  2.0079e-01, -1.7327e-01,\n",
       "                        1.7695e-01,  2.7657e-02,  4.4883e-02,  6.2445e-02, -7.9720e-04,\n",
       "                        1.0105e-01, -3.2926e-02, -1.3371e-01, -2.8672e-01,  6.4118e-02,\n",
       "                       -7.1125e-02,  1.5525e-01, -1.8737e-01,  8.1735e-02, -1.5962e-01,\n",
       "                        4.5336e-02,  3.3533e-02,  1.3642e-01, -3.8362e-02,  4.0673e-02],\n",
       "                      [ 1.6397e-01,  6.8609e-02,  1.7914e-01,  6.3678e-02, -1.7740e-01,\n",
       "                       -1.0213e-01,  7.9344e-02, -2.8484e-02, -4.0117e-01,  1.6960e-01,\n",
       "                       -2.8582e-02, -1.9001e-02,  8.0240e-03,  2.5397e-02, -2.1171e-03,\n",
       "                       -2.9664e-02, -1.8880e-02,  2.3491e-01,  2.7207e-01,  2.1715e-02,\n",
       "                        2.0162e-02,  1.0394e-01,  7.2040e-02, -6.7114e-02,  1.4845e-01,\n",
       "                       -3.3104e-02,  1.7107e-01, -9.0216e-02,  9.6797e-02, -1.3790e-02],\n",
       "                      [ 1.2372e-01,  7.8406e-02,  1.6794e-01,  6.7386e-02, -1.2316e-01,\n",
       "                       -1.2623e-01,  2.7770e-02, -1.8023e-01, -2.6114e-01,  1.7646e-01,\n",
       "                       -3.5417e-02, -4.1999e-03,  4.7895e-03,  5.1519e-02, -1.1621e-03,\n",
       "                        1.9312e-02,  1.7984e-02,  2.4792e-01,  1.0529e-01,  4.0536e-03,\n",
       "                        1.8738e-01,  7.2563e-02,  1.3339e-01, -8.8581e-02,  1.8168e-01,\n",
       "                        1.2372e-02,  9.6444e-02, -1.4811e-01, -2.3791e-01,  6.1985e-02],\n",
       "                      [ 1.1502e-01, -1.3083e-01,  4.7778e-02,  5.8403e-02,  2.1874e-02,\n",
       "                        1.6192e-02,  9.0613e-02, -4.2994e-02, -4.5914e-01,  7.7566e-02,\n",
       "                        1.8398e-02, -1.1680e-02, -5.8537e-03, -1.4732e-02,  9.3856e-04,\n",
       "                       -4.4748e-02,  1.0478e-02,  8.2098e-02,  1.7914e-01, -1.5791e-02,\n",
       "                        6.0109e-02, -1.7902e-01,  1.0216e-01, -5.2806e-02,  1.7474e-01,\n",
       "                       -3.4336e-02,  2.0950e-02, -3.3498e-03,  6.4155e-02, -2.2901e-02],\n",
       "                      [ 1.6097e-01,  1.4896e-01,  3.6928e-01,  9.0947e-02, -2.0448e-01,\n",
       "                       -5.0142e-02,  3.4545e-02,  4.8734e-02, -1.5546e-01,  1.1789e-02,\n",
       "                        2.7346e-02, -2.2496e-02,  2.3223e-02, -7.2487e-02, -2.6349e-03,\n",
       "                        8.3479e-02,  1.1105e-01,  3.5491e-01,  1.4893e-01, -1.9257e-01,\n",
       "                        2.2936e-01, -1.1969e-01,  1.9488e-01, -1.5525e-01,  9.9683e-02,\n",
       "                       -1.1463e-01,  7.6490e-02,  3.0010e-03, -4.1793e-02, -2.1081e-01],\n",
       "                      [-2.7180e-01, -7.5835e-02, -3.0651e-01, -8.4293e-02,  1.9937e-01,\n",
       "                        2.3790e-02,  1.0816e-01, -1.9613e-01, -1.0905e-01, -4.5888e-02,\n",
       "                        5.6817e-03,  1.5357e-02,  1.7079e-03,  1.3619e-01,  9.6020e-04,\n",
       "                       -7.4737e-02, -2.4907e-01, -2.5955e-01, -1.0708e-01,  2.2964e-01,\n",
       "                       -9.5709e-02,  7.1618e-02, -2.3380e-01,  1.1840e-01,  1.8036e-01,\n",
       "                        1.8959e-02,  1.7594e-01, -1.6732e-02, -1.9099e-02,  3.6629e-01],\n",
       "                      [ 2.2396e-02,  2.0415e-01,  1.6125e-01,  6.7195e-03, -4.6692e-02,\n",
       "                        8.7123e-02, -1.7638e-02,  7.3075e-02,  2.8487e-01, -2.9213e-02,\n",
       "                        1.6493e-02, -5.5392e-02,  2.1317e-02, -2.9914e-02, -1.3125e-03,\n",
       "                        5.4581e-03,  5.4555e-02,  2.0002e-01, -6.8696e-02, -2.3194e-02,\n",
       "                        4.4340e-03,  2.2908e-01,  1.8235e-01, -9.8035e-02,  1.4327e-01,\n",
       "                       -1.1810e-01,  5.4234e-02, -7.6718e-02,  3.7654e-02, -1.7441e-01]])),\n",
       "             ('dense0.bias',\n",
       "              tensor([-0.4114,  0.0234,  0.2938, -0.6735, -0.6598, -0.0888, -0.2826, -0.1262,\n",
       "                      -0.0328, -0.2498,  0.2107,  0.3318,  0.1844,  0.6723, -0.5277,  0.2847])),\n",
       "             ('dense1.weight',\n",
       "              tensor([[-1.0029e-01,  3.5433e-02,  9.5633e-02,  1.3159e-02, -1.5739e-01,\n",
       "                        5.9177e-03,  3.9960e-02,  8.3100e-02, -1.0575e-01, -6.6744e-02,\n",
       "                        1.1203e-01, -2.8251e-02,  6.0239e-02,  1.4483e-01, -6.1760e-02,\n",
       "                        2.3430e-02],\n",
       "                      [-1.5397e-02,  7.0438e-02,  8.9232e-02,  1.1414e-01,  3.2008e-02,\n",
       "                       -4.3898e-02,  4.3104e-02,  1.6362e-01,  1.1571e-02, -1.1215e-01,\n",
       "                       -1.1673e-01, -1.0538e-01, -3.0702e-03, -9.2608e-02, -1.0461e-02,\n",
       "                       -1.2220e-01],\n",
       "                      [-7.9867e-03,  3.8346e-02, -5.3236e-02, -2.1521e-01, -1.6859e-01,\n",
       "                        4.4660e-02, -2.1706e-01, -1.4724e-02,  1.0233e-02, -7.1916e-03,\n",
       "                       -4.7186e-03, -1.1945e-01, -1.6889e-03,  3.5186e-02,  1.6593e-02,\n",
       "                       -2.0873e-02],\n",
       "                      [-2.8205e-02,  1.7433e-02,  4.7703e-02, -9.0373e-02, -1.7639e-01,\n",
       "                       -8.3070e-03, -2.4661e-02, -1.3425e-02,  4.8283e-03, -4.5447e-02,\n",
       "                       -5.9039e-02,  1.8554e-02, -7.4814e-02,  5.5257e-02, -1.3383e-01,\n",
       "                        1.5375e-01],\n",
       "                      [-2.7570e-02,  1.1025e-01, -6.6355e-03,  2.5540e-02,  7.7272e-03,\n",
       "                       -7.6684e-03,  7.5805e-02,  2.6334e-02, -8.9124e-02, -9.8473e-02,\n",
       "                        1.1911e-03, -2.9950e-02,  9.0739e-02, -9.9149e-02, -6.9162e-02,\n",
       "                       -2.5472e-02],\n",
       "                      [-2.0401e-02,  9.4914e-03, -2.1900e-02, -3.5008e-02, -9.4557e-03,\n",
       "                       -6.2976e-02, -5.4865e-02,  1.1933e-02, -5.4022e-02, -2.1930e-02,\n",
       "                       -1.4762e-02, -1.2155e-02, -8.9887e-03, -1.0034e-03, -1.4171e-02,\n",
       "                        4.5045e-02],\n",
       "                      [ 9.6183e-03,  2.1495e-04,  1.3174e-01, -5.0008e-02, -3.7343e-02,\n",
       "                       -9.1560e-03,  8.5503e-02,  3.8170e-02, -4.0676e-02, -7.2641e-02,\n",
       "                        6.5812e-02,  6.3964e-02,  9.5070e-03,  1.2271e-01, -2.0301e-01,\n",
       "                       -7.8317e-03],\n",
       "                      [-4.4533e-02, -1.1646e-02, -6.3100e-02,  2.8015e-03, -1.2401e-01,\n",
       "                       -1.0696e-02, -1.2595e-01, -1.1920e-01, -3.7907e-02, -6.3052e-02,\n",
       "                        1.0230e-01,  1.6879e-01,  1.5000e-02,  1.4551e-01, -3.0861e-02,\n",
       "                       -4.2142e-02],\n",
       "                      [ 1.7671e-02,  2.3459e-02, -1.0503e-02, -4.3094e-02, -4.7934e-02,\n",
       "                       -3.4437e-02, -4.9786e-02, -4.2902e-03, -5.2672e-02, -4.7267e-02,\n",
       "                        1.1950e-02, -2.7750e-03,  2.1352e-02,  1.8602e-03, -4.1997e-03,\n",
       "                       -5.2404e-03],\n",
       "                      [-2.9861e-02,  3.1112e-02, -6.1760e-02, -8.4253e-02, -8.0742e-02,\n",
       "                       -1.0720e-01,  2.2662e-02,  4.4112e-02, -7.4107e-03, -2.1578e-02,\n",
       "                       -2.4016e-03, -6.2812e-03,  7.1033e-02,  9.2806e-03, -1.4182e-02,\n",
       "                        1.2278e-01],\n",
       "                      [ 8.4288e-02,  5.7949e-02,  2.5945e-02,  2.9725e-02, -6.0684e-02,\n",
       "                       -1.3030e-01,  1.6543e-02, -2.7941e-02, -2.6375e-02, -4.9556e-02,\n",
       "                        1.3800e-02, -6.7544e-02,  2.2766e-02, -7.2378e-02,  4.9153e-03,\n",
       "                       -3.2783e-04],\n",
       "                      [-7.3964e-02, -6.4645e-02,  4.6653e-02, -7.6985e-02,  1.3465e-03,\n",
       "                       -1.7300e-01, -7.5157e-02, -1.9378e-02, -9.2841e-02, -7.4327e-02,\n",
       "                        6.4683e-02,  3.9730e-02,  1.2609e-01,  4.2190e-02,  9.9833e-04,\n",
       "                        3.9423e-02],\n",
       "                      [-1.4251e-01,  8.4177e-02,  1.1208e-01, -6.1484e-02, -1.4042e-01,\n",
       "                       -4.0361e-03,  1.0633e-01,  6.1534e-02,  7.7934e-02,  2.6050e-02,\n",
       "                       -6.7746e-02, -1.6287e-02, -1.8096e-02, -5.9588e-02, -7.6280e-02,\n",
       "                       -6.2349e-02],\n",
       "                      [ 2.3734e-02,  1.0532e-02, -4.1594e-03,  1.9937e-01,  2.1020e-01,\n",
       "                        7.6599e-03,  6.1847e-02,  2.3863e-02,  2.3351e-02,  6.5871e-02,\n",
       "                       -5.5118e-02, -5.7730e-02, -3.9478e-02, -1.9481e-01,  1.3892e-01,\n",
       "                       -5.3119e-02],\n",
       "                      [-3.0232e-02,  2.4633e-02, -2.5868e-02, -1.2243e-01,  5.9536e-02,\n",
       "                       -6.3949e-02, -2.5579e-02,  3.5349e-03, -5.4313e-02, -3.1656e-02,\n",
       "                        1.0797e-02, -1.4735e-01,  5.0667e-02,  3.1610e-02, -1.0938e-01,\n",
       "                        3.3488e-02],\n",
       "                      [ 5.8865e-02,  1.3805e-01, -1.0589e-01,  7.1470e-02,  4.8713e-02,\n",
       "                       -1.0113e-01, -1.3802e-01, -5.2091e-02, -8.1645e-02, -3.6023e-02,\n",
       "                       -1.6549e-02, -1.0660e-02,  6.1852e-02, -9.1881e-02,  9.5874e-03,\n",
       "                        3.0770e-02]])),\n",
       "             ('dense1.bias',\n",
       "              tensor([ 1.5042e-01, -2.9310e-01,  3.6270e-01,  1.4214e-01, -4.5375e-01,\n",
       "                      -7.2463e-05,  4.3754e-01,  4.5048e-01, -6.3706e-04,  1.1343e-01,\n",
       "                      -1.4602e-01,  2.6893e-01,  2.0174e-01, -1.1144e+00,  1.3448e-01,\n",
       "                      -7.5206e-02])),\n",
       "             ('dense2.weight',\n",
       "              tensor([[ 0.1061, -0.1682,  0.0651,  0.1792, -0.1413, -0.0089,  0.0388,  0.0622,\n",
       "                       -0.0027,  0.1397, -0.0049,  0.0388,  0.1168, -0.0895,  0.0843,  0.1928]])),\n",
       "             ('dense2.bias', tensor([1.4961]))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classificador.state_dict(), './models/classificador_binario.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 5: Classificação binária Brest Cancer - carregar o classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2: Carregamento do classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch ainda não tinha estrutura para fazer CV e também salvar a estrutura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.normal_(self.dense0.weight, mean=0.0, std=0.05) # configuração de inicialização do keras\n",
    "\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.normal_(self.dense1.weight, mean=0.0, std=0.05)\n",
    "        self.activation1 = nn.ReLU()\n",
    "\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.output = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        \n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "    \n",
    "        X = self.dense2(X)\n",
    "        X = self.activation2(X)\n",
    "        X = self.dropout(X)\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dense0.weight',\n",
       "              tensor([[-2.9874e-01, -2.4318e-02, -2.8736e-01, -6.0441e-02,  9.2354e-02,\n",
       "                        1.2011e-01,  1.1482e-01, -2.1392e-01, -1.8156e-01, -6.7864e-02,\n",
       "                       -8.2370e-02, -5.9511e-02,  4.1297e-02,  1.0247e-01, -2.9426e-03,\n",
       "                       -8.9640e-02, -1.2226e-01, -1.1409e-01, -4.9019e-02,  1.6793e-01,\n",
       "                       -4.3635e-02,  4.4012e-02, -9.6899e-02,  6.4170e-02,  9.2050e-02,\n",
       "                       -1.0689e-01,  1.0891e-01, -8.0071e-03, -5.0805e-03,  1.2771e-01],\n",
       "                      [ 3.0755e-02, -3.1563e-02,  1.0958e-01,  2.5881e-02,  1.1664e-02,\n",
       "                        2.1585e-02, -1.1671e-01,  1.7554e-01,  3.4069e-01,  1.8274e-01,\n",
       "                        1.0541e-02,  5.2146e-02, -8.5857e-02,  1.2377e-02,  6.8551e-03,\n",
       "                       -6.6760e-03,  9.6605e-02, -3.8297e-02,  5.5081e-02, -9.7353e-02,\n",
       "                        1.9366e-02, -1.7553e-01, -3.7163e-02, -4.2237e-02, -2.3373e-01,\n",
       "                        7.1189e-02, -2.1782e-01,  4.7659e-02,  1.6429e-01, -6.3003e-02],\n",
       "                      [ 4.9967e-02,  8.0892e-02,  2.2192e-01,  1.4613e-01, -9.8111e-02,\n",
       "                        1.0391e-02, -1.2304e-01,  8.8150e-02,  2.7843e-01,  3.0207e-01,\n",
       "                        1.5305e-01,  1.0834e-02, -2.0710e-02, -1.2913e-01,  1.2625e-04,\n",
       "                        4.7569e-02,  1.6991e-01,  3.7408e-03,  1.5768e-01, -1.0286e-01,\n",
       "                        9.2780e-02, -8.2548e-02,  9.9204e-02, -5.6333e-02, -1.4469e-01,\n",
       "                        9.3571e-02, -2.0798e-01,  2.1017e-01,  4.2581e-02, -1.3277e-01],\n",
       "                      [-4.2734e-01, -1.2327e-01, -4.6343e-01, -5.3005e-03,  2.6898e-01,\n",
       "                       -4.7689e-02,  1.2500e-01, -2.6270e-01, -1.2325e-01,  2.9475e-03,\n",
       "                        9.3693e-02, -6.1424e-02,  8.6546e-03,  2.8584e-01,  1.4413e-03,\n",
       "                       -1.2435e-01, -2.2699e-01, -2.9913e-01,  7.3461e-04,  3.1369e-01,\n",
       "                       -1.4192e-01, -2.8628e-02, -2.0760e-01,  1.7534e-01,  1.0859e-01,\n",
       "                       -4.1695e-02,  3.4224e-02,  5.4715e-02,  2.9671e-03,  3.8898e-01],\n",
       "                      [-3.2812e-01, -9.1117e-02, -2.5910e-01, -8.8728e-02,  1.3875e-01,\n",
       "                       -4.8991e-02,  9.6003e-02, -2.0751e-01, -8.5803e-02, -6.0683e-02,\n",
       "                       -7.1936e-03, -1.9585e-02,  5.1971e-03,  1.4059e-01, -1.5283e-04,\n",
       "                       -1.2096e-01, -1.8908e-01, -3.5522e-01,  5.9391e-03,  3.1942e-01,\n",
       "                       -2.2382e-01,  1.2354e-01, -2.8997e-01,  1.5828e-01,  2.8433e-01,\n",
       "                       -3.1202e-02,  6.6569e-02, -8.4207e-02,  4.6430e-03,  2.5388e-01],\n",
       "                      [-1.0176e-01, -7.4125e-02, -1.1683e-02, -2.0187e-02, -8.7531e-02,\n",
       "                        1.3622e-02, -5.4220e-02, -4.2454e-02,  7.8827e-02,  1.1133e-01,\n",
       "                        2.1610e-02,  2.9047e-02,  9.1480e-03,  2.6509e-03, -1.4393e-03,\n",
       "                        1.5133e-02,  2.8915e-03, -4.1525e-02, -5.0242e-02, -2.7919e-02,\n",
       "                       -5.0075e-02, -1.7744e-02, -9.2409e-02,  4.1364e-02, -2.2209e-01,\n",
       "                       -9.3783e-02,  7.6536e-02,  3.2758e-02,  3.2598e-02, -1.6228e-01],\n",
       "                      [-1.6661e-01, -8.0217e-02, -1.1349e-01,  1.9623e-02,  4.6204e-03,\n",
       "                       -4.4442e-02, -5.8220e-02, -1.5612e-01,  2.2321e-01,  6.3275e-02,\n",
       "                        1.9530e-01, -4.3959e-03, -2.9385e-02, -1.0485e-01,  8.3811e-04,\n",
       "                       -6.2494e-02,  1.1070e-01, -1.6919e-01,  6.5585e-02,  9.5728e-02,\n",
       "                       -8.5827e-02, -1.0011e-01, -7.3816e-02,  1.5433e-01, -5.8502e-02,\n",
       "                        1.8834e-02, -6.9894e-02,  1.2320e-01, -6.4824e-02,  2.3403e-01],\n",
       "                      [-5.9566e-02, -8.8390e-02, -1.2549e-01,  1.9986e-02, -1.3198e-01,\n",
       "                        2.4052e-01, -1.2270e-01, -1.0511e-01,  2.6809e-01,  1.0866e-01,\n",
       "                        1.3738e-01,  2.2189e-02, -3.6283e-02, -8.3957e-02,  3.3513e-03,\n",
       "                       -4.6779e-02,  7.7518e-02, -1.2778e-01,  1.3518e-01, -4.2379e-02,\n",
       "                       -6.3630e-02, -2.0326e-01, -9.4790e-02,  9.3485e-02, -2.0508e-01,\n",
       "                       -3.4582e-02, -1.8295e-01,  1.4679e-01,  1.9059e-01, -7.4412e-02],\n",
       "                      [-3.4995e-03,  1.3597e-01, -6.8708e-02, -8.8499e-03,  1.2801e-01,\n",
       "                        1.6700e-01, -9.5022e-02,  2.2541e-01,  2.6925e-01, -8.1418e-02,\n",
       "                        7.8096e-02,  4.7086e-02,  1.0911e-02,  1.2308e-02,  9.9886e-04,\n",
       "                        9.3777e-02,  1.1744e-01, -9.4767e-02, -2.0384e-01, -7.3802e-02,\n",
       "                       -3.7567e-02, -2.2496e-02, -1.2320e-01, -2.4796e-03, -2.9595e-01,\n",
       "                        1.0044e-01, -1.4939e-01,  1.5951e-01, -1.4292e-01,  3.4325e-02],\n",
       "                      [-3.1491e-02,  2.3832e-03, -1.4274e-01, -1.0578e-01,  1.6987e-01,\n",
       "                        4.4722e-02, -1.7289e-01,  1.8527e-01,  2.0079e-01, -1.7327e-01,\n",
       "                        1.7695e-01,  2.7657e-02,  4.4883e-02,  6.2445e-02, -7.9720e-04,\n",
       "                        1.0105e-01, -3.2926e-02, -1.3371e-01, -2.8672e-01,  6.4118e-02,\n",
       "                       -7.1125e-02,  1.5525e-01, -1.8737e-01,  8.1735e-02, -1.5962e-01,\n",
       "                        4.5336e-02,  3.3533e-02,  1.3642e-01, -3.8362e-02,  4.0673e-02],\n",
       "                      [ 1.6397e-01,  6.8609e-02,  1.7914e-01,  6.3678e-02, -1.7740e-01,\n",
       "                       -1.0213e-01,  7.9344e-02, -2.8484e-02, -4.0117e-01,  1.6960e-01,\n",
       "                       -2.8582e-02, -1.9001e-02,  8.0240e-03,  2.5397e-02, -2.1171e-03,\n",
       "                       -2.9664e-02, -1.8880e-02,  2.3491e-01,  2.7207e-01,  2.1715e-02,\n",
       "                        2.0162e-02,  1.0394e-01,  7.2040e-02, -6.7114e-02,  1.4845e-01,\n",
       "                       -3.3104e-02,  1.7107e-01, -9.0216e-02,  9.6797e-02, -1.3790e-02],\n",
       "                      [ 1.2372e-01,  7.8406e-02,  1.6794e-01,  6.7386e-02, -1.2316e-01,\n",
       "                       -1.2623e-01,  2.7770e-02, -1.8023e-01, -2.6114e-01,  1.7646e-01,\n",
       "                       -3.5417e-02, -4.1999e-03,  4.7895e-03,  5.1519e-02, -1.1621e-03,\n",
       "                        1.9312e-02,  1.7984e-02,  2.4792e-01,  1.0529e-01,  4.0536e-03,\n",
       "                        1.8738e-01,  7.2563e-02,  1.3339e-01, -8.8581e-02,  1.8168e-01,\n",
       "                        1.2372e-02,  9.6444e-02, -1.4811e-01, -2.3791e-01,  6.1985e-02],\n",
       "                      [ 1.1502e-01, -1.3083e-01,  4.7778e-02,  5.8403e-02,  2.1874e-02,\n",
       "                        1.6192e-02,  9.0613e-02, -4.2994e-02, -4.5914e-01,  7.7566e-02,\n",
       "                        1.8398e-02, -1.1680e-02, -5.8537e-03, -1.4732e-02,  9.3856e-04,\n",
       "                       -4.4748e-02,  1.0478e-02,  8.2098e-02,  1.7914e-01, -1.5791e-02,\n",
       "                        6.0109e-02, -1.7902e-01,  1.0216e-01, -5.2806e-02,  1.7474e-01,\n",
       "                       -3.4336e-02,  2.0950e-02, -3.3498e-03,  6.4155e-02, -2.2901e-02],\n",
       "                      [ 1.6097e-01,  1.4896e-01,  3.6928e-01,  9.0947e-02, -2.0448e-01,\n",
       "                       -5.0142e-02,  3.4545e-02,  4.8734e-02, -1.5546e-01,  1.1789e-02,\n",
       "                        2.7346e-02, -2.2496e-02,  2.3223e-02, -7.2487e-02, -2.6349e-03,\n",
       "                        8.3479e-02,  1.1105e-01,  3.5491e-01,  1.4893e-01, -1.9257e-01,\n",
       "                        2.2936e-01, -1.1969e-01,  1.9488e-01, -1.5525e-01,  9.9683e-02,\n",
       "                       -1.1463e-01,  7.6490e-02,  3.0010e-03, -4.1793e-02, -2.1081e-01],\n",
       "                      [-2.7180e-01, -7.5835e-02, -3.0651e-01, -8.4293e-02,  1.9937e-01,\n",
       "                        2.3790e-02,  1.0816e-01, -1.9613e-01, -1.0905e-01, -4.5888e-02,\n",
       "                        5.6817e-03,  1.5357e-02,  1.7079e-03,  1.3619e-01,  9.6020e-04,\n",
       "                       -7.4737e-02, -2.4907e-01, -2.5955e-01, -1.0708e-01,  2.2964e-01,\n",
       "                       -9.5709e-02,  7.1618e-02, -2.3380e-01,  1.1840e-01,  1.8036e-01,\n",
       "                        1.8959e-02,  1.7594e-01, -1.6732e-02, -1.9099e-02,  3.6629e-01],\n",
       "                      [ 2.2396e-02,  2.0415e-01,  1.6125e-01,  6.7195e-03, -4.6692e-02,\n",
       "                        8.7123e-02, -1.7638e-02,  7.3075e-02,  2.8487e-01, -2.9213e-02,\n",
       "                        1.6493e-02, -5.5392e-02,  2.1317e-02, -2.9914e-02, -1.3125e-03,\n",
       "                        5.4581e-03,  5.4555e-02,  2.0002e-01, -6.8696e-02, -2.3194e-02,\n",
       "                        4.4340e-03,  2.2908e-01,  1.8235e-01, -9.8035e-02,  1.4327e-01,\n",
       "                       -1.1810e-01,  5.4234e-02, -7.6718e-02,  3.7654e-02, -1.7441e-01]])),\n",
       "             ('dense0.bias',\n",
       "              tensor([-0.4114,  0.0234,  0.2938, -0.6735, -0.6598, -0.0888, -0.2826, -0.1262,\n",
       "                      -0.0328, -0.2498,  0.2107,  0.3318,  0.1844,  0.6723, -0.5277,  0.2847])),\n",
       "             ('dense1.weight',\n",
       "              tensor([[-1.0029e-01,  3.5433e-02,  9.5633e-02,  1.3159e-02, -1.5739e-01,\n",
       "                        5.9177e-03,  3.9960e-02,  8.3100e-02, -1.0575e-01, -6.6744e-02,\n",
       "                        1.1203e-01, -2.8251e-02,  6.0239e-02,  1.4483e-01, -6.1760e-02,\n",
       "                        2.3430e-02],\n",
       "                      [-1.5397e-02,  7.0438e-02,  8.9232e-02,  1.1414e-01,  3.2008e-02,\n",
       "                       -4.3898e-02,  4.3104e-02,  1.6362e-01,  1.1571e-02, -1.1215e-01,\n",
       "                       -1.1673e-01, -1.0538e-01, -3.0702e-03, -9.2608e-02, -1.0461e-02,\n",
       "                       -1.2220e-01],\n",
       "                      [-7.9867e-03,  3.8346e-02, -5.3236e-02, -2.1521e-01, -1.6859e-01,\n",
       "                        4.4660e-02, -2.1706e-01, -1.4724e-02,  1.0233e-02, -7.1916e-03,\n",
       "                       -4.7186e-03, -1.1945e-01, -1.6889e-03,  3.5186e-02,  1.6593e-02,\n",
       "                       -2.0873e-02],\n",
       "                      [-2.8205e-02,  1.7433e-02,  4.7703e-02, -9.0373e-02, -1.7639e-01,\n",
       "                       -8.3070e-03, -2.4661e-02, -1.3425e-02,  4.8283e-03, -4.5447e-02,\n",
       "                       -5.9039e-02,  1.8554e-02, -7.4814e-02,  5.5257e-02, -1.3383e-01,\n",
       "                        1.5375e-01],\n",
       "                      [-2.7570e-02,  1.1025e-01, -6.6355e-03,  2.5540e-02,  7.7272e-03,\n",
       "                       -7.6684e-03,  7.5805e-02,  2.6334e-02, -8.9124e-02, -9.8473e-02,\n",
       "                        1.1911e-03, -2.9950e-02,  9.0739e-02, -9.9149e-02, -6.9162e-02,\n",
       "                       -2.5472e-02],\n",
       "                      [-2.0401e-02,  9.4914e-03, -2.1900e-02, -3.5008e-02, -9.4557e-03,\n",
       "                       -6.2976e-02, -5.4865e-02,  1.1933e-02, -5.4022e-02, -2.1930e-02,\n",
       "                       -1.4762e-02, -1.2155e-02, -8.9887e-03, -1.0034e-03, -1.4171e-02,\n",
       "                        4.5045e-02],\n",
       "                      [ 9.6183e-03,  2.1495e-04,  1.3174e-01, -5.0008e-02, -3.7343e-02,\n",
       "                       -9.1560e-03,  8.5503e-02,  3.8170e-02, -4.0676e-02, -7.2641e-02,\n",
       "                        6.5812e-02,  6.3964e-02,  9.5070e-03,  1.2271e-01, -2.0301e-01,\n",
       "                       -7.8317e-03],\n",
       "                      [-4.4533e-02, -1.1646e-02, -6.3100e-02,  2.8015e-03, -1.2401e-01,\n",
       "                       -1.0696e-02, -1.2595e-01, -1.1920e-01, -3.7907e-02, -6.3052e-02,\n",
       "                        1.0230e-01,  1.6879e-01,  1.5000e-02,  1.4551e-01, -3.0861e-02,\n",
       "                       -4.2142e-02],\n",
       "                      [ 1.7671e-02,  2.3459e-02, -1.0503e-02, -4.3094e-02, -4.7934e-02,\n",
       "                       -3.4437e-02, -4.9786e-02, -4.2902e-03, -5.2672e-02, -4.7267e-02,\n",
       "                        1.1950e-02, -2.7750e-03,  2.1352e-02,  1.8602e-03, -4.1997e-03,\n",
       "                       -5.2404e-03],\n",
       "                      [-2.9861e-02,  3.1112e-02, -6.1760e-02, -8.4253e-02, -8.0742e-02,\n",
       "                       -1.0720e-01,  2.2662e-02,  4.4112e-02, -7.4107e-03, -2.1578e-02,\n",
       "                       -2.4016e-03, -6.2812e-03,  7.1033e-02,  9.2806e-03, -1.4182e-02,\n",
       "                        1.2278e-01],\n",
       "                      [ 8.4288e-02,  5.7949e-02,  2.5945e-02,  2.9725e-02, -6.0684e-02,\n",
       "                       -1.3030e-01,  1.6543e-02, -2.7941e-02, -2.6375e-02, -4.9556e-02,\n",
       "                        1.3800e-02, -6.7544e-02,  2.2766e-02, -7.2378e-02,  4.9153e-03,\n",
       "                       -3.2783e-04],\n",
       "                      [-7.3964e-02, -6.4645e-02,  4.6653e-02, -7.6985e-02,  1.3465e-03,\n",
       "                       -1.7300e-01, -7.5157e-02, -1.9378e-02, -9.2841e-02, -7.4327e-02,\n",
       "                        6.4683e-02,  3.9730e-02,  1.2609e-01,  4.2190e-02,  9.9833e-04,\n",
       "                        3.9423e-02],\n",
       "                      [-1.4251e-01,  8.4177e-02,  1.1208e-01, -6.1484e-02, -1.4042e-01,\n",
       "                       -4.0361e-03,  1.0633e-01,  6.1534e-02,  7.7934e-02,  2.6050e-02,\n",
       "                       -6.7746e-02, -1.6287e-02, -1.8096e-02, -5.9588e-02, -7.6280e-02,\n",
       "                       -6.2349e-02],\n",
       "                      [ 2.3734e-02,  1.0532e-02, -4.1594e-03,  1.9937e-01,  2.1020e-01,\n",
       "                        7.6599e-03,  6.1847e-02,  2.3863e-02,  2.3351e-02,  6.5871e-02,\n",
       "                       -5.5118e-02, -5.7730e-02, -3.9478e-02, -1.9481e-01,  1.3892e-01,\n",
       "                       -5.3119e-02],\n",
       "                      [-3.0232e-02,  2.4633e-02, -2.5868e-02, -1.2243e-01,  5.9536e-02,\n",
       "                       -6.3949e-02, -2.5579e-02,  3.5349e-03, -5.4313e-02, -3.1656e-02,\n",
       "                        1.0797e-02, -1.4735e-01,  5.0667e-02,  3.1610e-02, -1.0938e-01,\n",
       "                        3.3488e-02],\n",
       "                      [ 5.8865e-02,  1.3805e-01, -1.0589e-01,  7.1470e-02,  4.8713e-02,\n",
       "                       -1.0113e-01, -1.3802e-01, -5.2091e-02, -8.1645e-02, -3.6023e-02,\n",
       "                       -1.6549e-02, -1.0660e-02,  6.1852e-02, -9.1881e-02,  9.5874e-03,\n",
       "                        3.0770e-02]])),\n",
       "             ('dense1.bias',\n",
       "              tensor([ 1.5042e-01, -2.9310e-01,  3.6270e-01,  1.4214e-01, -4.5375e-01,\n",
       "                      -7.2463e-05,  4.3754e-01,  4.5048e-01, -6.3706e-04,  1.1343e-01,\n",
       "                      -1.4602e-01,  2.6893e-01,  2.0174e-01, -1.1144e+00,  1.3448e-01,\n",
       "                      -7.5206e-02])),\n",
       "             ('dense2.weight',\n",
       "              tensor([[ 0.1061, -0.1682,  0.0651,  0.1792, -0.1413, -0.0089,  0.0388,  0.0622,\n",
       "                       -0.0027,  0.1397, -0.0049,  0.0388,  0.1168, -0.0895,  0.0843,  0.1928]])),\n",
       "             ('dense2.bias', tensor([1.4961]))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador = classificador_torch()\n",
    "state_dict = torch.load('./models/classificador_binario.pth')\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3: Previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo = torch.tensor([15.80, 8.34, 118, 900, 0.10, 0.26, 0.08, 0.134, 0.178, \n",
    "                     0.20, 0.05, 1098, 0.87, 4500, 145.2, 0.005, 0.04, 0.05, 0.015, \n",
    "                     0.03, 0.007, 23.15, 16.64, 178.5, 2018, 0.14, 0.185, 0.84, 158, 0.363], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.eval()\n",
    "previsao = classificador(novo)\n",
    "previsao = (previsao.detach().numpy() > 0.5)\n",
    "previsao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.7912],\n",
       "        [0.9314],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9644],\n",
       "        [0.8664],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.7069],\n",
       "        [0.9999],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5038],\n",
       "        [0.9743],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.6635],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.5271],\n",
       "        [0.9645],\n",
       "        [0.9711],\n",
       "        [0.9422],\n",
       "        [0.9589],\n",
       "        [0.9548],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9953],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9321],\n",
       "        [1.0000],\n",
       "        [0.9837],\n",
       "        [1.0000],\n",
       "        [0.5947],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9815],\n",
       "        [1.0000],\n",
       "        [0.9996],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9938],\n",
       "        [0.5000],\n",
       "        [0.9297],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9363],\n",
       "        [0.9988],\n",
       "        [0.9981],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9820],\n",
       "        [0.5000],\n",
       "        [0.5910],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.8503],\n",
       "        [0.6935],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9752],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9887],\n",
       "        [1.0000],\n",
       "        [0.9984],\n",
       "        [0.9532],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9988],\n",
       "        [1.0000],\n",
       "        [0.9915],\n",
       "        [0.5000],\n",
       "        [0.9984],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9756],\n",
       "        [1.0000],\n",
       "        [0.9974],\n",
       "        [0.9600],\n",
       "        [0.9779],\n",
       "        [1.0000],\n",
       "        [0.5499],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9084],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9372],\n",
       "        [0.9756],\n",
       "        [0.7476],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9988],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.6038],\n",
       "        [0.5000],\n",
       "        [0.7769],\n",
       "        [0.9977],\n",
       "        [0.9863],\n",
       "        [0.5000],\n",
       "        [0.9885],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9974],\n",
       "        [0.9308],\n",
       "        [0.9699],\n",
       "        [0.9575],\n",
       "        [0.5000],\n",
       "        [0.9603],\n",
       "        [0.5000],\n",
       "        [0.9222],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9998],\n",
       "        [0.7344],\n",
       "        [0.9992],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9991],\n",
       "        [0.9221],\n",
       "        [0.9708],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9917],\n",
       "        [0.5000],\n",
       "        [0.9907],\n",
       "        [0.9975],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9989],\n",
       "        [0.9830],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9998],\n",
       "        [0.9991],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.7837],\n",
       "        [0.9960],\n",
       "        [0.9458],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9792],\n",
       "        [0.5000],\n",
       "        [0.9888],\n",
       "        [0.5000],\n",
       "        [0.9155],\n",
       "        [0.9999],\n",
       "        [0.9181],\n",
       "        [0.5836],\n",
       "        [0.9853],\n",
       "        [1.0000],\n",
       "        [0.6046],\n",
       "        [0.8364],\n",
       "        [0.9531],\n",
       "        [0.8652],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.8711],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.6745],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9991],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.8646],\n",
       "        [0.5000],\n",
       "        [0.6929],\n",
       "        [0.7212],\n",
       "        [0.5000],\n",
       "        [0.9827],\n",
       "        [0.9910],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9118],\n",
       "        [0.8800],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.7392],\n",
       "        [0.5000],\n",
       "        [0.9719],\n",
       "        [0.8554],\n",
       "        [0.9850],\n",
       "        [0.6943],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9979],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9900],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9986],\n",
       "        [0.5000],\n",
       "        [0.9336],\n",
       "        [1.0000],\n",
       "        [0.9865],\n",
       "        [0.8410],\n",
       "        [0.5000],\n",
       "        [0.9991],\n",
       "        [0.9994],\n",
       "        [0.9315],\n",
       "        [0.9934],\n",
       "        [0.9425],\n",
       "        [0.5000],\n",
       "        [0.8832],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5727],\n",
       "        [0.5000],\n",
       "        [0.5313],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9922],\n",
       "        [0.9976],\n",
       "        [0.9999],\n",
       "        [0.9994],\n",
       "        [0.8592],\n",
       "        [0.9693],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9916],\n",
       "        [0.9447],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9963],\n",
       "        [0.5000],\n",
       "        [0.9467],\n",
       "        [0.5000],\n",
       "        [0.5870],\n",
       "        [0.9402],\n",
       "        [0.9347],\n",
       "        [0.9514],\n",
       "        [0.9842],\n",
       "        [0.9900],\n",
       "        [0.9990],\n",
       "        [0.7109],\n",
       "        [0.9411],\n",
       "        [0.9986],\n",
       "        [0.9990],\n",
       "        [0.9812],\n",
       "        [0.9741],\n",
       "        [0.9974],\n",
       "        [0.8958],\n",
       "        [0.6280],\n",
       "        [0.9836],\n",
       "        [0.5000],\n",
       "        [0.9794],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9195],\n",
       "        [0.9977],\n",
       "        [0.9404],\n",
       "        [1.0000],\n",
       "        [0.8155],\n",
       "        [0.8183],\n",
       "        [0.9772],\n",
       "        [0.5000],\n",
       "        [0.9085],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9784],\n",
       "        [0.9669],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9588],\n",
       "        [0.9989],\n",
       "        [0.5000],\n",
       "        [0.9652],\n",
       "        [0.5000],\n",
       "        [0.9025],\n",
       "        [0.9522],\n",
       "        [0.9867],\n",
       "        [0.9464],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9301],\n",
       "        [1.0000],\n",
       "        [0.9406],\n",
       "        [0.9992],\n",
       "        [0.5000],\n",
       "        [0.9815],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.9347],\n",
       "        [0.5000],\n",
       "        [0.7698],\n",
       "        [0.9968],\n",
       "        [0.9875],\n",
       "        [0.6036],\n",
       "        [0.9583],\n",
       "        [0.7914],\n",
       "        [0.9059],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9993],\n",
       "        [0.9615],\n",
       "        [0.9316],\n",
       "        [0.9832],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9284],\n",
       "        [0.8935],\n",
       "        [0.9858],\n",
       "        [0.8174],\n",
       "        [0.9952],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.6644],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9289],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9620],\n",
       "        [0.9985],\n",
       "        [1.0000],\n",
       "        [0.6638],\n",
       "        [0.9816],\n",
       "        [0.8991],\n",
       "        [0.9890],\n",
       "        [0.9892],\n",
       "        [1.0000],\n",
       "        [0.8561],\n",
       "        [0.9610],\n",
       "        [0.8684],\n",
       "        [0.9498],\n",
       "        [0.7210],\n",
       "        [0.9999],\n",
       "        [0.5000],\n",
       "        [0.9294],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9916],\n",
       "        [0.9988],\n",
       "        [0.9936],\n",
       "        [0.8940],\n",
       "        [0.9802],\n",
       "        [0.8914],\n",
       "        [0.5000],\n",
       "        [0.9583],\n",
       "        [0.8827],\n",
       "        [0.9963],\n",
       "        [0.9506],\n",
       "        [0.9711],\n",
       "        [0.7535],\n",
       "        [0.9998],\n",
       "        [0.5000],\n",
       "        [0.8130],\n",
       "        [0.9977],\n",
       "        [0.9941],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9910],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9573],\n",
       "        [0.9945],\n",
       "        [0.9786],\n",
       "        [0.9502],\n",
       "        [0.9312],\n",
       "        [0.9311],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9448],\n",
       "        [0.9950],\n",
       "        [1.0000],\n",
       "        [0.9348],\n",
       "        [0.5000],\n",
       "        [0.9901],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.7901],\n",
       "        [0.5000],\n",
       "        [0.9207],\n",
       "        [0.7313],\n",
       "        [0.9342],\n",
       "        [0.9456],\n",
       "        [0.9526],\n",
       "        [0.5000],\n",
       "        [0.8504],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9991],\n",
       "        [0.5000],\n",
       "        [0.7409],\n",
       "        [0.9153],\n",
       "        [0.5000],\n",
       "        [0.9961],\n",
       "        [0.5000],\n",
       "        [0.9983],\n",
       "        [0.9814],\n",
       "        [0.8222],\n",
       "        [0.9851],\n",
       "        [0.9997],\n",
       "        [0.9417],\n",
       "        [0.9705],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.8532],\n",
       "        [0.9877],\n",
       "        [0.9310],\n",
       "        [0.5000],\n",
       "        [0.8344],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [0.9788],\n",
       "        [1.0000],\n",
       "        [0.9878],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9440],\n",
       "        [0.5696],\n",
       "        [0.9084],\n",
       "        [0.9956],\n",
       "        [0.5000],\n",
       "        [0.9615],\n",
       "        [0.5000],\n",
       "        [0.9502],\n",
       "        [0.8821],\n",
       "        [0.9103],\n",
       "        [0.9756],\n",
       "        [0.9147],\n",
       "        [0.5000],\n",
       "        [0.9319],\n",
       "        [0.5000],\n",
       "        [0.7475],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9939],\n",
       "        [0.9151],\n",
       "        [0.9953],\n",
       "        [0.9633],\n",
       "        [0.9677],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.6568],\n",
       "        [0.9514],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9803],\n",
       "        [0.9981],\n",
       "        [0.7169],\n",
       "        [0.5000],\n",
       "        [0.9815],\n",
       "        [0.9370],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.9746],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.8217],\n",
       "        [0.9997],\n",
       "        [1.0000],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.8613],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.7798],\n",
       "        [1.0000],\n",
       "        [0.9828],\n",
       "        [0.8902],\n",
       "        [0.9687],\n",
       "        [0.9562],\n",
       "        [0.6422],\n",
       "        [0.5000],\n",
       "        [0.9999],\n",
       "        [0.5000],\n",
       "        [0.9300],\n",
       "        [0.9982],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9996],\n",
       "        [0.9341],\n",
       "        [0.9746],\n",
       "        [0.9962],\n",
       "        [0.9992],\n",
       "        [0.9695],\n",
       "        [0.9857],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.9739],\n",
       "        [0.9767],\n",
       "        [1.0000],\n",
       "        [0.9903],\n",
       "        [1.0000],\n",
       "        [0.9070],\n",
       "        [1.0000],\n",
       "        [0.9791],\n",
       "        [1.0000],\n",
       "        [0.9278],\n",
       "        [0.9990],\n",
       "        [0.8830],\n",
       "        [1.0000],\n",
       "        [0.5488],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores = pd.read_csv('./Bases/Bases/entradas_breast.csv')\n",
    "classes = pd.read_csv('./Bases/Bases/saidas_breast.csv')\n",
    "\n",
    "previsores = torch.tensor(np.array(previsores), dtype=torch.float)\n",
    "classes = torch.tensor(np.array(classes), dtype=torch.float)\n",
    "\n",
    "previsores = classificador(previsores).detach()\n",
    "previsores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3525)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(previsores, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226713532513181"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(classes, previsores.numpy() > 0.5)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
